{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"zae-limiter","text":"<p>A rate limiting library backed by DynamoDB using the token bucket algorithm.</p>"},{"location":"#overview","title":"Overview","text":"<p>zae-limiter excels at rate limiting scenarios where:</p> <ul> <li>Multiple limits are tracked per call (requests per minute, tokens per minute)</li> <li>Consumption is unknown upfront \u2014 adjust limits after the operation completes</li> <li>Hierarchical limits exist (API key \u2192 project, tenant \u2192 user)</li> <li>Cost matters \u2014 ~$1/1M requests (details)</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Token Bucket Algorithm - Precise rate limiting with configurable burst capacity</li> <li>Multiple Limits - Track requests per minute, tokens per minute, etc. in a single call</li> <li>Hierarchical Entities - Two-level hierarchy (project \u2192 API keys) with cascade mode</li> <li>Atomic Transactions - Multi-key updates via DynamoDB TransactWriteItems</li> <li>Rollback on Exception - Automatic rollback if your code throws</li> <li>Stored Limits - Configure per-entity limits in DynamoDB</li> <li>Usage Analytics - Lambda aggregator for hourly/daily usage snapshots</li> <li>Audit Logging - Track entity and limit changes for compliance</li> <li>Async + Sync APIs - First-class async support with sync wrapper</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from zae_limiter import RateLimiter, Limit, StackOptions\n\n# Async rate limiter with declarative infrastructure\nlimiter = RateLimiter(\n    name=\"my-app\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(),  # Declare desired state - CloudFormation ensures it\n)\n\n# Define default limits (can be overridden per-entity)\ndefault_limits = [\n    Limit.per_minute(\"rpm\", 100),\n    Limit.per_minute(\"tpm\", 10_000, burst=50_000),  # Token bucket with burst\n]\n\nasync with limiter.acquire(\n    entity_id=\"api-key-123\",\n    resource=\"gpt-4\",\n    limits=default_limits,\n    consume={\"rpm\": 1, \"tpm\": 500},  # Estimate tokens upfront\n) as lease:\n    response = await call_llm()\n    # Reconcile actual usage (can go negative for post-hoc adjustment)\n    await lease.adjust(tpm=response.usage.total_tokens - 500)\n    # On success: committed | On exception: rolled back automatically\n\n# Hierarchical entities: project \u2192 API key\nawait limiter.create_entity(entity_id=\"proj-1\", name=\"Production\")\nawait limiter.set_limits(\"proj-1\", [Limit.per_minute(\"tpm\", 100_000)])\nawait limiter.create_entity(entity_id=\"api-key-456\", parent_id=\"proj-1\", cascade=True)\n\n# cascade is an entity property \u2014 acquire() auto-cascades to parent\nasync with limiter.acquire(\n    entity_id=\"api-key-456\",\n    resource=\"gpt-4\",\n    limits=default_limits,\n    consume={\"rpm\": 1, \"tpm\": 500},\n    use_stored_limits=True,  # Uses proj-1's 100k tpm limit\n) as lease:\n    response = await call_llm()\n</code></pre>"},{"location":"#why-dynamodb","title":"Why DynamoDB?","text":"<ul> <li>Serverless - No infrastructure to manage, 99.99% SLA</li> <li>Regional - Deploy independently per region with low latency</li> <li>Scalable - Handles millions of requests per second</li> <li>Cost-effective - Pay per request, no idle costs</li> <li>Atomic - TransactWriteItems for multi-key consistency</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"Guide Description Getting Started Installation and first deployment Basic Usage Rate limiting patterns and error handling Hierarchical Limits Parent/child entities, cascade mode LLM Integration Token estimation and reconciliation Production Guide Security, monitoring, cost CLI Reference Deploy, status, delete commands"},{"location":"benchmarks/","title":"Benchmark Results","text":"<p>This document tracks benchmark results for performance-sensitive operations in zae-limiter.</p>"},{"location":"benchmarks/#baseline-results","title":"Baseline Results","text":"<p>Baseline benchmarks capture performance before optimization work. Compare new results against baselines to detect performance regressions.</p>"},{"location":"benchmarks/#moto-benchmarks-mocked-dynamodb","title":"Moto Benchmarks (Mocked DynamoDB)","text":"<p>Moto benchmarks run against mocked DynamoDB and measure operation latency without network overhead.</p> <pre><code>Run with:\npytest tests/benchmark/test_operations.py -v --benchmark-json=benchmark-moto.json\n</code></pre> <p>Key Metrics: - <code>acquire_release_single_limit</code>: Single limit acquire/release (baseline) - <code>acquire_release_multiple_limits</code>: Multi-limit overhead (rpm + tpm) - <code>cascade_optimized</code>: BatchGetItem optimization impact - <code>config_lookup_cached</code>: Config cache hit performance - <code>config_lookup_cold</code>: Config cache miss performance</p>"},{"location":"benchmarks/#localstack-benchmarks-realistic-dynamodb","title":"LocalStack Benchmarks (Realistic DynamoDB)","text":"<p>LocalStack benchmarks run against an emulated DynamoDB including realistic network latency.</p> <pre><code>Run with:\ndocker compose up -d\nexport AWS_ENDPOINT_URL=http://localhost:4566\nexport AWS_ACCESS_KEY_ID=test\nexport AWS_SECRET_ACCESS_KEY=test\nexport AWS_DEFAULT_REGION=us-east-1\npytest tests/benchmark/test_localstack.py -v --benchmark-json=benchmark-localstack.json\n</code></pre> <p>Key Metrics: - <code>acquire_release_localstack</code>: Basic acquire with realistic latency - <code>cascade_with_batchgetitem_optimization</code>: Cascade using optimized pattern - <code>cascade_with_config_cache_optimization</code>: Combined optimizations</p>"},{"location":"benchmarks/#performance-targets","title":"Performance Targets","text":"Scenario Metric Target Single limit acquire p50 latency No regression Single limit acquire p95 latency No regression Single limit acquire p99 latency &lt; 2x baseline Cascade (BatchGetItem) p50 latency 10-20% reduction vs sequential Config lookup (cached) p50 latency &lt; 5ms overhead Config lookup (cold) p50 latency &lt; 20ms overhead"},{"location":"benchmarks/#historical-comparison","title":"Historical Comparison","text":"<p>Benchmark JSON files are stored alongside this document for version-to-version comparison:</p> <ul> <li><code>benchmark-v0.11.0.json</code> - Baseline before config caching and cascade optimization</li> <li><code>benchmark-v0.12.0.json</code> - After centralized config implementation (v0.5.0 feature)</li> <li><code>benchmark-v0.12.1.json</code> - After cascade BatchGetItem optimization (issue #133)</li> </ul>"},{"location":"benchmarks/#benchmark-organization","title":"Benchmark Organization","text":"<pre><code>tests/benchmark/\n\u251c\u2500\u2500 conftest.py           # Shared fixtures (CapacityCounter, benchmark_entities)\n\u251c\u2500\u2500 test_operations.py    # Moto benchmarks (fast, no Docker)\n\u251c\u2500\u2500 test_localstack.py    # LocalStack benchmarks (realistic latency)\n\u251c\u2500\u2500 test_latency.py       # p50/p95/p99 latency breakdown\n\u251c\u2500\u2500 test_throughput.py    # Sequential/concurrent throughput\n\u251c\u2500\u2500 test_capacity.py      # DynamoDB RCU/WCU tracking\n\u2514\u2500\u2500 test_aws.py           # Real AWS benchmarks (production metrics)\n</code></pre>"},{"location":"benchmarks/#running-benchmarks","title":"Running Benchmarks","text":""},{"location":"benchmarks/#unitmoto-benchmarks-fast","title":"Unit/Moto Benchmarks (Fast)","text":"<pre><code># All moto benchmarks\nuv run pytest tests/benchmark/test_operations.py -v --benchmark-json=bench.json\n\n# Specific benchmark class\nuv run pytest tests/benchmark/test_operations.py::TestConfigLookupBenchmarks -v\n\n# Compare against baseline\nuv run pytest tests/benchmark/test_operations.py --benchmark-compare=bench-baseline.json\n</code></pre>"},{"location":"benchmarks/#localstack-benchmarks-requires-docker","title":"LocalStack Benchmarks (Requires Docker)","text":"<pre><code># Start LocalStack\ndocker compose up -d\n\n# Run LocalStack benchmarks\nexport AWS_ENDPOINT_URL=http://localhost:4566\nexport AWS_ACCESS_KEY_ID=test\nexport AWS_SECRET_ACCESS_KEY=test\nexport AWS_DEFAULT_REGION=us-east-1\nuv run pytest tests/benchmark/test_localstack.py -v --benchmark-json=bench-ls.json\n\n# Stop LocalStack\ndocker compose down\n</code></pre>"},{"location":"benchmarks/#real-aws-benchmarks-optional","title":"Real AWS Benchmarks (Optional)","text":"<pre><code># Run against real AWS (costs money!)\nAWS_PROFILE=zeroae-code/AWSPowerUserAccess \\\n  uv run pytest tests/benchmark/test_aws.py --run-aws -v --benchmark-json=bench-aws.json\n</code></pre>"},{"location":"benchmarks/#interpreting-results","title":"Interpreting Results","text":""},{"location":"benchmarks/#pytest-benchmark-output","title":"Pytest-Benchmark Output","text":"<pre><code>benchmark: 5 tests\n\ntest_operations.py::TestAcquireReleaseBenchmarks::test_acquire_release_single_limit\n  mean \u00b1 std dev: 1.23 \u00b1 0.15 ms [min: 0.98 ms, max: 1.65 ms]\n  \"... 1000 rounds\"\n</code></pre> <p>Columns: - <code>mean</code>: Average latency - <code>std dev</code>: Standard deviation (consistency) - <code>min/max</code>: Range of observed values - Rounds: Number of iterations</p>"},{"location":"benchmarks/#comparing-baselines","title":"Comparing Baselines","text":"<pre><code># Generate JSON from new run\npytest tests/benchmark/test_operations.py -v --benchmark-json=new.json\n\n# Compare against saved baseline\npytest tests/benchmark/test_operations.py -v --benchmark-compare=baseline.json\n</code></pre> <p>Output interpretation: - <code>PASS</code>: Performance stable (no regression) - <code>FAIL</code>: Performance degraded (potential issue) - <code>5.50%</code> or <code>+5.50%</code>: Performance improved by 5.5% - <code>-5.50%</code>: Performance degraded by 5.5%</p>"},{"location":"benchmarks/#adding-new-benchmarks","title":"Adding New Benchmarks","text":"<p>When adding new benchmarks:</p> <ol> <li>Create test in appropriate file (<code>test_operations.py</code>, <code>test_localstack.py</code>, etc.)</li> <li>Use <code>@pytest.mark.benchmark</code> marker</li> <li>Include clear docstring explaining what's measured</li> <li>Compare against related baseline test</li> <li>Run benchmark locally: <code>pytest tests/benchmark/test_*.py -v --benchmark-json=bench.json</code></li> <li>Commit JSON baseline for future comparison</li> <li>Update this document with new metrics and targets</li> </ol>"},{"location":"benchmarks/#markers-and-filters","title":"Markers and Filters","text":"Marker Purpose Filter <code>@pytest.mark.benchmark</code> Benchmark test <code>pytest -m benchmark</code> <code>@pytest.mark.integration</code> Requires LocalStack <code>pytest -m integration</code> <code>@pytest.mark.slow</code> &gt; 30s runtime <code>pytest -m \"not slow\"</code>"},{"location":"benchmarks/#ci-integration","title":"CI Integration","text":"<p>Benchmarks can be integrated into CI/CD:</p> <pre><code># .github/workflows/benchmark.yml (optional)\n- name: Run benchmarks\n  run: |\n    docker compose up -d\n    pytest tests/benchmark/ -v --benchmark-json=results.json\n    docker compose down\n</code></pre> <p>Note: Consider benchmark flakiness before enabling in CI (network latency varies).</p>"},{"location":"cli/","title":"CLI Reference","text":"<p>zae-limiter provides a command-line interface for managing infrastructure and deployments.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>The CLI is included with the package:</p> <pre><code>pip install zae-limiter\n# or\nconda install -c conda-forge zae-limiter\n</code></pre> <p>Verify installation:</p> <pre><code>zae-limiter --version\n</code></pre>"},{"location":"cli/#commands","title":"Commands","text":""},{"location":"cli/#deploy","title":"deploy","text":"<p>Deploy the CloudFormation stack with DynamoDB table and Lambda aggregator.</p> <pre><code>zae-limiter deploy [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--name</code> Resource identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint (LocalStack) None <code>--snapshot-windows</code> Comma-separated snapshot windows <code>hourly,daily</code> <code>--retention-days</code> Usage snapshot retention (days) <code>90</code> <code>--enable-aggregator/--no-aggregator</code> Deploy Lambda aggregator <code>true</code> <code>--pitr-recovery-days</code> Point-in-time recovery (1-35 days) None (disabled) <code>--log-retention-days</code> CloudWatch log retention (days) <code>30</code> <code>--lambda-timeout</code> Lambda timeout (1-900 seconds) <code>60</code> <code>--lambda-memory</code> Lambda memory (128-3008 MB) <code>256</code> <code>--enable-alarms/--no-alarms</code> Deploy CloudWatch alarms <code>true</code> <code>--alarm-sns-topic</code> SNS topic ARN for notifications None <code>--permission-boundary</code> IAM permission boundary None <code>--role-name-format</code> Lambda role name format None <code>--enable-audit-archival/--no-audit-archival</code> Archive expired audit events to S3 <code>true</code> <code>--audit-archive-glacier-days</code> Days before Glacier IR transition (1-3650) <code>90</code> <code>--enable-tracing/--no-tracing</code> Enable AWS X-Ray tracing <code>false</code> <code>--wait/--no-wait</code> Wait for stack creation <code>true</code> <p>Examples:</p> <pre><code># Basic deployment\nzae-limiter deploy --name limiter --region us-east-1\n\n# With custom settings\nzae-limiter deploy \\\n    --name prod \\\n    --region us-west-2 \\\n    --log-retention-days 90 \\\n    --pitr-recovery-days 7\n\n# Deploy to LocalStack\nzae-limiter deploy \\\n    --name limiter \\\n    --endpoint-url http://localhost:4566 \\\n    --region us-east-1\n\n# Without Lambda aggregator\nzae-limiter deploy \\\n    --name limiter \\\n    --region us-east-1 \\\n    --no-aggregator\n\n# With custom audit archival settings\nzae-limiter deploy \\\n    --name limiter \\\n    --region us-east-1 \\\n    --audit-archive-glacier-days 180\n\n# Disable audit archival\nzae-limiter deploy \\\n    --name limiter \\\n    --region us-east-1 \\\n    --no-audit-archival\n\n# With X-Ray tracing enabled\nzae-limiter deploy \\\n    --name limiter \\\n    --region us-east-1 \\\n    --enable-tracing\n</code></pre>"},{"location":"cli/#list","title":"list","text":"<p>List all deployed rate limiter instances in a region.</p> <pre><code>zae-limiter list [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint (LocalStack) None <p>Examples:</p> <pre><code># List all limiters in us-east-1\nzae-limiter list --region us-east-1\n\n# List limiters using default region\nzae-limiter list\n\n# List limiters in LocalStack\nzae-limiter list --endpoint-url http://localhost:4566 --region us-east-1\n</code></pre> <p>Output:</p> <pre><code>Rate Limiter Instances (us-east-1)\n===========================================================================================\n\nName                 Status                    Version      Lambda       Schema     Created\n-------------------------------------------------------------------------------------------\nprod-api             CREATE_COMPLETE           0.2.0        0.2.0        1.0.0      2024-01-15\nstaging              CREATE_COMPLETE           0.2.0        0.2.0        1.0.0      2024-01-10\ndev-test             UPDATE_IN_PROGRESS        0.1.0        0.1.0        1.0.0      2023-12-01\n\nTotal: 3 instance(s)\n  1 instance(s) need attention\n</code></pre> <p>The output includes:</p> <ul> <li>Name: User-friendly name</li> <li>Status: CloudFormation stack status with visual indicator</li> <li>Version: Client version at deployment (from stack tag)</li> <li>Lambda: Lambda aggregator version</li> <li>Schema: DynamoDB schema version</li> <li>Created: Stack creation date</li> </ul>"},{"location":"cli/#status","title":"status","text":"<p>Check the status of a deployed CloudFormation stack.</p> <pre><code>zae-limiter status [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--name</code> Resource identifier Required <code>--region</code> AWS region Required <code>--endpoint-url</code> Custom AWS endpoint None <p>Example:</p> <pre><code>zae-limiter status --name limiter --region us-east-1\n</code></pre> <p>Output:</p> <pre><code>Status: limiter\n==================================================\n\nConnectivity\n  Available:     \u2713 Yes\n  Latency:       42ms\n  Region:        us-east-1\n\nInfrastructure\n  Stack:         CREATE_COMPLETE\n  Table:         ACTIVE\n  Aggregator:    Enabled\n\nVersions\n  Client:        0.2.0\n  Schema:        1.0.0\n  Lambda:        0.2.0\n\nTable Metrics\n  Items:         1,234\n  Size:          128.5 KB\n\n\u2713 Infrastructure is ready\n</code></pre>"},{"location":"cli/#delete","title":"delete","text":"<p>Delete a CloudFormation stack and all its resources.</p> <pre><code>zae-limiter delete [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--name</code> Resource identifier Required <code>--region</code> AWS region Required <code>--yes</code> Skip confirmation prompt <code>false</code> <code>--endpoint-url</code> Custom AWS endpoint None <p>Example:</p> <pre><code># With confirmation\nzae-limiter delete --name limiter --region us-east-1\n\n# Skip confirmation\nzae-limiter delete --name limiter --region us-east-1 --yes\n</code></pre> <p>Data Loss</p> <p>Deleting a stack removes the DynamoDB table and all its data. This action cannot be undone.</p>"},{"location":"cli/#cfn-template","title":"cfn-template","text":"<p>Export the CloudFormation template to stdout.</p> <pre><code>zae-limiter cfn-template [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--output</code>, <code>-o</code> Output file path stdout <p>Examples:</p> <pre><code># Export template to stdout\nzae-limiter cfn-template &gt; template.yaml\n\n# Export template to file\nzae-limiter cfn-template --output template.yaml\n\n# View template\nzae-limiter cfn-template | less\n</code></pre>"},{"location":"cli/#lambda-export","title":"lambda-export","text":"<p>Export the Lambda deployment package.</p> <pre><code>zae-limiter lambda-export [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--output</code>, <code>-o</code> Output file path <code>lambda.zip</code> <code>--info</code> Show package info without building <code>false</code> <code>--force</code>, <code>-f</code> Overwrite existing file <code>false</code> <p>Examples:</p> <pre><code># Export Lambda package\nzae-limiter lambda-export --output lambda.zip\n\n# Show package info\nzae-limiter lambda-export --info\n</code></pre> <p>Info output:</p> <pre><code>Lambda Package Info:\n  Handler: zae_limiter.aggregator.handler.lambda_handler\n  Runtime: python3.12\n  Estimated size: ~30KB\n  Dependencies: boto3 (provided by Lambda runtime)\n</code></pre>"},{"location":"cli/#version","title":"version","text":"<p>Show infrastructure version information for a deployed stack.</p> <pre><code>zae-limiter version [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--name</code> Resource identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint None <p>Example:</p> <pre><code>zae-limiter version --name limiter --region us-east-1\n</code></pre> <p>Output:</p> <pre><code>zae-limiter Infrastructure Version\n====================================\n\nClient Version:     0.1.0\nSchema Version:     1.0.0\n\nInfra Schema:       1.0.0\nLambda Version:     0.1.0\nMin Client Version: 0.0.0\n\nStatus: COMPATIBLE\n</code></pre>"},{"location":"cli/#check","title":"check","text":"<p>Check infrastructure compatibility without modifying.</p> <pre><code>zae-limiter check [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--name</code> Resource identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint None <p>Example:</p> <pre><code>zae-limiter check --name limiter --region us-east-1\n</code></pre> <p>Output:</p> <pre><code>Compatibility Check\n====================\n\nClient:      0.1.0\nSchema:      1.0.0\nLambda:      0.1.0\n\nResult: COMPATIBLE\n\nClient and infrastructure are fully compatible.\n</code></pre>"},{"location":"cli/#upgrade","title":"upgrade","text":"<p>Upgrade a table schema to the latest version.</p> <pre><code>zae-limiter upgrade [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--name</code> Resource identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint None <code>--lambda-only</code> Only update Lambda code <code>false</code> <code>--force</code> Force update even if version matches <code>false</code> <p>Example:</p> <pre><code># Upgrade infrastructure\nzae-limiter upgrade --name limiter --region us-east-1\n\n# Force Lambda update only\nzae-limiter upgrade --name limiter --region us-east-1 --lambda-only --force\n</code></pre>"},{"location":"cli/#audit-list","title":"audit list","text":"<p>List audit events for an entity.</p> <pre><code>zae-limiter audit list [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--name</code>, <code>-n</code> Resource identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint None <code>--entity-id</code>, <code>-e</code> Entity ID to query (required) - <code>--limit</code>, <code>-l</code> Maximum events to return <code>100</code> <code>--start-event-id</code> Event ID for pagination None <p>Examples:</p> <pre><code># List audit events for an entity\nzae-limiter audit list --name limiter --entity-id proj-1\n\n# Limit results\nzae-limiter audit list --entity-id proj-1 --limit 10\n\n# Paginate through results\nzae-limiter audit list --entity-id proj-1 --start-event-id 01HXYZ...\n</code></pre> <p>Output:</p> <pre><code>Audit Events for: proj-1\n================================================================================\n\nTimestamp                Action             Principal            Resource\n--------------------------------------------------------------------------------\n2024-01-15T10:30:00Z     limits_set         admin@example.com    gpt-4\n2024-01-15T10:00:00Z     entity_created     admin@example.com    -\n\nTotal: 2 events\n</code></pre>"},{"location":"cli/#usage-list","title":"usage list","text":"<p>List usage snapshots for historical consumption data.</p> <p>Usage snapshots are created by the Lambda aggregator from DynamoDB stream events. They track token consumption per entity/resource within time windows (hourly, daily).</p> <pre><code>zae-limiter usage list [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--name</code>, <code>-n</code> Resource identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint None <code>--entity-id</code>, <code>-e</code> Entity ID to query - <code>--resource</code>, <code>-r</code> Resource name filter - <code>--window</code>, <code>-w</code> Window type (<code>hourly</code>, <code>daily</code>) None <code>--start</code> Start time (ISO format) None <code>--end</code> End time (ISO format) None <code>--limit</code>, <code>-l</code> Maximum snapshots to return <code>100</code> <code>--plot</code>, <code>-p</code> Display as ASCII charts instead of table <code>false</code> <p>Note</p> <p>Either <code>--entity-id</code> or <code>--resource</code> must be provided.</p> <p>ASCII Charts</p> <p>The <code>--plot</code> flag requires the optional <code>plot</code> extra. Install with: <pre><code>pip install 'zae-limiter[plot]'\n</code></pre></p> <p>Examples:</p> <pre><code># List all snapshots for an entity\nzae-limiter usage list --entity-id user-123\n\n# Filter by resource and window type\nzae-limiter usage list --entity-id user-123 --resource gpt-4 --window hourly\n\n# Query by resource across all entities\nzae-limiter usage list --resource gpt-4 --window daily\n\n# Filter by time range\nzae-limiter usage list --entity-id user-123 \\\n    --start 2024-01-01T00:00:00Z \\\n    --end 2024-01-31T23:59:59Z\n\n# Limit results\nzae-limiter usage list --entity-id user-123 --limit 10\n\n# Display as ASCII chart (requires: pip install 'zae-limiter[plot]')\nzae-limiter usage list --entity-id user-123 --plot\n</code></pre> <p>Table Output (default):</p> <pre><code>Usage Snapshots\n====================================================================================================\n\nWindow Start           Type     Resource         Entity               Events Counters\n----------------------------------------------------------------------------------------------------\n2024-01-15T14:00:00Z   hourly   gpt-4            user-123                 25 rpm=25, tpm=12,500\n2024-01-15T13:00:00Z   hourly   gpt-4            user-123                 18 rpm=18, tpm=9,000\n2024-01-15T12:00:00Z   hourly   gpt-4            user-123                 32 rpm=32, tpm=16,000\n\nTotal: 3 snapshots\n</code></pre> <p>Plot Output (<code>--plot</code> flag):</p> <pre><code>Usage Plot: gpt-4 (hourly)\nEntity: user-123\n================================================================================\n\nRPM                             TPM\n----------------------------    -------------------------------\n32  \u2524      \u256d                    16,000  \u2524      \u256d\n28  \u2524     \u256d\u256f                    14,000  \u2524     \u256d\u256f\n25  \u2524    \u256d\u256f                     12,500  \u2524    \u256d\u256f\n21  \u2524   \u256d\u256f                      11,000  \u2524   \u256d\u256f\n18  \u253c\u2500\u2500\u2500\u256f                        9,000  \u253c\u2500\u2500\u2500\u256f\n\nTime range: 2024-01-15T12:00:00Z to 2024-01-15T14:00:00Z\nData points: 3\n\nTotal: 3 snapshots\n</code></pre> <p>The plot shows counters side-by-side (2 per row) with:</p> <ul> <li>Header: Resource name, window type, and entity ID</li> <li>Y-axis: Right-aligned labels with thousands separators</li> <li>Downsampling: Large datasets (&gt;60 points) are automatically downsampled with a note</li> </ul>"},{"location":"cli/#usage-summary","title":"usage summary","text":"<p>Show aggregated usage summary across multiple snapshots.</p> <p>Computes total and average consumption statistics over matching snapshots. Useful for billing, reporting, and capacity planning.</p> <pre><code>zae-limiter usage summary [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--name</code>, <code>-n</code> Resource identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint None <code>--entity-id</code>, <code>-e</code> Entity ID to query - <code>--resource</code>, <code>-r</code> Resource name filter - <code>--window</code>, <code>-w</code> Window type (<code>hourly</code>, <code>daily</code>) None <code>--start</code> Start time (ISO format) None <code>--end</code> End time (ISO format) None <p>Note</p> <p>Either <code>--entity-id</code> or <code>--resource</code> must be provided.</p> <p>Examples:</p> <pre><code># Get summary for an entity\nzae-limiter usage summary --entity-id user-123\n\n# Summary for a specific resource\nzae-limiter usage summary --entity-id user-123 --resource gpt-4\n\n# Summary for a time range\nzae-limiter usage summary --entity-id user-123 \\\n    --resource gpt-4 \\\n    --window hourly \\\n    --start 2024-01-01T00:00:00Z \\\n    --end 2024-01-31T23:59:59Z\n</code></pre> <p>Output:</p> <pre><code>Usage Summary\n============================================================\n\nEntity:     user-123\nResource:   gpt-4\nWindow:     hourly\nSnapshots:  720\nTime Range: 2024-01-01T00:00:00Z to 2024-01-31T23:00:00Z\n\nLimit                   Total         Average\n------------------------------------------------------------\nrpm                     18,000           25.00\ntpm                  9,000,000       12,500.00\n</code></pre>"},{"location":"cli/#resource","title":"resource","text":"<p>Manage resource-level limit configurations. These limits apply to a specific resource and override system-level defaults.</p>"},{"location":"cli/#resource-set-defaults","title":"resource set-defaults","text":"<p>Set default limits for a resource.</p> <pre><code>zae-limiter resource set-defaults &lt;RESOURCE_NAME&gt; [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Description <code>RESOURCE_NAME</code> The resource to configure (e.g., 'gpt-4', 'claude-3') <p>Options:</p> Option Description Default <code>--name</code>, <code>-n</code> Stack identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint (LocalStack) None <code>--limit</code>, <code>-l</code> Limit specification (required, repeatable) - <p>Limit format: <code>name:capacity[:burst]</code></p> <ul> <li><code>name</code> - Limit name (e.g., <code>tpm</code>, <code>rpm</code>)</li> <li><code>capacity</code> - Maximum tokens per period</li> <li><code>burst</code> - Optional burst capacity (defaults to capacity)</li> </ul> <p>Examples:</p> <pre><code># Set TPM and RPM defaults for gpt-4\nzae-limiter resource set-defaults gpt-4 -l tpm:100000 -l rpm:1000\n\n# Set defaults with burst capacity\nzae-limiter resource set-defaults claude-3 -l tpm:50000:75000 -l rpm:500:750\n\n# Use a specific limiter instance\nzae-limiter resource set-defaults gpt-4 --name prod -l tpm:100000\n</code></pre>"},{"location":"cli/#resource-get-defaults","title":"resource get-defaults","text":"<p>Get default limits for a resource.</p> <pre><code>zae-limiter resource get-defaults &lt;RESOURCE_NAME&gt; [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Description <code>RESOURCE_NAME</code> The resource to query (e.g., 'gpt-4', 'claude-3') <p>Options:</p> Option Description Default <code>--name</code>, <code>-n</code> Stack identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint (LocalStack) None <p>Examples:</p> <pre><code># Get defaults for gpt-4\nzae-limiter resource get-defaults gpt-4\n\n# Query from a specific instance\nzae-limiter resource get-defaults gpt-4 --name prod --region us-west-2\n</code></pre> <p>Output:</p> <pre><code>Defaults for resource 'gpt-4':\n  tpm: 100,000/min (burst: 100,000)\n  rpm: 1,000/min (burst: 1,000)\n</code></pre>"},{"location":"cli/#resource-delete-defaults","title":"resource delete-defaults","text":"<p>Delete default limits for a resource.</p> <pre><code>zae-limiter resource delete-defaults &lt;RESOURCE_NAME&gt; [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Description <code>RESOURCE_NAME</code> The resource to delete defaults from <p>Options:</p> Option Description Default <code>--name</code>, <code>-n</code> Stack identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint (LocalStack) None <code>--yes</code>, <code>-y</code> Skip confirmation prompt <code>false</code> <p>Examples:</p> <pre><code># Delete with confirmation\nzae-limiter resource delete-defaults gpt-4\n\n# Skip confirmation\nzae-limiter resource delete-defaults gpt-4 --yes\n</code></pre>"},{"location":"cli/#resource-list","title":"resource list","text":"<p>List all resources with configured defaults.</p> <pre><code>zae-limiter resource list [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--name</code>, <code>-n</code> Stack identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint (LocalStack) None <p>Examples:</p> <pre><code># List all resources with defaults\nzae-limiter resource list\n\n# List from a specific instance\nzae-limiter resource list --name prod\n</code></pre> <p>Output:</p> <pre><code>Resources with configured defaults:\n  claude-3\n  gpt-4\n  gpt-4-turbo\n</code></pre>"},{"location":"cli/#system","title":"system","text":"<p>Manage system-wide default limit configurations. These are global defaults that apply to all resources unless overridden at the resource or entity level.</p>"},{"location":"cli/#system-set-defaults","title":"system set-defaults","text":"<p>Set system-wide default limits (applies to all resources).</p> <pre><code>zae-limiter system set-defaults [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--name</code>, <code>-n</code> Stack identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint (LocalStack) None <code>--limit</code>, <code>-l</code> Limit specification (required, repeatable) - <code>--on-unavailable</code> Behavior when DynamoDB unavailable (<code>allow</code> or <code>block</code>) None <p>Limit format: <code>name:capacity[:burst]</code></p> <p>Examples:</p> <pre><code># Set system-wide defaults (applies to ALL resources)\nzae-limiter system set-defaults -l tpm:10000 -l rpm:100\n\n# Set defaults with on_unavailable behavior\nzae-limiter system set-defaults -l tpm:10000 -l rpm:100 --on-unavailable allow\n\n# Set defaults with burst\nzae-limiter system set-defaults -l tpm:5000:7500 -l rpm:50:75\n</code></pre>"},{"location":"cli/#system-get-defaults","title":"system get-defaults","text":"<p>Get system-wide default limits.</p> <pre><code>zae-limiter system get-defaults [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--name</code>, <code>-n</code> Stack identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint (LocalStack) None <p>Examples:</p> <pre><code># Get system-wide defaults\nzae-limiter system get-defaults\n</code></pre> <p>Output:</p> <pre><code>System-wide defaults:\n  tpm: 10,000/min (burst: 10,000)\n  rpm: 100/min (burst: 100)\n  on_unavailable: allow\n</code></pre>"},{"location":"cli/#system-delete-defaults","title":"system delete-defaults","text":"<p>Delete system-wide default limits.</p> <pre><code>zae-limiter system delete-defaults [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--name</code>, <code>-n</code> Stack identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint (LocalStack) None <code>--yes</code>, <code>-y</code> Skip confirmation prompt <code>false</code> <p>Examples:</p> <pre><code># Delete with confirmation\nzae-limiter system delete-defaults\n\n# Skip confirmation\nzae-limiter system delete-defaults --yes\n</code></pre>"},{"location":"cli/#entity","title":"entity","text":"<p>Manage entity-level limit configurations. These limits apply to a specific entity and resource combination, overriding both system and resource defaults.</p>"},{"location":"cli/#entity-set-limits","title":"entity set-limits","text":"<p>Set limits for a specific entity and resource.</p> <pre><code>zae-limiter entity set-limits &lt;ENTITY_ID&gt; [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Description <code>ENTITY_ID</code> The entity to configure (e.g., 'user-123', 'api-key-abc') <p>Options:</p> Option Description Default <code>--resource</code>, <code>-r</code> Resource name (required) - <code>--name</code>, <code>-n</code> Stack identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint (LocalStack) None <code>--limit</code>, <code>-l</code> Limit specification (required, repeatable) - <p>Limit format: <code>name:capacity[:burst]</code></p> <p>Examples:</p> <pre><code># Set premium user limits for gpt-4\nzae-limiter entity set-limits user-premium --resource gpt-4 -l tpm:100000 -l rpm:1000\n\n# Set API key limits with burst\nzae-limiter entity set-limits api-key-123 --resource claude-3 -l tpm:50000:75000\n</code></pre>"},{"location":"cli/#entity-get-limits","title":"entity get-limits","text":"<p>Get limits for a specific entity and resource.</p> <pre><code>zae-limiter entity get-limits &lt;ENTITY_ID&gt; [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Description <code>ENTITY_ID</code> The entity to query <p>Options:</p> Option Description Default <code>--resource</code>, <code>-r</code> Resource name (required) - <code>--name</code>, <code>-n</code> Stack identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint (LocalStack) None <p>Examples:</p> <pre><code># Get entity limits for gpt-4\nzae-limiter entity get-limits user-premium --resource gpt-4\n</code></pre> <p>Output:</p> <pre><code>Limits for entity 'user-premium' on resource 'gpt-4':\n  tpm: 100,000/min (burst: 100,000)\n  rpm: 1,000/min (burst: 1,000)\n</code></pre>"},{"location":"cli/#entity-delete-limits","title":"entity delete-limits","text":"<p>Delete limits for a specific entity and resource.</p> <pre><code>zae-limiter entity delete-limits &lt;ENTITY_ID&gt; [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Description <code>ENTITY_ID</code> The entity to delete limits from <p>Options:</p> Option Description Default <code>--resource</code>, <code>-r</code> Resource name (required) - <code>--name</code>, <code>-n</code> Stack identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom AWS endpoint (LocalStack) None <code>--yes</code>, <code>-y</code> Skip confirmation prompt <code>false</code> <p>Examples:</p> <pre><code># Delete with confirmation\nzae-limiter entity delete-limits user-premium --resource gpt-4\n\n# Skip confirmation\nzae-limiter entity delete-limits user-premium --resource gpt-4 --yes\n</code></pre>"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"<p>The CLI respects standard AWS environment variables:</p> Variable Description <code>AWS_ACCESS_KEY_ID</code> AWS access key <code>AWS_SECRET_ACCESS_KEY</code> AWS secret key <code>AWS_SESSION_TOKEN</code> AWS session token <code>AWS_DEFAULT_REGION</code> Default AWS region <code>AWS_PROFILE</code> AWS profile name <code>AWS_ENDPOINT_URL</code> Custom endpoint URL"},{"location":"cli/#exit-codes","title":"Exit Codes","text":"Code Description <code>0</code> Success <code>1</code> General error <code>2</code> Invalid arguments <code>3</code> AWS API error <code>4</code> Stack not found"},{"location":"cli/#next-steps","title":"Next Steps","text":"<ul> <li>Deployment - Deployment guide</li> <li>LocalStack - Local development</li> <li>API Reference - Python API documentation</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you install zae-limiter and set up rate limiting in your application.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"pipuvpoetryconda <pre><code>pip install zae-limiter\n</code></pre> <pre><code>uv pip install zae-limiter\n</code></pre> <pre><code>poetry add zae-limiter\n</code></pre> <pre><code>conda install -c conda-forge zae-limiter\n</code></pre>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":"<p>zae-limiter creates its own infrastructure automatically. Here's a complete example:</p>"},{"location":"getting-started/#async-api-recommended","title":"Async API (Recommended)","text":"<pre><code>from zae_limiter import RateLimiter, Limit, StackOptions, RateLimitExceeded\n\n# Create rate limiter with declarative infrastructure\nlimiter = RateLimiter(\n    name=\"my-app\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(),  # Declare desired state - CloudFormation ensures it\n)\n\ntry:\n    async with limiter.acquire(\n        entity_id=\"user-123\",\n        resource=\"api\",\n        limits=[Limit.per_minute(\"requests\", 100)],\n        consume={\"requests\": 1},\n    ) as lease:\n        await do_work()\nexcept RateLimitExceeded as e:\n    print(f\"Rate limited! Retry after {e.retry_after_seconds:.1f}s\")\n</code></pre>"},{"location":"getting-started/#sync-api","title":"Sync API","text":"<pre><code>from zae_limiter import SyncRateLimiter, Limit, StackOptions\n\nlimiter = SyncRateLimiter(\n    name=\"my-app\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(),\n)\n\nwith limiter.acquire(\n    entity_id=\"user-123\",\n    resource=\"api\",\n    limits=[Limit.per_minute(\"requests\", 100)],\n    consume={\"requests\": 1},\n) as lease:\n    response = call_api()\n</code></pre>"},{"location":"getting-started/#infrastructure-persistence","title":"Infrastructure Persistence","text":"<p>When you pass <code>stack_options=StackOptions()</code>, zae-limiter creates real AWS infrastructure via CloudFormation:</p> Resource Purpose Persists? DynamoDB Table Rate limit state, entities, usage Yes - until deleted Lambda Function Usage aggregation Yes - until deleted IAM Role Lambda permissions Yes - until deleted CloudWatch Logs Lambda logs Yes - with retention <p>Infrastructure Outlives Your Python Session</p> <p>This infrastructure persists beyond your Python session. Restarting your application reconnects to existing resources. Rate limit state is preserved across restarts. You only pay when the limiter is used (~$1/1M requests).</p>"},{"location":"getting-started/#infrastructure-lifecycle","title":"Infrastructure Lifecycle","text":"<p>Both programmatic API and CLI are fully supported for managing infrastructure.</p>"},{"location":"getting-started/#creating-infrastructure","title":"Creating Infrastructure","text":"ProgrammaticCLI <p>Pass <code>stack_options</code> to declare the desired infrastructure state:</p> <pre><code>limiter = RateLimiter(\n    name=\"my-app\",\n    stack_options=StackOptions(),  # Desired state declaration\n)\n</code></pre> <p>CloudFormation ensures the infrastructure matches your declaration.</p> <pre><code>zae-limiter deploy --name my-app --region us-east-1\n</code></pre> <p>Useful for: CI/CD pipelines, GitOps workflows, infrastructure-as-code.</p>"},{"location":"getting-started/#connecting-to-existing-infrastructure","title":"Connecting to Existing Infrastructure","text":"<p>If you omit <code>stack_options</code>, the limiter connects to existing infrastructure without attempting to create or modify it:</p> <pre><code># Connect only - fails if my-app stack doesn't exist\nlimiter = RateLimiter(\n    name=\"my-app\",\n    region=\"us-east-1\",\n    # No stack_options = connect only, no create/update\n)\n</code></pre> <p>This is useful when infrastructure is managed separately (e.g., via CLI or Terraform).</p> <p>Declarative State Management</p> <p><code>StackOptions</code> declares the desired infrastructure state. If multiple applications use the same limiter name with different settings, CloudFormation will update the stack to match the most recent declaration\u2014similar to how Terraform applies the last-written configuration.</p> <p>To maintain consistent state:</p> <ul> <li>Use identical <code>StackOptions</code> across all clients sharing a limiter</li> <li>Omit <code>stack_options</code> in application code and manage infrastructure externally</li> <li>Use different limiter names for different configurations</li> </ul>"},{"location":"getting-started/#checking-status","title":"Checking Status","text":"ProgrammaticCLI <pre><code>status = await limiter.get_status()  # Async\n# or\nstatus = limiter.get_status()  # Sync\n\nif not status.available:\n    print(\"DynamoDB not reachable\")\nelif status.stack_status == \"CREATE_COMPLETE\":\n    print(\"Stack is ready\")\n    print(f\"Latency: {status.latency_ms}ms\")\nelif status.stack_status and \"IN_PROGRESS\" in status.stack_status:\n    print(f\"Operation in progress: {status.stack_status}\")\nelif status.stack_status and \"FAILED\" in status.stack_status:\n    print(f\"Stack in failed state: {status.stack_status}\")\n</code></pre> <pre><code>zae-limiter status --name my-app --region us-east-1\n</code></pre>"},{"location":"getting-started/#deleting-infrastructure","title":"Deleting Infrastructure","text":"ProgrammaticCLI <pre><code># After you're done with the limiter\nawait limiter.delete_stack()  # Async\n# or\nlimiter.delete_stack()  # Sync\n</code></pre> <pre><code>zae-limiter delete --name my-app --region us-east-1 --yes\n</code></pre> <p>Data Loss</p> <p>Deleting infrastructure permanently removes all rate limit data, entity configurations, and usage history. This cannot be undone.</p> <p>Deployment Options</p> <p>For organizations requiring strict infrastructure/application separation, see CLI deployment or CloudFormation template export.</p>"},{"location":"getting-started/#understanding-limits","title":"Understanding Limits","text":"<p>Rate limiting in zae-limiter tracks who is making requests, what they're accessing, and how much they can use.</p>"},{"location":"getting-started/#the-core-concepts","title":"The Core Concepts","text":"<p>When you call <code>acquire()</code>, you specify:</p> <ul> <li><code>entity_id</code>: Who is being rate limited (e.g., <code>\"user-123\"</code>, <code>\"api-key-abc\"</code>, <code>\"tenant-xyz\"</code>)</li> <li><code>resource</code>: What they're accessing (e.g., <code>\"gpt-4\"</code>, <code>\"api\"</code>, <code>\"embeddings\"</code>)</li> <li><code>limits</code>: The rate limit rules to apply</li> <li><code>consume</code>: How much capacity this request uses</li> </ul> <p>Each entity has separate buckets per resource. A user rate limited on <code>\"gpt-4\"</code> can still access <code>\"gpt-3.5-turbo\"</code>:</p> <pre><code># User 123 accessing GPT-4 - tracked separately from GPT-3.5\nasync with limiter.acquire(\n    entity_id=\"user-123\",\n    resource=\"gpt-4\",        # Bucket: user-123 + gpt-4\n    limits=[Limit.per_minute(\"rpm\", 10)],\n    consume={\"rpm\": 1},\n) as lease:\n    ...\n\n# Same user, different resource - separate bucket\nasync with limiter.acquire(\n    entity_id=\"user-123\",\n    resource=\"gpt-3.5-turbo\",  # Bucket: user-123 + gpt-3.5-turbo\n    limits=[Limit.per_minute(\"rpm\", 100)],\n    consume={\"rpm\": 1},\n) as lease:\n    ...\n</code></pre>"},{"location":"getting-started/#defining-limits","title":"Defining Limits","text":"<p>A <code>Limit</code> defines a rate limit using the token bucket algorithm:</p> <pre><code># 100 requests per minute\nLimit.per_minute(\"rpm\", 100)\n\n# 10,000 tokens per minute with 15,000 burst capacity\nLimit.per_minute(\"tpm\", 10_000, burst=15_000)\n\n# 1,000 requests per hour\nLimit.per_hour(\"rph\", 1_000)\n\n# Custom: 50 requests per 30 seconds\nLimit.custom(\"requests\", capacity=50, refill_period_seconds=30)\n</code></pre> Parameter Description <code>name</code> Unique identifier (e.g., \"rpm\", \"tpm\") <code>capacity</code> Tokens that refill per period (sustained rate) <code>burst</code> Maximum bucket size (defaults to capacity) <p>See Token Bucket Algorithm for details on how capacity, burst, and refill work together.</p>"},{"location":"getting-started/#handling-rate-limit-errors","title":"Handling Rate Limit Errors","text":"<p>When a rate limit is exceeded, <code>RateLimitExceeded</code> is raised with full details:</p> <pre><code>from zae_limiter import RateLimitExceeded\n\ntry:\n    async with limiter.acquire(...):\n        await do_work()\nexcept RateLimitExceeded as e:\n    # Get retry delay\n    print(f\"Retry after: {e.retry_after_seconds}s\")\n\n    # For HTTP responses\n    return JSONResponse(\n        status_code=429,\n        content=e.as_dict(),\n        headers={\"Retry-After\": e.retry_after_header},\n    )\n</code></pre>"},{"location":"getting-started/#centralized-configuration-v050","title":"Centralized Configuration (v0.5.0+)","text":"<p>zae-limiter supports storing rate limit configurations in DynamoDB, eliminating the need to hardcode limits in application code.</p>"},{"location":"getting-started/#setting-up-defaults","title":"Setting Up Defaults","text":"<p>Configure limits at system and resource levels (typically done by admins during deployment):</p> <pre><code># Set system-wide defaults (applies to ALL resources)\nzae-limiter system set-defaults -l rpm:100 -l tpm:10000\n\n# Set resource-specific defaults (override system for this resource)\nzae-limiter resource set-defaults gpt-4 -l rpm:50 -l tpm:100000\nzae-limiter resource set-defaults gpt-3.5-turbo -l rpm:200 -l tpm:500000\n\n# Set entity-specific limits (premium users)\nzae-limiter entity set-limits user-premium --resource gpt-4 -l rpm:500 -l tpm:500000\n</code></pre>"},{"location":"getting-started/#automatic-resolution","title":"Automatic Resolution","text":"<p>With limits configured, application code becomes simpler\u2014no need to pass limits:</p> <pre><code># Limits are resolved automatically from stored config\nasync with limiter.acquire(\n    entity_id=\"user-123\",\n    resource=\"gpt-4\",\n    limits=None,  # Auto-resolves: Entity &gt; Resource &gt; System\n    consume={\"rpm\": 1},\n) as lease:\n    await call_api()\n</code></pre> <p>Resolution order (highest to lowest precedence):</p> <ol> <li>Entity level - Specific limits for entity+resource</li> <li>Resource level - Default limits for a resource</li> <li>System level - Global defaults for all resources</li> <li>Override parameter - Fallback if no stored config</li> </ol> <p>See Configuration Hierarchy for full details.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Usage - Multiple limits, adjustments, capacity queries</li> <li>Configuration Hierarchy - Three-tier limit resolution</li> <li>Hierarchical Limits - Parent/child entities, cascade mode</li> <li>LLM Integration - Token estimation and reconciliation</li> <li>Deployment Guide - Production deployment options</li> <li>CLI Reference - Full CLI command reference</li> </ul>"},{"location":"migrations/","title":"Migrations","text":"<p>This guide covers managing schema migrations for zae-limiter infrastructure.</p>"},{"location":"migrations/#overview","title":"Overview","text":"<p>Schema migrations handle changes to the DynamoDB table structure when upgrading between major versions of zae-limiter. The migration framework provides:</p> <ul> <li>Version tracking - Infrastructure version stored in DynamoDB</li> <li>Compatibility checking - Automatic client/infrastructure compatibility validation</li> <li>Migration registry - Ordered list of migrations between versions</li> <li>Rollback support - Optional rollback functions for reversible changes</li> </ul>"},{"location":"migrations/#when-migrations-are-needed","title":"When Migrations Are Needed","text":"Change Type Migration Required? Version Bump Add optional attribute No Patch (x.x.1) Add new index (GSI) Yes Minor (x.1.0) Change key structure Yes Major (1.0.0) Remove attribute Yes Major (1.0.0) Change attribute type Yes Major (1.0.0)"},{"location":"migrations/#version-compatibility","title":"Version Compatibility","text":"<p>zae-limiter uses semantic versioning with specific compatibility rules.</p>"},{"location":"migrations/#compatibility-rules","title":"Compatibility Rules","text":"<pre><code>Client 1.x.x + Schema 1.x.x = Compatible\nClient 2.x.x + Schema 1.x.x = Incompatible (migration required)\nClient 1.2.0 + Min Client 1.3.0 = Incompatible (upgrade client)\n</code></pre> <p>Major version mismatch: Always incompatible. Schema migration required before the client can operate.</p> <p>Minor/patch version mismatch: Compatible. Client and infrastructure can operate together.</p> <p>Minimum client version: Infrastructure can require a minimum client version. Older clients are rejected.</p>"},{"location":"migrations/#checking-compatibility","title":"Checking Compatibility","text":"<p>Use the CLI to check compatibility without modifying anything:</p> <pre><code># Check current compatibility status\nzae-limiter check --name limiter --region us-east-1\n</code></pre> <p>Output:</p> <pre><code>Compatibility Check\n====================\n\nClient:      1.2.0\nSchema:      1.0.0\nLambda:      1.1.0\n\nResult: COMPATIBLE (update available)\n\nLambda update available: 1.1.0 -&gt; 1.2.0\n\nRun 'zae-limiter upgrade' to update.\n</code></pre>"},{"location":"migrations/#viewing-version-information","title":"Viewing Version Information","text":"<pre><code># Show detailed version information\nzae-limiter version --name limiter --region us-east-1\n</code></pre> <p>Output:</p> <pre><code>zae-limiter Infrastructure Version\n====================================\n\nClient Version:     1.2.0\nSchema Version:     1.0.0\n\nInfra Schema:       1.0.0\nLambda Version:     1.1.0\nMin Client Version: 0.0.0\n\nStatus: COMPATIBLE (Lambda update available)\n\n  Lambda update available: 1.1.0 -&gt; 1.2.0\n\nRun 'zae-limiter upgrade' to update Lambda.\n</code></pre>"},{"location":"migrations/#upgrading-infrastructure","title":"Upgrading Infrastructure","text":"<p>For minor updates (Lambda code, no schema changes):</p> <pre><code># Upgrade Lambda to match client version\nzae-limiter upgrade --name limiter --region us-east-1\n</code></pre> <p>For major version upgrades requiring schema migration, see Sample Migration: v2.0.0.</p>"},{"location":"migrations/#schema-modification-approaches","title":"Schema Modification Approaches","text":"<p>When modifying the DynamoDB schema, prefer solutions that preserve backward compatibility.</p>"},{"location":"migrations/#non-breaking-changes-preferred","title":"Non-Breaking Changes (Preferred)","text":"<p>These changes don't require migrations:</p> <p>Adding optional attributes: <pre><code># Old code works - attribute simply missing\nitem = {\"PK\": \"ENTITY#123\", \"SK\": \"#META\", \"name\": \"test\"}\n\n# New code adds optional attribute\nitem = {\"PK\": \"ENTITY#123\", \"SK\": \"#META\", \"name\": \"test\", \"tags\": [\"prod\"]}\n</code></pre></p> <p>Using <code>if_not_exists()</code> for new nested structures: <pre><code># Initialize nested map only if missing\nupdate_expression = \"SET #data.#metrics = if_not_exists(#data.#metrics, :empty_map)\"\n</code></pre></p> <p>Adding conditional logic for missing attributes: <pre><code># Handle missing attribute gracefully\nmetrics = item.get(\"data\", {}).get(\"metrics\", {})\nrequest_count = metrics.get(\"requests\", 0)\n</code></pre></p>"},{"location":"migrations/#breaking-changes-major-version","title":"Breaking Changes (Major Version)","text":"<p>These changes require migrations and major version bumps:</p> <ul> <li>Changing partition or sort key structure</li> <li>Removing required attributes</li> <li>Changing attribute data types</li> <li>Renaming attributes</li> <li>Restructuring nested data</li> </ul>"},{"location":"migrations/#dynamodb-specific-considerations","title":"DynamoDB-Specific Considerations","text":"<p>GSI Changes:</p> <ul> <li>Adding a GSI: Can be done without migration (CloudFormation update)</li> <li>Removing a GSI: Requires ensuring no code depends on it</li> <li>Changing GSI keys: Requires data migration</li> </ul> <p>Key Pattern Changes:</p> <pre><code># v1.0.0 pattern\nPK = f\"ENTITY#{entity_id}\"\nSK = f\"#BUCKET#{resource}#{limit_name}\"\n\n# v2.0.0 pattern (breaking change!)\nPK = f\"ENT#{entity_id}\"  # Changed prefix\nSK = f\"BKT#{resource}#{limit_name}\"  # Changed prefix\n</code></pre> <p>Key pattern changes require migrating all existing data.</p>"},{"location":"migrations/#creating-a-migration","title":"Creating a Migration","text":""},{"location":"migrations/#migration-file-structure","title":"Migration File Structure","text":"<p>Create a new file in <code>src/zae_limiter/migrations/</code>:</p> <pre><code># src/zae_limiter/migrations/v1_1_0.py\n\"\"\"\nMigration: v1.1.0 (Add metrics tracking)\n\nThis migration adds a metrics attribute to entity metadata\nfor tracking request statistics.\n\nChanges:\n- Add 'metrics' map to entity #META records\n- Initialize with empty counters\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom . import Migration, register_migration\n\nif TYPE_CHECKING:\n    from ..repository import Repository\n\n\nasync def migrate_v1_1_0(repository: Repository) -&gt; None:\n    \"\"\"\n    Add metrics attribute to all entity metadata records.\n\n    This is a forward-only migration that initializes the\n    metrics structure for existing entities.\n    \"\"\"\n    # Query all entity metadata records\n    # Update each to add metrics if missing\n    # Use conditional updates to be idempotent\n    pass\n\n\nasync def rollback_v1_1_0(repository: Repository) -&gt; None:\n    \"\"\"\n    Remove metrics attribute from entity metadata.\n\n    Note: This loses all collected metrics data.\n    \"\"\"\n    # Remove metrics attribute from all entities\n    pass\n\n\n# Register the migration\nregister_migration(\n    Migration(\n        version=\"1.1.0\",\n        description=\"Add metrics tracking to entities\",\n        reversible=True,\n        migrate=migrate_v1_1_0,\n        rollback=rollback_v1_1_0,\n    )\n)\n</code></pre>"},{"location":"migrations/#migration-protocol","title":"Migration Protocol","text":"<p>Migration functions must follow this protocol:</p> <pre><code>async def __call__(self, repository: Repository) -&gt; None:\n    \"\"\"Execute the migration.\"\"\"\n    ...\n</code></pre> <p>Key requirements:</p> <ol> <li>Async: All migrations are async functions</li> <li>Repository parameter: Receives a configured Repository instance</li> <li>Idempotent: Safe to run multiple times (use conditional updates)</li> <li>Atomic where possible: Use transactions for related changes</li> </ol>"},{"location":"migrations/#registering-migrations","title":"Registering Migrations","text":"<p>Migrations are auto-registered when imported. Ensure your migration module is imported in <code>migrations/__init__.py</code>:</p> <pre><code># src/zae_limiter/migrations/__init__.py\n\n# ... existing code ...\n\n# Import built-in migrations to register them\nfrom . import v1_0_0 as _v1_0_0  # noqa: F401, E402\nfrom . import v1_1_0 as _v1_1_0  # noqa: F401, E402  # Add new migration\n</code></pre>"},{"location":"migrations/#updating-schema-version","title":"Updating Schema Version","text":"<p>After adding a migration, update the current schema version:</p> <pre><code># src/zae_limiter/version.py\n\n# Current schema version - increment when schema changes\nCURRENT_SCHEMA_VERSION = \"1.1.0\"  # Updated from \"1.0.0\"\n</code></pre>"},{"location":"migrations/#validating-migrations","title":"Validating Migrations","text":""},{"location":"migrations/#unit-testing-with-moto","title":"Unit Testing with Moto","text":"<p>Test migrations using moto for fast, isolated tests:</p> <pre><code># tests/test_migrations.py\nimport pytest\nfrom moto import mock_aws\n\nfrom zae_limiter.migrations import get_migrations_between, apply_migrations\nfrom zae_limiter.repository import Repository\n\n\n@pytest.fixture\ndef mock_dynamodb():\n    with mock_aws():\n        # Create table and seed test data\n        yield\n\n\n@pytest.mark.asyncio\nasync def test_migration_v1_1_0(mock_dynamodb):\n    \"\"\"Test v1.1.0 migration adds metrics to entities.\"\"\"\n    repo = Repository(\"test_table\", \"us-east-1\", None)\n\n    # Create test entity without metrics\n    await repo.create_entity(entity_id=\"test-1\", name=\"Test Entity\")\n\n    # Apply migration\n    applied = await apply_migrations(repo, \"1.0.0\", \"1.1.0\")\n\n    assert applied == [\"1.1.0\"]\n\n    # Verify metrics added - use raw DynamoDB query since Entity\n    # dataclass won't have the new 'metrics' field until code is updated\n    client = await repo._get_client()\n    response = await client.get_item(\n        TableName=repo.table_name,\n        Key={\"PK\": {\"S\": \"ENTITY#test-1\"}, \"SK\": {\"S\": \"#META\"}},\n    )\n    item = response[\"Item\"]\n    assert \"metrics\" in item[\"data\"][\"M\"]\n    assert item[\"data\"][\"M\"][\"metrics\"][\"M\"][\"requests\"][\"N\"] == \"0\"\n\n\n@pytest.mark.asyncio\nasync def test_migration_idempotent(mock_dynamodb):\n    \"\"\"Test migration can be safely run multiple times.\"\"\"\n    repo = Repository(\"test_table\", \"us-east-1\", None)\n\n    # Create test entity\n    await repo.create_entity(entity_id=\"test-1\", name=\"Test Entity\")\n\n    # Run migration twice\n    await apply_migrations(repo, \"1.0.0\", \"1.1.0\")\n    await apply_migrations(repo, \"1.0.0\", \"1.1.0\")  # Should not fail\n\n    # Verify single application via raw DynamoDB query\n    client = await repo._get_client()\n    response = await client.get_item(\n        TableName=repo.table_name,\n        Key={\"PK\": {\"S\": \"ENTITY#test-1\"}, \"SK\": {\"S\": \"#META\"}},\n    )\n    item = response[\"Item\"]\n    assert item[\"data\"][\"M\"][\"metrics\"][\"M\"][\"requests\"][\"N\"] == \"0\"\n</code></pre>"},{"location":"migrations/#integration-testing-with-localstack","title":"Integration Testing with LocalStack","text":"<p>Test migrations against real AWS-compatible infrastructure:</p> <pre><code># tests/test_migrations_integration.py\nimport os\nimport pytest\n\n# Skip if LocalStack not available\npytestmark = pytest.mark.skipif(\n    not os.environ.get(\"AWS_ENDPOINT_URL\"),\n    reason=\"LocalStack not available\"\n)\n\n\n@pytest.mark.integration\n@pytest.mark.asyncio\nasync def test_migration_with_localstack():\n    \"\"\"Test migration against LocalStack.\"\"\"\n    endpoint_url = os.environ[\"AWS_ENDPOINT_URL\"]\n\n    repo = Repository(\n        \"test_migrations\",\n        \"us-east-1\",\n        endpoint_url,\n    )\n\n    # Deploy infrastructure\n    # ... create table with CloudFormation ...\n\n    # Seed test data\n    # ... create entities ...\n\n    # Apply migration\n    applied = await apply_migrations(repo, \"1.0.0\", \"1.1.0\")\n\n    # Verify changes persisted\n    # ... assertions ...\n</code></pre> <p>Run integration tests:</p> <pre><code># Start LocalStack (from project root)\ndocker compose up -d\n\n# Set environment variables and run integration tests\nexport AWS_ENDPOINT_URL=http://localhost:4566\nexport AWS_ACCESS_KEY_ID=test\nexport AWS_SECRET_ACCESS_KEY=test\nexport AWS_DEFAULT_REGION=us-east-1\npytest -m integration -v\n</code></pre>"},{"location":"migrations/#production-validation-checklist","title":"Production Validation Checklist","text":"<p>Before running migrations in production:</p> <ul> <li> Backup: Enable Point-in-Time Recovery (PITR) or create on-demand backup</li> <li> Test: Run migration against production data copy</li> <li> Monitor: Set up CloudWatch alarms for errors</li> <li> Rollback plan: Document rollback procedure</li> <li> Maintenance window: Schedule during low-traffic period</li> <li> Communication: Notify stakeholders of potential downtime</li> </ul> <pre><code># Create on-demand backup before migration\naws dynamodb create-backup \\\n  --table-name limiter \\\n  --backup-name \"pre-migration-$(date +%Y%m%d)\"\n\n# Verify PITR is enabled\naws dynamodb describe-continuous-backups \\\n  --table-name limiter\n</code></pre>"},{"location":"migrations/#rollback-strategies","title":"Rollback Strategies","text":""},{"location":"migrations/#reversible-migrations","title":"Reversible Migrations","text":"<p>For migrations that can be safely undone:</p> <pre><code>register_migration(\n    Migration(\n        version=\"1.1.0\",\n        description=\"Add metrics tracking\",\n        reversible=True,  # Can be rolled back\n        migrate=migrate_v1_1_0,\n        rollback=rollback_v1_1_0,  # Rollback function\n    )\n)\n</code></pre> <p>Rollback removes or reverts the changes:</p> <pre><code>async def rollback_v1_1_0(repository: Repository) -&gt; None:\n    \"\"\"Remove metrics attribute from all entities.\"\"\"\n    # Implementation to remove metrics attribute\n    pass\n</code></pre>"},{"location":"migrations/#forward-only-migrations","title":"Forward-Only Migrations","text":"<p>Some migrations cannot be reversed:</p> <pre><code>register_migration(\n    Migration(\n        version=\"2.0.0\",\n        description=\"Restructure key patterns\",\n        reversible=False,  # Cannot be rolled back\n        migrate=migrate_v2_0_0,\n        rollback=None,  # No rollback function\n    )\n)\n</code></pre> <p>Forward-only migrations typically involve:</p> <ul> <li>Data transformation with information loss</li> <li>Key structure changes</li> <li>Removing deprecated attributes</li> </ul>"},{"location":"migrations/#emergency-rollback-procedures","title":"Emergency Rollback Procedures","text":"<p>If a migration fails or causes issues:</p> <p>1. Stop the bleeding: <pre><code># Revert to previous client version\npip install zae-limiter==1.0.0\n</code></pre></p> <p>2. Restore from backup (if needed): <pre><code># Restore from PITR\naws dynamodb restore-table-to-point-in-time \\\n  --source-table-name limiter \\\n  --target-table-name limiter-restored \\\n  --restore-date-time \"2024-01-15T10:00:00Z\"\n</code></pre></p> <p>3. Run rollback (if reversible): <pre><code>from zae_limiter.migrations import get_migrations\nfrom zae_limiter.repository import Repository\n\nasync def emergency_rollback():\n    repo = Repository(\"limiter\", \"us-east-1\", None)\n\n    migrations = get_migrations()\n    target_migration = next(m for m in migrations if m.version == \"1.1.0\")\n\n    if target_migration.reversible and target_migration.rollback:\n        await target_migration.rollback(repo)\n        print(\"Rollback complete\")\n    else:\n        print(\"Migration is not reversible - restore from backup\")\n</code></pre></p> <p>4. Update version record: <pre><code>await repo.set_version_record(\n    schema_version=\"1.0.0\",  # Reverted version\n    lambda_version=\"1.0.0\",\n    updated_by=\"emergency_rollback\",\n)\n</code></pre></p>"},{"location":"migrations/#sample-migration-v200","title":"Sample Migration: v2.0.0","text":"<p>This example demonstrates a complete migration scenario for a hypothetical v2.0.0 release that adds a new Global Secondary Index for querying entities by creation date.</p>"},{"location":"migrations/#scenario","title":"Scenario","text":"<p>Goal: Add ability to query entities by creation timestamp for audit purposes.</p> <p>Changes: 1. Add <code>created_at</code> attribute to entity metadata 2. Add GSI3 for querying by creation date 3. Backfill <code>created_at</code> for existing entities</p>"},{"location":"migrations/#migration-implementation","title":"Migration Implementation","text":"<pre><code># src/zae_limiter/migrations/v2_0_0.py\n\"\"\"\nMigration: v2.0.0 (Add creation timestamp tracking)\n\nThis migration adds a created_at timestamp to all entities and\ncreates a new GSI for querying entities by creation date.\n\nSchema changes:\n- Add 'created_at' attribute to entity #META records\n- Add GSI3: GSI3PK=CREATED#{YYYY-MM}, GSI3SK=ENTITY#{id}\n\nBreaking changes:\n- Requires CloudFormation stack update for GSI3\n- All queries using GSI3 require v2.0.0+ client\n\nRollback:\n- This migration is NOT reversible (GSI removal loses query capability)\n- Restore from backup if rollback needed\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom datetime import datetime, timezone\nfrom typing import TYPE_CHECKING\n\nfrom . import Migration, register_migration\n\nif TYPE_CHECKING:\n    from ..repository import Repository\n\n\nasync def migrate_v2_0_0(repository: Repository) -&gt; None:\n    \"\"\"\n    Add created_at timestamp to all existing entities.\n\n    Note: GSI3 must be added via CloudFormation update before\n    running this migration. The migration only backfills data.\n\n    Steps:\n    1. Scan all entity metadata records\n    2. Add created_at if missing (set to migration timestamp)\n    3. Add GSI3 keys for index population\n    \"\"\"\n    migration_time = datetime.now(timezone.utc).isoformat()\n    migration_month = datetime.now(timezone.utc).strftime(\"%Y-%m\")\n\n    # Get DynamoDB client\n    client = await repository._get_client()\n\n    # Scan for all entity metadata records\n    paginator = client.get_paginator('scan')\n\n    async for page in paginator.paginate(\n        TableName=repository.table_name,\n        FilterExpression=\"begins_with(SK, :meta)\",\n        ExpressionAttributeValues={\":meta\": {\"S\": \"#META\"}},\n    ):\n        items = page.get(\"Items\", [])\n\n        # Process in batches of 25 (DynamoDB limit)\n        for i in range(0, len(items), 25):\n            batch = items[i:i + 25]\n\n            # Build batch update\n            update_requests = []\n            for item in batch:\n                pk = item[\"PK\"][\"S\"]\n                sk = item[\"SK\"][\"S\"]\n\n                # Use conditional update - only if created_at missing\n                update_requests.append({\n                    \"Update\": {\n                        \"TableName\": repository.table_name,\n                        \"Key\": {\"PK\": {\"S\": pk}, \"SK\": {\"S\": sk}},\n                        \"UpdateExpression\": (\n                            \"SET #created = if_not_exists(#created, :ts), \"\n                            \"GSI3PK = if_not_exists(GSI3PK, :gsi3pk), \"\n                            \"GSI3SK = if_not_exists(GSI3SK, :gsi3sk)\"\n                        ),\n                        \"ExpressionAttributeNames\": {\n                            \"#created\": \"created_at\",\n                        },\n                        \"ExpressionAttributeValues\": {\n                            \":ts\": {\"S\": migration_time},\n                            \":gsi3pk\": {\"S\": f\"CREATED#{migration_month}\"},\n                            \":gsi3sk\": {\"S\": pk},\n                        },\n                    }\n                })\n\n            # Execute batch (transactions limited to 100 items)\n            if update_requests:\n                await client.transact_write_items(\n                    TransactItems=update_requests\n                )\n\n        # Rate limiting to avoid throttling\n        await asyncio.sleep(0.1)\n\n    # Update version record\n    await repository.set_version_record(\n        schema_version=\"2.0.0\",\n        client_min_version=\"2.0.0\",  # Require v2.0.0+ clients\n        updated_by=\"migration:v2.0.0\",\n    )\n\n\n# Register the migration\nregister_migration(\n    Migration(\n        version=\"2.0.0\",\n        description=\"Add creation timestamp tracking with GSI3\",\n        reversible=False,  # GSI changes are not easily reversible\n        migrate=migrate_v2_0_0,\n        rollback=None,\n    )\n)\n</code></pre>"},{"location":"migrations/#cloudformation-update","title":"CloudFormation Update","text":"<p>The GSI must be added before running the migration:</p> <pre><code># Addition to cfn_template.yaml\nResources:\n  RateLimitsTable:\n    Type: AWS::DynamoDB::Table\n    Properties:\n      # ... existing properties ...\n      GlobalSecondaryIndexes:\n        # ... existing GSIs ...\n        - IndexName: GSI3\n          KeySchema:\n            - AttributeName: GSI3PK\n              KeyType: HASH\n            - AttributeName: GSI3SK\n              KeyType: RANGE\n          Projection:\n            ProjectionType: ALL\n      AttributeDefinitions:\n        # ... existing attributes ...\n        - AttributeName: GSI3PK\n          AttributeType: S\n        - AttributeName: GSI3SK\n          AttributeType: S\n</code></pre>"},{"location":"migrations/#running-the-migration","title":"Running the Migration","text":"<pre><code># 1. Create backup\naws dynamodb create-backup \\\n  --table-name limiter \\\n  --backup-name \"pre-v2-migration-$(date +%Y%m%d)\"\n\n# 2. Update CloudFormation stack (adds GSI3)\naws cloudformation update-stack \\\n  --stack-name limiter \\\n  --template-body file://updated-template.yaml \\\n  --capabilities CAPABILITY_NAMED_IAM\n\n# 3. Wait for GSI to be active\naws dynamodb wait table-exists --table-name limiter\n\n# 4. Install new client version\npip install zae-limiter==2.0.0\n\n# 5. Run migration (via upgrade command or programmatically)\npython -c \"\nimport asyncio\nfrom zae_limiter.migrations import apply_migrations\nfrom zae_limiter.repository import Repository\n\nasync def run():\n    repo = Repository('limiter', 'us-east-1', None)\n    applied = await apply_migrations(repo, '1.0.0', '2.0.0')\n    print(f'Applied migrations: {applied}')\n    await repo.close()\n\nasyncio.run(run())\n\"\n\n# 6. Verify migration\nzae-limiter version --name limiter --region us-east-1\n</code></pre>"},{"location":"migrations/#testing-the-migration","title":"Testing the Migration","text":"<pre><code>@pytest.mark.asyncio\nasync def test_v2_migration_adds_created_at(mock_dynamodb):\n    \"\"\"Test v2.0.0 migration adds created_at to entities.\"\"\"\n    repo = Repository(\"test_table\", \"us-east-1\", None)\n\n    # Create entities without created_at (v1 schema)\n    await repo.create_entity(entity_id=\"entity-1\", name=\"Test 1\")\n    await repo.create_entity(entity_id=\"entity-2\", name=\"Test 2\")\n\n    # Verify no created_at before migration\n    entity = await repo.get_entity(\"entity-1\")\n    assert entity.created_at is None\n\n    # Apply migration\n    applied = await apply_migrations(repo, \"1.0.0\", \"2.0.0\")\n    assert applied == [\"2.0.0\"]\n\n    # Verify created_at added - check raw DynamoDB for GSI3 keys\n    # (Entity dataclass has created_at, but GSI3PK/GSI3SK are schema-level)\n    client = await repo._get_client()\n    response = await client.get_item(\n        TableName=repo.table_name,\n        Key={\"PK\": {\"S\": \"ENTITY#entity-1\"}, \"SK\": {\"S\": \"#META\"}},\n    )\n    item = response[\"Item\"]\n    assert \"created_at\" in item\n    assert item[\"created_at\"][\"S\"].startswith(\"20\")  # Valid ISO timestamp\n    assert \"GSI3PK\" in item  # GSI key was added\n    assert item[\"GSI3PK\"][\"S\"].startswith(\"CREATED#\")\n\n\n@pytest.mark.asyncio\nasync def test_v2_migration_idempotent(mock_dynamodb):\n    \"\"\"Test v2.0.0 migration is idempotent.\"\"\"\n    repo = Repository(\"test_table\", \"us-east-1\", None)\n\n    # Create entity with created_at already set\n    # (simulating an entity created after v2.0.0 code deployed)\n    original_time = \"2024-01-01T00:00:00Z\"\n    client = await repo._get_client()\n    await client.put_item(\n        TableName=repo.table_name,\n        Item={\n            \"PK\": {\"S\": \"ENTITY#entity-1\"},\n            \"SK\": {\"S\": \"#META\"},\n            \"data\": {\"M\": {\"name\": {\"S\": \"Test\"}}},\n            \"created_at\": {\"S\": original_time},\n            \"GSI3PK\": {\"S\": \"CREATED#2024-01\"},\n            \"GSI3SK\": {\"S\": \"ENTITY#entity-1\"},\n        },\n    )\n\n    # Apply migration - should not overwrite existing values\n    await apply_migrations(repo, \"1.0.0\", \"2.0.0\")\n\n    # Verify original created_at preserved (if_not_exists)\n    response = await client.get_item(\n        TableName=repo.table_name,\n        Key={\"PK\": {\"S\": \"ENTITY#entity-1\"}, \"SK\": {\"S\": \"#META\"}},\n    )\n    item = response[\"Item\"]\n    assert item[\"created_at\"][\"S\"] == original_time\n</code></pre>"},{"location":"migrations/#reference","title":"Reference","text":""},{"location":"migrations/#version-record-structure","title":"Version Record Structure","text":"<p>The version record is stored in DynamoDB:</p> Attribute Value Description PK <code>SYSTEM#</code> Partition key SK <code>#VERSION</code> Sort key schema_version <code>\"1.0.0\"</code> Current schema version lambda_version <code>\"1.2.0\"</code> Deployed Lambda version client_min_version <code>\"1.0.0\"</code> Minimum client version updated_at ISO timestamp Last update time updated_by <code>\"cli:1.2.0\"</code> What performed the update"},{"location":"migrations/#key-patterns-v100","title":"Key Patterns (v1.0.0)","text":"Pattern Example Description Entity metadata <code>PK=ENTITY#123, SK=#META</code> Entity configuration Bucket state <code>PK=ENTITY#123, SK=#BUCKET#gpt-4#rpm</code> Token bucket state Limit config <code>PK=ENTITY#123, SK=#LIMIT#gpt-4#rpm</code> Stored limit config Usage snapshot <code>PK=ENTITY#123, SK=#USAGE#gpt-4#2024-01-15</code> Usage data Version <code>PK=SYSTEM#, SK=#VERSION</code> Infrastructure version"},{"location":"migrations/#migration-api-reference","title":"Migration API Reference","text":"<pre><code>from zae_limiter.migrations import (\n    Migration,           # Migration dataclass\n    register_migration,  # Register a migration\n    get_migrations,      # Get all registered migrations\n    get_migrations_between,  # Get migrations between versions\n    apply_migrations,    # Apply migrations\n)\n\nfrom zae_limiter.version import (\n    CURRENT_SCHEMA_VERSION,  # Current schema version constant\n    parse_version,           # Parse version string\n    check_compatibility,     # Check client/infra compatibility\n    get_schema_version,      # Get current schema version\n    InfrastructureVersion,   # Version info dataclass\n    CompatibilityResult,     # Compatibility check result\n)\n</code></pre>"},{"location":"migrations/#cli-commands","title":"CLI Commands","text":"<pre><code># Check compatibility\nzae-limiter check --name NAME --region REGION\n\n# Show version information\nzae-limiter version --name NAME --region REGION\n\n# Upgrade infrastructure\nzae-limiter upgrade --name NAME --region REGION [--lambda-only] [--force]\n</code></pre>"},{"location":"monitoring/","title":"Monitoring and Observability","text":"<p>This guide covers monitoring and observability practices for zae-limiter deployments, including structured logging, CloudWatch metrics, alerts, and dashboard templates.</p>"},{"location":"monitoring/#overview","title":"Overview","text":"<p>Effective monitoring of a rate limiter is critical for:</p> <ul> <li>Availability - Detecting service degradation before users are impacted</li> <li>Latency - Ensuring rate limit checks don't become a bottleneck</li> <li>Throughput - Understanding capacity and scaling needs</li> <li>Errors - Identifying and resolving issues quickly</li> </ul> <p>zae-limiter provides built-in observability through:</p> Component Purpose CloudWatch Alarms Proactive alerting on anomalies Structured Logs JSON-formatted logs for analysis Dead Letter Queue Capturing failed events for investigation Usage Snapshots Aggregated consumption metrics Audit Logging Security and compliance tracking <p>Compliance Requirements</p> <p>For tracking who changed what and when, see the Audit Logging Guide.</p> <p>Usage Data</p> <p>For querying historical consumption data (billing, capacity planning), see the Usage Snapshots Guide.</p>"},{"location":"monitoring/#structured-logging","title":"Structured Logging","text":"<p>The Lambda aggregator uses structured JSON logging compatible with CloudWatch Logs Insights.</p>"},{"location":"monitoring/#log-format","title":"Log Format","text":"<p>All log entries follow this JSON structure:</p> <pre><code>{\n  \"timestamp\": \"2024-01-15T10:30:00.000000+00:00\",\n  \"level\": \"INFO\",\n  \"logger\": \"zae_limiter.aggregator.handler\",\n  \"message\": \"Lambda invocation completed\",\n  \"request_id\": \"abc123-def456\",\n  \"processed\": 50,\n  \"snapshots_updated\": 100,\n  \"processing_time_ms\": 45.23\n}\n</code></pre>"},{"location":"monitoring/#log-fields-reference","title":"Log Fields Reference","text":"Field Type Description <code>timestamp</code> string ISO 8601 timestamp (UTC) <code>level</code> string Log level: DEBUG, INFO, WARNING, ERROR <code>logger</code> string Logger name (module path) <code>message</code> string Human-readable message <code>request_id</code> string Lambda request ID for correlation <code>function_name</code> string Lambda function name <code>record_count</code> int DynamoDB stream records in batch <code>processed</code> int Records successfully processed <code>deltas_extracted</code> int Consumption deltas found <code>snapshots_updated</code> int Usage snapshots updated <code>error_count</code> int Processing errors <code>processing_time_ms</code> float Total execution time (ms)"},{"location":"monitoring/#log-levels","title":"Log Levels","text":"Level When Used DEBUG Detailed processing info (snapshot updates) INFO Invocation start/end, batch processing summary WARNING Recoverable errors (single record failures) ERROR Unrecoverable errors (batch failures)"},{"location":"monitoring/#example-log-entries","title":"Example Log Entries","text":"<p>Invocation Start: <pre><code>{\n  \"timestamp\": \"2024-01-15T10:30:00.000000+00:00\",\n  \"level\": \"INFO\",\n  \"logger\": \"zae_limiter.aggregator.handler\",\n  \"message\": \"Lambda invocation started\",\n  \"request_id\": \"abc123-def456\",\n  \"function_name\": \"limiter-aggregator\",\n  \"record_count\": 50,\n  \"table_name\": \"limiter\",\n  \"snapshot_windows\": [\"hourly\", \"daily\"]\n}\n</code></pre></p> <p>Batch Complete: <pre><code>{\n  \"timestamp\": \"2024-01-15T10:30:00.500000+00:00\",\n  \"level\": \"INFO\",\n  \"logger\": \"zae_limiter.aggregator.processor\",\n  \"message\": \"Batch processing completed\",\n  \"processed_count\": 50,\n  \"deltas_extracted\": 45,\n  \"snapshots_updated\": 90,\n  \"error_count\": 0,\n  \"processing_time_ms\": 423.15\n}\n</code></pre></p> <p>Error with Exception: <pre><code>{\n  \"timestamp\": \"2024-01-15T10:30:01.000000+00:00\",\n  \"level\": \"ERROR\",\n  \"logger\": \"zae_limiter.aggregator.processor\",\n  \"message\": \"Error processing record\",\n  \"record_index\": 12,\n  \"exception\": \"Traceback (most recent call last):\\n...\"\n}\n</code></pre></p>"},{"location":"monitoring/#cloudwatch-metrics","title":"CloudWatch Metrics","text":""},{"location":"monitoring/#lambda-metrics","title":"Lambda Metrics","text":"<p>Monitor the aggregator Lambda function:</p> Metric Namespace Description Recommended Threshold <code>Invocations</code> AWS/Lambda Total executions Baseline + 50% <code>Errors</code> AWS/Lambda Failed executions &gt; 1 per 5 min <code>Duration</code> AWS/Lambda Execution time (ms) &gt; 80% of timeout <code>Throttles</code> AWS/Lambda Throttled invocations &gt; 0 <code>IteratorAge</code> AWS/Lambda Stream processing lag (ms) &gt; 30,000 ms <code>ConcurrentExecutions</code> AWS/Lambda Parallel executions Account limit"},{"location":"monitoring/#dynamodb-metrics","title":"DynamoDB Metrics","text":"<p>Monitor table performance:</p> Metric Namespace Description Recommended Threshold <code>ConsumedReadCapacityUnits</code> AWS/DynamoDB RCU usage Provisioned capacity <code>ConsumedWriteCapacityUnits</code> AWS/DynamoDB WCU usage Provisioned capacity <code>ReadThrottleEvents</code> AWS/DynamoDB Read throttles &gt; 0 <code>WriteThrottleEvents</code> AWS/DynamoDB Write throttles &gt; 0 <code>SystemErrors</code> AWS/DynamoDB Service errors &gt; 0 <code>SuccessfulRequestLatency</code> AWS/DynamoDB Request latency (ms) p99 &gt; 100ms"},{"location":"monitoring/#sqs-metrics-dead-letter-queue","title":"SQS Metrics (Dead Letter Queue)","text":"<p>Monitor failed event processing:</p> Metric Namespace Description Recommended Threshold <code>ApproximateNumberOfMessagesVisible</code> AWS/SQS Messages in DLQ &gt; 0 <code>ApproximateAgeOfOldestMessage</code> AWS/SQS Oldest message age (s) &gt; 3600"},{"location":"monitoring/#cloudwatch-logs-insights-queries","title":"CloudWatch Logs Insights Queries","text":""},{"location":"monitoring/#batch-processing-performance","title":"Batch Processing Performance","text":"<p>Analyze processing latency over time:</p> <pre><code>fields @timestamp, @message\n| filter @message like /Batch processing completed/\n| parse @message /processing_time_ms\":(?&lt;duration&gt;[\\d.]+)/\n| stats avg(duration) as avg_ms,\n        pct(duration, 50) as p50_ms,\n        pct(duration, 95) as p95_ms,\n        pct(duration, 99) as p99_ms\n  by bin(1h)\n| sort @timestamp desc\n</code></pre>"},{"location":"monitoring/#error-analysis","title":"Error Analysis","text":"<p>Find recent errors and warnings:</p> <pre><code>fields @timestamp, @message, @logStream\n| filter level = \"ERROR\" or level = \"WARNING\"\n| parse @message /message\":\"(?&lt;error_message&gt;[^\"]+)/\n| sort @timestamp desc\n| limit 100\n</code></pre>"},{"location":"monitoring/#invocation-summary","title":"Invocation Summary","text":"<p>Aggregate processing metrics:</p> <pre><code>fields @timestamp, @message\n| filter @message like /Lambda invocation completed/\n| parse @message /processed\":(?&lt;processed&gt;\\d+).*snapshots_updated\":(?&lt;snapshots&gt;\\d+)/\n| stats sum(processed) as total_processed,\n        sum(snapshots) as total_snapshots,\n        count() as invocations\n  by bin(1h)\n| sort @timestamp desc\n</code></pre>"},{"location":"monitoring/#entity-usage-analysis","title":"Entity Usage Analysis","text":"<p>Find highest-usage entities:</p> <pre><code>fields @timestamp, @message\n| filter @message like /Snapshot updated/\n| parse @message /entity_id\":\"(?&lt;entity&gt;[^\"]+)\".*resource\":\"(?&lt;resource&gt;[^\"]+)/\n| stats count() as updates by entity, resource\n| sort updates desc\n| limit 50\n</code></pre>"},{"location":"monitoring/#cold-start-detection","title":"Cold Start Detection","text":"<p>Identify Lambda cold starts:</p> <pre><code>fields @timestamp, @message, @duration\n| filter @type = \"REPORT\"\n| filter @message like /Init Duration/\n| parse @message /Init Duration: (?&lt;init_duration&gt;[\\d.]+) ms/\n| stats count() as cold_starts,\n        avg(init_duration) as avg_init_ms\n  by bin(1h)\n</code></pre>"},{"location":"monitoring/#error-rate-calculation","title":"Error Rate Calculation","text":"<p>Calculate error rate percentage:</p> <pre><code>fields @timestamp\n| filter @message like /Lambda invocation/\n| parse @message /error_count\":(?&lt;errors&gt;\\d+)/\n| stats sum(errors) as total_errors, count() as total_invocations\n| display total_errors, total_invocations,\n         (total_errors * 100.0 / total_invocations) as error_rate_pct\n</code></pre>"},{"location":"monitoring/#x-ray-tracing","title":"X-Ray Tracing","text":"<p>AWS X-Ray tracing is available for the Lambda aggregator function. When enabled, traces are automatically captured for Lambda invocations, providing visibility into stream processing performance.</p>"},{"location":"monitoring/#enabling-x-ray","title":"Enabling X-Ray","text":"<p>X-Ray tracing is opt-in to avoid unexpected costs:</p> <pre><code># Enable tracing via CLI\nzae-limiter deploy --name my-app --enable-tracing\n\n# Or programmatically\nfrom zae_limiter import RateLimiter, StackOptions\n\nlimiter = RateLimiter(\n    name=\"my-app\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(\n        enable_tracing=True,\n    ),\n)\n</code></pre>"},{"location":"monitoring/#whats-traced-phase-1","title":"What's Traced (Phase 1)","text":"<ul> <li>Lambda Active Tracing - End-to-end request visibility for aggregator invocations</li> <li>Automatic segments - AWS SDK calls (DynamoDB, S3) are automatically instrumented</li> </ul>"},{"location":"monitoring/#future-enhancements","title":"Future Enhancements","text":"<p>Track progress on additional X-Ray features in Issue #107:</p> <ul> <li>DynamoDB SDK instrumentation for client-side operations</li> <li>Custom subsegments for acquire/release operations</li> <li>Trace header propagation for cross-service correlation</li> </ul>"},{"location":"monitoring/#dashboard-templates","title":"Dashboard Templates","text":""},{"location":"monitoring/#operations-dashboard","title":"Operations Dashboard","text":"<p>Create a CloudWatch dashboard for day-to-day operations:</p> <pre><code>{\n  \"widgets\": [\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"title\": \"Lambda Invocations &amp; Errors\",\n        \"region\": \"${AWS::Region}\",\n        \"metrics\": [\n          [\"AWS/Lambda\", \"Invocations\", \"FunctionName\", \"${TableName}-aggregator\", {\"stat\": \"Sum\"}],\n          [\".\", \"Errors\", \".\", \".\", {\"stat\": \"Sum\", \"color\": \"#d62728\"}]\n        ],\n        \"period\": 300,\n        \"view\": \"timeSeries\"\n      }\n    },\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"title\": \"Lambda Duration (p50/p95/p99)\",\n        \"region\": \"${AWS::Region}\",\n        \"metrics\": [\n          [\"AWS/Lambda\", \"Duration\", \"FunctionName\", \"${TableName}-aggregator\", {\"stat\": \"p50\"}],\n          [\"...\", {\"stat\": \"p95\"}],\n          [\"...\", {\"stat\": \"p99\"}]\n        ],\n        \"period\": 300,\n        \"view\": \"timeSeries\"\n      }\n    },\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"title\": \"Stream Iterator Age\",\n        \"region\": \"${AWS::Region}\",\n        \"metrics\": [\n          [\"AWS/Lambda\", \"IteratorAge\", \"FunctionName\", \"${TableName}-aggregator\", {\"stat\": \"Maximum\"}]\n        ],\n        \"period\": 60,\n        \"view\": \"timeSeries\",\n        \"annotations\": {\n          \"horizontal\": [{\"value\": 30000, \"label\": \"Threshold (30s)\"}]\n        }\n      }\n    },\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"title\": \"DynamoDB Capacity\",\n        \"region\": \"${AWS::Region}\",\n        \"metrics\": [\n          [\"AWS/DynamoDB\", \"ConsumedReadCapacityUnits\", \"TableName\", \"${TableName}\", {\"stat\": \"Sum\"}],\n          [\".\", \"ConsumedWriteCapacityUnits\", \".\", \".\", {\"stat\": \"Sum\"}]\n        ],\n        \"period\": 300,\n        \"view\": \"timeSeries\"\n      }\n    },\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"title\": \"DynamoDB Throttles\",\n        \"region\": \"${AWS::Region}\",\n        \"metrics\": [\n          [\"AWS/DynamoDB\", \"ReadThrottleEvents\", \"TableName\", \"${TableName}\", {\"stat\": \"Sum\"}],\n          [\".\", \"WriteThrottleEvents\", \".\", \".\", {\"stat\": \"Sum\"}]\n        ],\n        \"period\": 300,\n        \"view\": \"timeSeries\"\n      }\n    },\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"title\": \"Dead Letter Queue\",\n        \"region\": \"${AWS::Region}\",\n        \"metrics\": [\n          [\"AWS/SQS\", \"ApproximateNumberOfMessagesVisible\", \"QueueName\", \"${TableName}-aggregator-dlq\"]\n        ],\n        \"period\": 60,\n        \"view\": \"singleValue\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"monitoring/#capacity-planning-dashboard","title":"Capacity Planning Dashboard","text":"<p>Create a dashboard for capacity analysis:</p> <pre><code>{\n  \"widgets\": [\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"title\": \"RCU/WCU Consumption Trend\",\n        \"region\": \"${AWS::Region}\",\n        \"metrics\": [\n          [\"AWS/DynamoDB\", \"ConsumedReadCapacityUnits\", \"TableName\", \"${TableName}\", {\"stat\": \"Sum\", \"period\": 3600}],\n          [\".\", \"ConsumedWriteCapacityUnits\", \".\", \".\", {\"stat\": \"Sum\", \"period\": 3600}]\n        ],\n        \"view\": \"timeSeries\",\n        \"stacked\": false\n      }\n    },\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"title\": \"Request Latency Distribution\",\n        \"region\": \"${AWS::Region}\",\n        \"metrics\": [\n          [\"AWS/DynamoDB\", \"SuccessfulRequestLatency\", \"TableName\", \"${TableName}\", \"Operation\", \"GetItem\", {\"stat\": \"p50\"}],\n          [\"...\", {\"stat\": \"p95\"}],\n          [\"...\", {\"stat\": \"p99\"}]\n        ],\n        \"period\": 300,\n        \"view\": \"timeSeries\"\n      }\n    },\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"title\": \"Lambda Concurrent Executions\",\n        \"region\": \"${AWS::Region}\",\n        \"metrics\": [\n          [\"AWS/Lambda\", \"ConcurrentExecutions\", \"FunctionName\", \"${TableName}-aggregator\", {\"stat\": \"Maximum\"}]\n        ],\n        \"period\": 60,\n        \"view\": \"timeSeries\"\n      }\n    },\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"title\": \"Throttle Events (7 Day)\",\n        \"region\": \"${AWS::Region}\",\n        \"metrics\": [\n          [\"AWS/DynamoDB\", \"ReadThrottleEvents\", \"TableName\", \"${TableName}\", {\"stat\": \"Sum\", \"period\": 86400}],\n          [\".\", \"WriteThrottleEvents\", \".\", \".\", {\"stat\": \"Sum\", \"period\": 86400}]\n        ],\n        \"view\": \"bar\"\n      }\n    }\n  ]\n}\n</code></pre> <p>Dashboard Deployment</p> <p>Replace <code>${TableName}</code> with your actual table name (e.g., <code>limiter</code>) and <code>${AWS::Region}</code> with your region before deploying.</p>"},{"location":"monitoring/#alert-configuration","title":"Alert Configuration","text":""},{"location":"monitoring/#default-alarms","title":"Default Alarms","text":"<p>The stack deploys these alarms when <code>--enable-alarms</code> is set:</p> Alarm Metric Threshold Period Evaluation <code>{name}-aggregator-error-rate</code> Lambda Errors &gt; 1 5 min 2 periods <code>{name}-aggregator-duration</code> Lambda Duration &gt; 80% timeout 5 min 2 periods <code>{name}-stream-iterator-age</code> IteratorAge &gt; 30,000 ms 5 min 2 periods <code>{name}-aggregator-dlq-alarm</code> SQS Messages &gt;= 1 5 min 1 period <code>{name}-read-throttle</code> ReadThrottleEvents &gt; 1 5 min 2 periods <code>{name}-write-throttle</code> WriteThrottleEvents &gt; 1 5 min 2 periods"},{"location":"monitoring/#deploying-with-alarms","title":"Deploying with Alarms","text":"<pre><code># Deploy with alarms enabled (default)\nzae-limiter deploy --name limiter --region us-east-1\n\n# Deploy with SNS notifications\nzae-limiter deploy --name limiter --region us-east-1 \\\n    --alarm-sns-topic arn:aws:sns:us-east-1:123456789012:alerts\n\n# Customize duration threshold (70% of timeout)\nzae-limiter deploy --name limiter --region us-east-1 \\\n    --lambda-duration-threshold-pct 70\n\n# Disable alarms (not recommended for production)\nzae-limiter deploy --name limiter --region us-east-1 --no-alarms\n</code></pre>"},{"location":"monitoring/#threshold-tuning-guide","title":"Threshold Tuning Guide","text":"Alarm Default When to Increase When to Decrease Error Rate &gt;1/5min High-volume systems with rare transient errors Critical systems requiring immediate response Duration 80% timeout Batch workloads with variable processing time Latency-sensitive applications Iterator Age 30 seconds Batch-tolerant analytics workloads Real-time processing requirements DLQ Messages &gt;=1 Never (always investigate DLQ messages) N/A Throttles &gt;1/5min During planned traffic spikes Before hitting capacity limits"},{"location":"monitoring/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>from zae_limiter import RateLimiter, StackOptions\n\nlimiter = RateLimiter(\n    name=\"limiter\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(\n        enable_alarms=True,\n        alarm_sns_topic=\"arn:aws:sns:us-east-1:123456789012:alerts\",\n        lambda_duration_threshold_pct=75,  # Alert at 75% of timeout\n        log_retention_days=90,\n    ),\n)\n</code></pre>"},{"location":"monitoring/#next-steps","title":"Next Steps","text":"<ul> <li>Operations Guide - Troubleshooting and operational procedures</li> <li>Audit Logging - Security and compliance tracking</li> <li>Performance Tuning - Capacity planning and optimization</li> <li>Deployment Guide - Infrastructure setup</li> <li>CloudFormation Reference - Template customization</li> </ul>"},{"location":"performance/","title":"Performance Tuning Guide","text":"<p>This guide provides detailed recommendations for optimizing zae-limiter performance, covering DynamoDB capacity planning, Lambda configuration, and cost optimization strategies.</p>"},{"location":"performance/#1-dynamodb-capacity-planning","title":"1. DynamoDB Capacity Planning","text":""},{"location":"performance/#understanding-rcuwcu-costs","title":"Understanding RCU/WCU Costs","text":"<p>Each zae-limiter operation has specific DynamoDB capacity costs. Use this table for capacity planning:</p> Operation RCUs WCUs Notes <code>acquire()</code> - single limit 1 1 BatchGetItem + TransactWrite <code>acquire()</code> - N limits N N 1 BatchGetItem + TransactWrite(N items) <code>acquire()</code> with cascade entity N\u00d72 N\u00d72 1 GetEntity + 1 BatchGetItem + TransactWrite <code>acquire(use_stored_limits=True)</code> +2 0 +2 Query operations for limits <code>available()</code> 1 per limit 0 Read-only, no transaction <code>get_limits()</code> 1 0 Query operation <code>set_limits()</code> 1 N+1 Query + N PutItems <code>delete_entity()</code> 1 batched Query + BatchWrite in 25-item chunks <p>Read Optimization (v0.5.0)</p> <p><code>acquire()</code> uses <code>BatchGetItem</code> to fetch all buckets in a single DynamoDB call, reducing round trips for cascade scenarios. Previously, each bucket was fetched with a separate <code>GetItem</code> call.</p> <p>Capacity Validation</p> <p>These costs are validated by automated tests. Run <code>uv run pytest tests/benchmark/test_capacity.py -v</code> to verify.</p>"},{"location":"performance/#capacity-estimation-formula","title":"Capacity Estimation Formula","text":"<p>Use these formulas to estimate hourly capacity requirements:</p> <pre><code>Hourly RCUs = requests/hour \u00d7 limits/request \u00d7 (1 + cascade_pct \u00d7 2 + stored_limits_pct \u00d7 2)\nHourly WCUs = requests/hour \u00d7 limits/request \u00d7 (1 + cascade_pct)\n</code></pre> <p>With the Lambda aggregator enabled (2 windows: hourly, daily): <pre><code>Additional WCUs = requests/hour \u00d7 limits/request \u00d7 2\n</code></pre></p>"},{"location":"performance/#example-calculations","title":"Example Calculations","text":""},{"location":"performance/#scenario-1-simple-api-rate-limiting","title":"Scenario 1: Simple API Rate Limiting","text":"<ul> <li>10,000 requests/hour</li> <li>2 limits per request (rpm, tpm)</li> <li>No cascade, no stored limits</li> </ul> <p>Calculation: <pre><code>RCUs = 10,000 \u00d7 2 \u00d7 1.0 = 20,000 RCUs/hour\nWCUs = 10,000 \u00d7 2 \u00d7 1.0 = 20,000 WCUs/hour\n</code></pre></p>"},{"location":"performance/#scenario-2-hierarchical-llm-limiting","title":"Scenario 2: Hierarchical LLM Limiting","text":"<ul> <li>10,000 requests/hour</li> <li>2 limits per request</li> <li>50% use cascade (API key \u2192 project)</li> <li>20% use stored limits</li> </ul> <p>Calculation: <pre><code>RCUs = 10,000 \u00d7 2 \u00d7 (1 + 0.5\u00d72 + 0.2\u00d72) = 10,000 \u00d7 2 \u00d7 2.4 = 48,000 RCUs/hour\nWCUs = 10,000 \u00d7 2 \u00d7 (1 + 0.5) = 10,000 \u00d7 2 \u00d7 1.5 = 30,000 WCUs/hour\n</code></pre></p>"},{"location":"performance/#billing-mode-selection","title":"Billing Mode Selection","text":"Mode Best For Trade-offs PAY_PER_REQUEST (default) Variable traffic, new deployments Higher per-request cost, no planning needed Provisioned Steady traffic &gt;100 TPS Lower cost at scale, requires planning Provisioned + Reserved High-volume production Lowest cost, 1-year commitment <p>Migration Guidance</p> <p>Start with PAY_PER_REQUEST. Once traffic patterns stabilize (typically 2-4 weeks), analyze CloudWatch metrics to determine optimal provisioned capacity. Switch when monthly on-demand costs exceed provisioned + 20% buffer.</p>"},{"location":"performance/#2-lambda-concurrency-settings","title":"2. Lambda Concurrency Settings","text":"<p>The aggregator Lambda processes DynamoDB Stream events to maintain usage snapshots.</p>"},{"location":"performance/#default-configuration","title":"Default Configuration","text":"Setting Default Range Impact Memory 256 MB 128-3008 MB Higher = faster, more expensive Timeout 60 seconds 1-900 seconds Should be 2\u00d7 typical duration Reserved Concurrency None 1-1000 Limits parallel executions"},{"location":"performance/#memory-tuning","title":"Memory Tuning","text":"<p>Lambda CPU scales linearly with memory allocation:</p> Memory vCPUs Best For 128 MB ~0.08 Minimal workloads (testing only) 256 MB ~0.15 Most workloads (default) 512 MB ~0.30 High-throughput streams 1024 MB ~0.60 Rarely needed <p>Guidance based on batch size:</p> <ul> <li>&lt;50 records/batch: 128-256 MB sufficient</li> <li>50-100 records/batch: 256-512 MB recommended</li> <li>Peak streams: Monitor Lambda duration; increase memory if &gt;50% of timeout</li> </ul>"},{"location":"performance/#concurrency-management","title":"Concurrency Management","text":"<p>DynamoDB Streams creates one shard per 1000 WCU (or ~3000 writes/sec). Each shard invokes one Lambda instance.</p> <p>Recommendations:</p> Volume Reserved Concurrency Notes &lt;1000 writes/sec None Default scaling sufficient 1000-10000/sec 10-50 Prevents runaway scaling &gt;10000/sec Expected shards + 20% Based on table monitoring"},{"location":"performance/#error-handling","title":"Error Handling","text":"<p>Configure error handling for production reliability:</p> <pre><code># Deploy with DLQ and alarms\nzae-limiter deploy --table-name rate_limits \\\n  --alarm-sns-topic arn:aws:sns:us-east-1:123456789012:alerts\n</code></pre> <ul> <li>Retries: Failed records retry 3 times within the same batch</li> <li>DLQ: Persistent failures go to Dead Letter Queue (if configured)</li> <li>Duration Alarm: Triggers at 80% of timeout (48s default)</li> </ul>"},{"location":"performance/#3-batch-operation-patterns","title":"3. Batch Operation Patterns","text":""},{"location":"performance/#transaction-limits","title":"Transaction Limits","text":"<p>DynamoDB enforces these limits:</p> Constraint Limit Impact TransactWriteItems 100 items max Affects multi-limit updates BatchWriteItem 25 items per request Entity deletion is chunked Optimistic locking Entire transaction fails Causes retry on contention"},{"location":"performance/#efficient-patterns","title":"Efficient Patterns","text":""},{"location":"performance/#multi-limit-acquisition","title":"Multi-Limit Acquisition","text":"<pre><code># Efficient: Single lease for multiple limits\nasync with limiter.acquire(\n    \"entity-id\",\n    \"llm-api\",\n    [rpm_limit, tpm_limit],\n    {\"rpm\": 1},  # Initial consumption (1 request)\n) as lease:\n    # 2 GetItems + 1 TransactWrite (2 items)\n    response = await call_llm()\n    lease.adjust({\"tpm\": response.usage.total_tokens})\n\n# Inefficient: Separate acquisitions\nasync with limiter.acquire(\"entity-id\", \"llm-api\", [rpm_limit], {\"rpm\": 1}):\n    async with limiter.acquire(\"entity-id\", \"llm-api\", [tpm_limit], {\"tpm\": 100}):\n        # 2 GetItems + 2 TransactWrites (doubles write cost!)\n        pass\n</code></pre>"},{"location":"performance/#cascade-optimization","title":"Cascade Optimization","text":"<pre><code># Entity without cascade (default) \u2014 saves 1 GetEntity + parent bucket operations\nawait limiter.create_entity(entity_id=\"api-key\", parent_id=\"project-1\")\n\nasync with limiter.acquire(\"api-key\", \"llm-api\", limits, {\"rpm\": 1}):\n    pass  # Only checks api-key's limits\n\n# Entity with cascade \u2014 checks and updates parent limits too\nawait limiter.create_entity(entity_id=\"api-key\", parent_id=\"project-1\", cascade=True)\n\nasync with limiter.acquire(\"api-key\", \"llm-api\", limits, {\"rpm\": 1}):\n    pass  # Checks both api-key AND project-1 limits\n</code></pre>"},{"location":"performance/#write-sharding-for-high-fanout-parents","title":"Write Sharding for High-Fanout Parents","text":"<p>When a parent entity has many children (1000+) with <code>cascade=True</code>, the parent partition may experience write throttling. DynamoDB limits throughput per partition to ~1,000 WCU (or ~3,000 RCU).</p> <p>Manual Write Sharding Solution:</p> <p>Instead of one parent, distribute ownership across multiple sharded parent entities:</p> <pre><code># OLD: Single parent becomes a hotspot\n# \u251c\u2500\u2500 project-1 (parent)\n# \u2502   \u251c\u2500\u2500 api-key-1 (child, cascade=True)\n# \u2502   \u251c\u2500\u2500 api-key-2 (child, cascade=True)\n# \u2502   \u2514\u2500\u2500 ... (1000+ children)\n\n# NEW: Distribute across shards (e.g., 10 shards = 10x capacity)\nnum_shards = 10\napi_key_id = \"api-key-12345\"\nshard_id = hash(api_key_id) % num_shards\nparent_id = f\"project-1-shard-{shard_id}\"\n\n# Create shard parents once during setup\nfor shard in range(num_shards):\n    parent_id = f\"project-1-shard-{shard}\"\n    await limiter.create_entity(entity_id=parent_id, parent_id=\"project-1\")\n    # Set the same limits on all shards\n    await limiter.set_limits(\n        parent_id,\n        [\n            Limit.per_minute(\"rpm\", capacity=10000),\n            Limit.per_minute(\"tpm\", capacity=100000),\n        ],\n        resource=\"llm-api\"\n    )\n\n# For each child, assign to a random shard\nshard_id = hash(api_key_id) % num_shards\nsharded_parent = f\"project-1-shard-{shard_id}\"\nawait limiter.create_entity(\n    entity_id=api_key_id,\n    parent_id=sharded_parent,\n    cascade=True\n)\n\n# On acquire, use the same sharding logic\nshard_id = hash(api_key_id) % num_shards\nsharded_parent = f\"project-1-shard-{shard_id}\"\nasync with limiter.acquire(api_key_id, \"llm-api\", limits, {\"rpm\": 1}):\n    pass  # Cascades to sharded parent instead of single hotspot\n</code></pre> <p>Benefits: - Distributes parent write traffic across N partitions - With 10 shards: ~10x capacity improvement - Only requires application-level sharding logic</p> <p>Drawbacks: - More parent entities to manage - Limits checked per shard (not globally across all shards) - Requires hash consistency in sharding logic</p> <p>When to use: - Parent has &gt;500 API keys with <code>cascade=True</code> and hitting throttling - Cost-effective alternative to on-demand billing - Temporary solution before implementing more sophisticated load distribution</p>"},{"location":"performance/#stored-limits-optimization","title":"Stored Limits Optimization","text":"<pre><code># Default: No stored limits lookup (saves 2 RCUs per acquire)\nlimiter = RateLimiter(\n    name=\"rate_limits\",\n    region=\"us-east-1\",\n)\n\n# Enable only when limits vary per entity\nlimiter = RateLimiter(\n    name=\"rate_limits\",\n    region=\"us-east-1\",\n    use_stored_limits=True,  # +2 Queries per acquire\n)\n</code></pre>"},{"location":"performance/#bulk-operations","title":"Bulk Operations","text":"<pre><code># Efficient bulk limit setup\nawait limiter.set_limits(\"entity-1\", [rpm_limit, tpm_limit], resource=\"llm-api\")\nawait limiter.set_limits(\"entity-2\", [rpm_limit, tpm_limit], resource=\"llm-api\")\n# Runs 2 Queries + 2\u00d72 PutItems\n\n# Entity deletion (automatically batched in 25-item chunks)\nawait limiter.delete_entity(\"entity-id\")\n# Runs 1 Query + BatchWrite (up to 25 WCUs per chunk)\n</code></pre>"},{"location":"performance/#4-expected-latencies","title":"4. Expected Latencies","text":""},{"location":"performance/#operation-latencies","title":"Operation Latencies","text":"<p>Latencies vary by environment and depend on network conditions, DynamoDB utilization, and operation complexity.</p> Operation Moto p50 LocalStack p50 AWS p50 <code>acquire()</code> - single limit 14ms 36ms 36ms <code>acquire()</code> - two limits 30ms 52ms 43ms <code>acquire()</code> with cascade entity 28ms 57ms 51ms <code>available()</code> check 1ms 9ms 8ms <p>Environment Differences</p> <ul> <li>Moto: In-memory mock, measures code overhead only</li> <li>LocalStack: Docker-based, includes local network latency</li> <li>AWS: Production DynamoDB with real network round-trips</li> </ul>"},{"location":"performance/#latency-breakdown","title":"Latency Breakdown","text":"<p>Typical <code>acquire()</code> latency breakdown for a single limit:</p> <pre><code>acquire() latency breakdown:\n\u251c\u2500\u2500 DynamoDB GetItem (bucket)     ~5-15ms   (network + read)\n\u251c\u2500\u2500 Token bucket calculation      &lt;1ms      (in-memory math)\n\u251c\u2500\u2500 TransactWriteItems            ~10-25ms  (network + write + condition check)\n\u2514\u2500\u2500 Network overhead              variable  (region, instance type)\n</code></pre>"},{"location":"performance/#environment-selection","title":"Environment Selection","text":"Environment Use Case Latency Factor Moto Unit tests, CI/CD 1\u00d7 (baseline) LocalStack Integration tests, local dev 2-3\u00d7 AWS Production, load testing 2-4\u00d7 <p>Run benchmarks to measure your specific environment:</p> <pre><code># Moto benchmarks (fast)\nuv run pytest tests/benchmark/test_latency.py -v --benchmark-json=latency.json\n\n# LocalStack benchmarks (requires Docker)\ndocker compose up -d\nexport AWS_ENDPOINT_URL=http://localhost:4566\nuv run pytest tests/benchmark/test_localstack.py -v --benchmark-json=latency.json\n\n# AWS benchmarks (requires credentials)\nuv run pytest tests/benchmark/test_aws.py --run-aws -v\n</code></pre>"},{"location":"performance/#5-throughput-benchmarks","title":"5. Throughput Benchmarks","text":""},{"location":"performance/#maximum-throughput","title":"Maximum Throughput","text":"<p>Theoretical and practical throughput limits depend on contention patterns:</p> Scenario Expected TPS Bottleneck Sequential, single entity 50-200 Serialized operations Sequential, multiple entities 50-200 Network round-trip Concurrent, separate entities 100-500 Scales with parallelism Concurrent, single entity 20-100 Optimistic locking contention Cascade operations 30-100 Parent bucket contention"},{"location":"performance/#contention-analysis","title":"Contention Analysis","text":"<p>When multiple requests update the same bucket concurrently, DynamoDB's optimistic locking causes transaction retries:</p> <pre><code>Concurrent updates to same bucket:\n\u251c\u2500\u2500 Request A: Read bucket version=1\n\u251c\u2500\u2500 Request B: Read bucket version=1\n\u251c\u2500\u2500 Request A: Write with condition version=1 \u2192 SUCCESS, version=2\n\u251c\u2500\u2500 Request B: Write with condition version=1 \u2192 FAIL (ConditionalCheckFailed)\n\u2514\u2500\u2500 Request B: Retry with version=2 \u2192 SUCCESS\n</code></pre> <p>Each retry adds ~10-30ms latency.</p>"},{"location":"performance/#mitigation-strategies","title":"Mitigation Strategies","text":"<pre><code># Strategy 1: Larger bucket windows (reduces update frequency)\nrpm_limit = Limit.per_minute(\"rpm\", capacity=1000, window_seconds=60)\n\n# Strategy 2: Distribute load across entities\n# Instead of one shared entity, use sharded entities:\nshard = hash(request_id) % 10\nentity_id = f\"api-key-shard-{shard}\"\n\n# Strategy 3: Client-side rate limiting before acquire\n# Reduce concurrent requests to the same entity\n</code></pre>"},{"location":"performance/#running-benchmarks","title":"Running Benchmarks","text":"<p>Use the automated benchmark runner:</p> <pre><code># Run all benchmarks (moto + LocalStack)\npython scripts/run_benchmarks.py\n\n# Include AWS benchmarks\npython scripts/run_benchmarks.py --run-aws\n\n# Skip LocalStack (moto only)\npython scripts/run_benchmarks.py --skip-localstack\n\n# Custom output directory\npython scripts/run_benchmarks.py --output-dir ./results\n</code></pre> <p>Or run individual test suites:</p> <pre><code># Throughput tests\nuv run pytest tests/benchmark/test_throughput.py -v\n\n# Analyze results\npython -c \"import json; print(json.load(open('benchmark.json'))['benchmarks'])\"\n</code></pre>"},{"location":"performance/#6-cost-optimization-strategies","title":"6. Cost Optimization Strategies","text":""},{"location":"performance/#dynamodb-cost-breakdown","title":"DynamoDB Cost Breakdown","text":"<p>Costs vary by region. Using us-east-1 as reference:</p> Component On-Demand Cost Notes Write Request Units $0.625 per million Each WCU = one write Read Request Units $0.125 per million Each RCU = one read Storage $0.25 per GB/month Usually minimal Streams $0.02 per 100K reads Lambda polling Lambda $0.20 per million + duration Aggregator function"},{"location":"performance/#cost-estimation-examples","title":"Cost Estimation Examples","text":""},{"location":"performance/#low-volume-10k-requestsday","title":"Low Volume: 10K requests/day","text":"<pre><code>DynamoDB:\n  Writes: 10K \u00d7 2 limits \u00d7 30 days = 600K WCUs = $0.38\n  Reads:  10K \u00d7 2 limits \u00d7 30 days = 600K RCUs = $0.08\n  Streams: 600K events                         = $0.12\nLambda: 600K invocations                       \u2248 $0.12 + duration\nStorage: ~10 MB                                = negligible\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTotal: ~$0.70/month\n</code></pre>"},{"location":"performance/#medium-volume-1m-requestsday","title":"Medium Volume: 1M requests/day","text":"<pre><code>DynamoDB:\n  Writes: 1M \u00d7 2 \u00d7 30 = 60M WCUs               = $37.50\n  Reads:  1M \u00d7 2 \u00d7 30 = 60M RCUs               = $7.50\n  Streams: 60M events                          = $12.00\nLambda: 60M invocations                        \u2248 $12.00 + duration\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTotal (on-demand): ~$70/month\nTotal (provisioned with auto-scaling): ~$45/month\n</code></pre>"},{"location":"performance/#cost-reduction-strategies","title":"Cost Reduction Strategies","text":""},{"location":"performance/#1-disable-unused-features","title":"1. Disable Unused Features","text":"<pre><code># Create entity without cascade if not needed (saves 1-2 WCUs per request)\nawait limiter.create_entity(entity_id=\"entity\", parent_id=\"parent\")  # cascade=False by default\nasync with limiter.acquire(\"entity\", \"api\", limits, {\"rpm\": 1}):\n    pass\n\n# Disable stored limits if static (saves 2 RCUs per request)\nlimiter = RateLimiter(name=\"rate_limits\", region=\"us-east-1\")\n</code></pre>"},{"location":"performance/#2-optimize-ttl-settings","title":"2. Optimize TTL Settings","text":"<pre><code># Shorter TTL = faster cleanup = less storage\nlimiter = RateLimiter(\n    name=\"rate_limits\",\n    region=\"us-east-1\",\n    bucket_ttl_seconds=3600,  # 1 hour vs 24 hour default\n)\n</code></pre>"},{"location":"performance/#3-reduce-snapshot-granularity","title":"3. Reduce Snapshot Granularity","text":"<pre><code># Deploy without aggregator if usage tracking not needed\nzae-limiter deploy --table-name rate_limits --no-aggregator\n</code></pre>"},{"location":"performance/#4-switch-to-provisioned-at-scale","title":"4. Switch to Provisioned at Scale","text":"<ul> <li>Break-even: ~5M operations/month</li> <li>Use auto-scaling with 70% target utilization</li> <li>Consider reserved capacity for &gt;20M ops/month</li> </ul>"},{"location":"performance/#5-batch-similar-operations","title":"5. Batch Similar Operations","text":"<pre><code># Combine multiple limits into single acquire\nasync with limiter.acquire(\n    \"entity\",\n    \"api\",\n    [rpm_limit, tpm_limit, daily_limit],\n    {\"rpm\": 1},  # 1 transaction vs 3\n):\n    pass\n</code></pre>"},{"location":"performance/#cost-monitoring","title":"Cost Monitoring","text":"<p>Set up CloudWatch metrics for cost tracking:</p> <p>DynamoDB Metrics:</p> <ul> <li><code>ConsumedReadCapacityUnits</code></li> <li><code>ConsumedWriteCapacityUnits</code></li> <li><code>AccountProvisionedReadCapacityUtilization</code></li> <li><code>AccountProvisionedWriteCapacityUtilization</code></li> </ul> <p>Lambda Metrics:</p> <ul> <li><code>Invocations</code></li> <li><code>Duration</code></li> <li><code>ConcurrentExecutions</code></li> </ul> <p>Recommended Alerts:</p> <pre><code># Deploy with alarms for cost anomalies\nzae-limiter deploy --table-name rate_limits \\\n  --alarm-sns-topic arn:aws:sns:us-east-1:123456789012:billing-alerts\n\n# Set AWS Budgets alert at 80% of expected monthly cost\naws budgets create-budget \\\n  --account-id 123456789012 \\\n  --budget file://budget.json \\\n  --notifications-with-subscribers file://notifications.json\n</code></pre>"},{"location":"performance/#7-config-cache-tuning","title":"7. Config Cache Tuning","text":"<p>The config cache reduces DynamoDB reads by caching system defaults, resource defaults, and entity limits. This section covers tuning the cache for your workload.</p>"},{"location":"performance/#cache-configuration","title":"Cache Configuration","text":"<pre><code>from zae_limiter import RateLimiter, Repository\n\n# Default: 60-second TTL (recommended for most workloads)\nlimiter = RateLimiter(\n    repository=Repository(name=\"my-app\", region=\"us-east-1\"),\n    config_cache_ttl=60,  # Default\n)\n\n# High-frequency updates: Shorter TTL for faster propagation\nlimiter = RateLimiter(\n    repository=Repository(name=\"my-app\", region=\"us-east-1\"),\n    config_cache_ttl=10,  # 10 seconds - faster updates, more cache misses\n)\n\n# Disable caching: For testing or when config changes must be immediate\nlimiter = RateLimiter(\n    repository=Repository(name=\"my-app\", region=\"us-east-1\"),\n    config_cache_ttl=0,  # Disabled - every acquire reads from DynamoDB\n)\n</code></pre>"},{"location":"performance/#cost-impact","title":"Cost Impact","text":"<p>Without caching, each <code>acquire()</code> call performs 3 DynamoDB reads to resolve limits:</p> <ol> <li>Entity-level config lookup (1 RCU)</li> <li>Resource-level config lookup (1 RCU)</li> <li>System-level config lookup (1 RCU)</li> </ol> <p>With caching (default):</p> Traffic Rate Cache Hit Rate Amortized RCU/request 1 req/sec 98.3% 0.05 RCU 10 req/sec 99.8% 0.005 RCU 100 req/sec 99.98% 0.0005 RCU <p>Negative caching also helps: When an entity has no custom config (95%+ of entities typically), the cache remembers this to avoid repeated lookups.</p>"},{"location":"performance/#manual-invalidation","title":"Manual Invalidation","text":"<p>After modifying config, you can force immediate refresh:</p> <pre><code># Update config\nawait limiter.set_system_defaults([Limit.per_minute(\"rpm\", 1000)])\n\n# Force immediate cache refresh (optional)\nawait limiter.invalidate_config_cache()\n</code></pre> <p>Without manual invalidation, changes propagate within the TTL period (max 60 seconds by default).</p>"},{"location":"performance/#monitoring-cache-performance","title":"Monitoring Cache Performance","text":"<pre><code># Get cache statistics\nstats = limiter.get_cache_stats()\nprint(f\"Cache hit rate: {stats.hits / (stats.hits + stats.misses):.1%}\")\nprint(f\"Cache entries: {stats.size}\")\nprint(f\"TTL: {stats.ttl}s\")\n</code></pre>"},{"location":"performance/#ttl-selection-guidelines","title":"TTL Selection Guidelines","text":"Scenario Recommended TTL Rationale Production (stable config) 60s (default) Best cost/latency trade-off Development/testing 10-30s Faster config iteration Compliance-critical 10-30s Minimizes staleness Testing with frequent changes 0 (disabled) Immediate visibility High-traffic APIs (&gt;100 req/s) 60-120s Maximize cache hits"},{"location":"performance/#summary","title":"Summary","text":"Optimization Area Key Recommendations Capacity Start with on-demand, switch to provisioned at 5M+ ops/month Latency Expect 30-50ms p50 on AWS; use LocalStack for realistic testing Throughput Distribute load across entities to avoid contention Cost Disable cascade/stored_limits when not needed Config Cache Use default 60s TTL; invalidate manually for immediate changes Monitoring Set up CloudWatch alerts for capacity and cost anomalies <p>For detailed benchmark data, run: <pre><code>python scripts/run_benchmarks.py --run-aws\n</code></pre></p>"},{"location":"adr/000-adr-format-standard/","title":"ADR-000: ADR Format Standard","text":"<p>Status: Accepted Date: 2026-01-19</p>"},{"location":"adr/000-adr-format-standard/#context","title":"Context","text":"<p>Architecture Decision Records help teams understand why past decisions were made. Without consistent formatting, ADRs become hard to scan, compare, and maintain. Some ADRs balloon into design documents with code examples and implementation checklists, while others lack essential sections like alternatives considered.</p> <p>The team needs a standard format that keeps ADRs focused on the \"why\" while ensuring they remain concise and actionable.</p>"},{"location":"adr/000-adr-format-standard/#decision","title":"Decision","text":"<p>All ADRs must follow this format:</p> <ol> <li>Maximum 100 lines - If longer, split into multiple ADRs or move details to issues</li> <li>One decision per ADR - Each ADR documents exactly one architectural choice</li> <li>Required sections:</li> <li>Context (2-3 paragraphs max)</li> <li>Decision (1-2 sentences)</li> <li>Consequences (positive and negative bullet lists)</li> <li>Alternatives Considered (1 sentence rejection reason each)</li> <li>Excluded content (belongs in issues or design docs):</li> <li>Code examples or API signatures</li> <li>Implementation checklists</li> <li>Test cases</li> <li>Phased rollout plans</li> <li>Detailed cost calculations</li> <li>Enforceable decisions - Write decisions that can be deterministically verified:</li> <li>Use explicit prose requirements (\"must use X\", \"Y owns Z\")</li> <li>Avoid vague language (\"should\", \"ideally\", \"consider\")</li> <li>Link to issues for implementation details and examples</li> </ol>"},{"location":"adr/000-adr-format-standard/#consequences","title":"Consequences","text":"<p>Positive: - ADRs are quick to read and compare - Reviewers can focus on architectural reasoning, not implementation details - Consistent structure makes it easy to find information</p> <p>Negative: - Some decisions require linking to external design documents - Contributors must learn the format before writing ADRs</p>"},{"location":"adr/000-adr-format-standard/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr/000-adr-format-standard/#free-form-documentation","title":"Free-form documentation","text":"<p>Rejected because: Inconsistent formats make ADRs hard to scan and maintain over time.</p>"},{"location":"adr/000-adr-format-standard/#detailed-design-documents","title":"Detailed design documents","text":"<p>Rejected because: Design docs focus on \"how\" rather than \"why\", and become stale after implementation.</p>"},{"location":"adr/001-single-table-dynamodb/","title":"ADR-001: Single-Table DynamoDB Design","text":"<p>Status: Accepted Date: 2026-01-09 Commit: 3902c8c Milestone: v0.1.0</p>"},{"location":"adr/001-single-table-dynamodb/#context","title":"Context","text":"<p>zae-limiter needs to store multiple entity types: entities (users/API keys), token buckets, limits, audit events, usage snapshots, and version metadata. Traditional relational design would use separate tables, but DynamoDB pricing and access patterns favor different approaches.</p> <p>Key requirements: - Atomic multi-item transactions (entity + buckets in single transaction) - Efficient parent-child lookups (hierarchical entities) - Resource-level aggregation (capacity across all entities) - Pay-per-request pricing optimization</p>"},{"location":"adr/001-single-table-dynamodb/#decision","title":"Decision","text":"<p>Use single-table design with composite keys and GSIs for all access patterns.</p> <p>Key structure: - <code>PK</code>: Entity/resource identifier (e.g., <code>ENTITY#user-1</code>, <code>RESOURCE#gpt-4</code>, <code>SYSTEM#</code>) - <code>SK</code>: Record type and identifiers (e.g., <code>#META</code>, <code>#BUCKET#gpt-4#tpm</code>, <code>#AUDIT#2026-01-01T00:00:00Z</code>)</p> <p>Global Secondary Indexes: - <code>GSI1</code>: Parent \u2192 Children lookups (<code>GSI1PK=PARENT#{id}</code>) - <code>GSI2</code>: Resource aggregation (<code>GSI2PK=RESOURCE#{name}</code>)</p>"},{"location":"adr/001-single-table-dynamodb/#consequences","title":"Consequences","text":"<p>Positive: - Single table = single provisioning decision, simpler cost management - TransactWriteItems works across all record types (max 100 items) - Efficient queries: entity + all buckets in single query - Natural fit for hierarchical data (parent/child via GSI1)</p> <p>Negative: - More complex key design requires careful documentation - Hot partition risk if single entity has extreme traffic - GSI costs for every write (mitigated by sparse indexes)</p>"},{"location":"adr/001-single-table-dynamodb/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Multi-table design: Rejected due to cross-table transaction limitations and higher operational complexity</li> <li>Adjacency list only: Rejected; GSIs provide cleaner access patterns for resource aggregation</li> </ul>"},{"location":"adr/002-integer-arithmetic-millitokens/","title":"ADR-002: Integer Arithmetic with Millitokens","text":"<p>Status: Accepted Date: 2026-01-09 Commit: 3902c8c Milestone: v0.1.0</p>"},{"location":"adr/002-integer-arithmetic-millitokens/#context","title":"Context","text":"<p>Token bucket algorithms require tracking fractional tokens during refill calculations. In distributed systems, floating-point arithmetic can cause precision issues:</p> <ul> <li>IEEE 754 floating-point has representation errors (0.1 + 0.2 \u2260 0.3)</li> <li>Different clients/languages may have slightly different FP implementations</li> <li>Accumulated rounding errors cause bucket drift over time</li> <li>DynamoDB stores numbers as strings, adding serialization concerns</li> </ul>"},{"location":"adr/002-integer-arithmetic-millitokens/#decision","title":"Decision","text":"<p>Store all token values as millitokens (\u00d71000) using integer arithmetic.</p> <p>Implementation: - Internal storage: <code>tokens_milli</code> (integer) - User-facing API: <code>tokens</code> (float, converted at boundary) - Refill stored as fraction: <code>refill_amount</code> / <code>refill_period_seconds</code> - All bucket math uses integer operations</p>"},{"location":"adr/002-integer-arithmetic-millitokens/#consequences","title":"Consequences","text":"<p>Positive: - Exact arithmetic: no precision loss across any number of operations - Deterministic: same calculation yields identical results everywhere - Simple debugging: values are exact integers - DynamoDB-friendly: integers serialize cleanly</p> <p>Negative: - API boundary conversion required (minor complexity) - Sub-millitoken precision not supported (acceptable for rate limiting) - Developers must remember internal representation when debugging</p>"},{"location":"adr/002-integer-arithmetic-millitokens/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Decimal type: Rejected; Python's Decimal doesn't map cleanly to DynamoDB Number type</li> <li>Floating-point with rounding: Rejected; accumulated errors in long-running buckets</li> <li>Fixed-point with higher precision (microtokens): Rejected; millitokens sufficient for rate limiting use cases</li> </ul>"},{"location":"adr/003-cloudformation-infrastructure/","title":"ADR-003: CloudFormation Infrastructure Deployment","text":"<p>Status: Accepted Date: 2026-01-10 PR: #8 Milestone: v0.1.0</p>"},{"location":"adr/003-cloudformation-infrastructure/#context","title":"Context","text":"<p>zae-limiter requires DynamoDB tables, Lambda functions, DynamoDB Streams, IAM roles, and CloudWatch resources. Users need a way to deploy and manage this infrastructure.</p> <p>Options considered: - Programmatic table creation via boto3 - CloudFormation templates - CDK constructs - Terraform modules</p>"},{"location":"adr/003-cloudformation-infrastructure/#decision","title":"Decision","text":"<p>Use CloudFormation as the primary infrastructure deployment mechanism, with a CLI for user convenience.</p> <p>Implementation: - Embedded CloudFormation template (<code>cfn_template.yaml</code>) - CLI commands: <code>deploy</code>, <code>delete</code>, <code>status</code>, <code>cfn-template</code> - Stack naming convention: <code>ZAEL-{identifier}</code> prefix - Auto-detection of LocalStack for local development</p>"},{"location":"adr/003-cloudformation-infrastructure/#consequences","title":"Consequences","text":"<p>Positive: - Declarative infrastructure with drift detection - Easy cleanup: delete entire stack removes all resources - Native AWS integration (no external tools required) - Template export for users who want customization - Consistent naming across all AWS resources</p> <p>Negative: - CloudFormation deployment is slower than direct API calls - Template complexity grows with features (alarms, DLQ, etc.) - LocalStack CloudFormation support occasionally lags AWS</p>"},{"location":"adr/003-cloudformation-infrastructure/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Programmatic creation only: Rejected; no drift detection, harder cleanup, inconsistent state on partial failures</li> <li>CDK-first: Rejected; adds Node.js dependency, higher barrier for simple deployments</li> <li>Terraform: Rejected; external tool dependency, different ecosystem than target users</li> </ul>"},{"location":"adr/004-declarative-stack-options/","title":"ADR-004: Declarative Infrastructure with StackOptions","text":"<p>Status: Accepted Date: 2026-01-12 PR: #69 Milestone: v0.2.0</p>"},{"location":"adr/004-declarative-stack-options/#context","title":"Context","text":"<p>The initial <code>create_stack=True</code> boolean parameter was insufficient for production deployments. Users needed to configure Lambda memory, alarms, SNS topics, and other stack parameters. The API had grown to include both <code>create_stack: bool</code> and <code>stack_parameters: dict[str, str]</code>, which was error-prone and not type-safe.</p>"},{"location":"adr/004-declarative-stack-options/#decision","title":"Decision","text":"<p>Replace <code>create_stack</code> and <code>stack_parameters</code> with a unified <code>stack_options: StackOptions | None</code> parameter.</p> <p>Implementation: - <code>StackOptions</code> frozen dataclass with all configuration - Validation in <code>__post_init__</code> for bounds checking - <code>to_parameters()</code> method for CloudFormation conversion - <code>None</code> = don't manage infrastructure; <code>StackOptions()</code> = manage with defaults</p>"},{"location":"adr/004-declarative-stack-options/#consequences","title":"Consequences","text":"<p>Positive: - Type-safe configuration with IDE autocomplete - Self-documenting API (all options visible in dataclass) - Validation at construction time, not deployment time - Clear semantics: presence of StackOptions = infrastructure management enabled - Enables \"self-deploying\" applications</p> <p>Negative: - Breaking change from <code>create_stack</code> parameter - More verbose for simple cases (<code>StackOptions()</code> vs <code>create_stack=True</code>)</p>"},{"location":"adr/004-declarative-stack-options/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Keep boolean + dict: Rejected; not type-safe, no validation, poor discoverability</li> <li>Builder pattern: Rejected; more complex API for same outcome</li> <li>Separate configuration file: Rejected; adds deployment complexity, harder to keep in sync</li> </ul>"},{"location":"adr/005-exception-hierarchy/","title":"ADR-005: Categorized Exception Hierarchy","text":"<p>Status: Accepted Date: 2026-01-11 PR: #53 Issue: #29 Milestone: v0.2.0</p>"},{"location":"adr/005-exception-hierarchy/#context","title":"Context","text":"<p>The initial exception design had flat inheritance from <code>Exception</code>, making it difficult to catch categories of errors. <code>EntityNotFoundError</code> inherited from <code>RateLimitError</code> despite not being a rate limit issue. Users couldn't distinguish infrastructure failures from entity errors.</p>"},{"location":"adr/005-exception-hierarchy/#decision","title":"Decision","text":"<p>Implement categorized exception hierarchy with semantic grouping.</p> <p>Hierarchy: <pre><code>ZAELimiterError (base)\n\u251c\u2500\u2500 RateLimitError (rate limiting)\n\u2502   \u251c\u2500\u2500 RateLimitExceeded\n\u2502   \u2514\u2500\u2500 RateLimiterUnavailable\n\u251c\u2500\u2500 InfrastructureError (AWS/CloudFormation)\n\u2502   \u251c\u2500\u2500 StackCreationError\n\u2502   \u2514\u2500\u2500 InfrastructureNotFoundError\n\u251c\u2500\u2500 EntityError (CRUD operations)\n\u2502   \u251c\u2500\u2500 EntityNotFoundError\n\u2502   \u2514\u2500\u2500 EntityExistsError\n\u2514\u2500\u2500 VersionError (compatibility)\n    \u251c\u2500\u2500 VersionMismatchError\n    \u2514\u2500\u2500 IncompatibleSchemaError\n</code></pre></p>"},{"location":"adr/005-exception-hierarchy/#consequences","title":"Consequences","text":"<p>Positive: - Catch broad categories (<code>except RateLimitError</code>) or specific exceptions - Semantic clarity: exception type indicates problem domain - Follows Python conventions (<code>requests.RequestException</code>, etc.) - Backward compatible: existing catches still work</p> <p>Negative: - More exception classes to maintain - Migration required for code catching moved exceptions (e.g., <code>EntityNotFoundError</code>)</p>"},{"location":"adr/005-exception-hierarchy/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Flat hierarchy: Rejected; no category-level catching, poor semantics</li> <li>Error codes instead of types: Rejected; not Pythonic, loses type checking benefits</li> <li>Fewer categories: Rejected; infrastructure vs entity distinction valuable for error handling</li> </ul>"},{"location":"adr/006-flat-schema-snapshots/","title":"ADR-006: Flat Schema for Usage Snapshots","text":"<p>Status: Accepted Date: 2026-01-11 (initial), 2026-01-16 (confirmed) PRs: #20, #172 Issue: #168 Milestone: v0.1.0, v0.4.0</p>"},{"location":"adr/006-flat-schema-snapshots/#context","title":"Context","text":"<p>Usage snapshots aggregate consumption data (tpm, rpm, total_events) per entity/resource/time window. The Lambda aggregator uses <code>UpdateItem</code> with <code>ADD</code> for atomic counter increments. Initial implementation used nested <code>data.M</code> maps like other record types.</p> <p>DynamoDB limitation discovered: You cannot SET a map path (<code>#data = if_not_exists(#data, :map)</code>) AND ADD to paths within it (<code>#data.counter</code>) in the same UpdateExpression. It fails with \"overlapping document paths\" error.</p>"},{"location":"adr/006-flat-schema-snapshots/#decision","title":"Decision","text":"<p>Use flat schema (top-level attributes) for usage snapshot records, diverging from the nested <code>data.M</code> pattern used elsewhere.</p> <p>Snapshot structure: <pre><code>{\n    \"PK\": \"ENTITY#user-1\",\n    \"SK\": \"#USAGE#gpt-4#2024-01-01T14:00:00Z\",\n    \"resource\": \"gpt-4\",      # Top-level\n    \"window\": \"hourly\",       # Top-level\n    \"tpm\": 5000,              # Counter at top-level\n    \"total_events\": 10,       # Counter at top-level\n}\n</code></pre></p>"},{"location":"adr/006-flat-schema-snapshots/#consequences","title":"Consequences","text":"<p>Positive: - Atomic upsert with ADD counters works correctly - Single UpdateItem call creates or updates record - No pre-existence check required</p> <p>Negative: - Schema inconsistency with other record types (entities, buckets use nested <code>data.M</code>) - Established pattern for v0.6.0 full schema flattening (see #180)</p>"},{"location":"adr/006-flat-schema-snapshots/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Two-step create-then-update: Rejected; race conditions, higher latency, more RCU/WCU</li> <li>Conditional expressions with retries: Rejected; complex error handling, still potential races</li> <li>Store in separate table: Rejected; loses single-table benefits, complicates transactions</li> </ul>"},{"location":"adr/007-input-validation/","title":"ADR-007: Input Validation for Injection Prevention","text":"<p>Status: Accepted Date: 2026-01-12 PR: #75 Issue: #48 Milestone: v0.2.0</p>"},{"location":"adr/007-input-validation/#context","title":"Context","text":"<p>DynamoDB keys use <code>#</code> as a delimiter (e.g., <code>ENTITY#user-1#BUCKET#gpt-4</code>). Without validation, malicious input containing <code>#</code> could:</p> <ul> <li>Traverse to different record types</li> <li>Access other entities' data</li> <li>Corrupt key structure</li> </ul> <p>This is analogous to SQL injection but for DynamoDB key-based access patterns.</p>"},{"location":"adr/007-input-validation/#decision","title":"Decision","text":"<p>Add comprehensive input validation at model construction time, forbidding the <code>#</code> delimiter character in all user-provided identifiers.</p> <p>Validation rules:</p> Field Pattern Max Length <code>entity_id</code>, <code>parent_id</code> <code>^[a-zA-Z0-9][a-zA-Z0-9_.\\-:@]*$</code> 256 <code>limit_name</code>, <code>resource</code> <code>^[a-zA-Z][a-zA-Z0-9_.\\-]*$</code> 64 <p>New exceptions: - <code>ValidationError</code> (base) - <code>InvalidIdentifierError</code> (entity_id, parent_id) - <code>InvalidNameError</code> (limit_name, resource)</p>"},{"location":"adr/007-input-validation/#consequences","title":"Consequences","text":"<p>Positive: - Injection attacks impossible by construction - Early failure with clear error messages - Supports common ID formats: UUIDs, API keys, emails - Validation happens once at model creation, not on every operation</p> <p>Negative: - Some previously-valid IDs may be rejected (containing <code>#</code>) - Migration required for existing data with invalid characters</p>"},{"location":"adr/007-input-validation/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Escape/encode special characters: Rejected; complex, error-prone, obscures data in DynamoDB console</li> <li>Validate only at repository layer: Rejected; too late, models could be passed around with invalid data</li> <li>Allowlist specific formats (UUID only): Rejected; too restrictive for diverse use cases</li> </ul>"},{"location":"adr/008-security-audit-logging/","title":"ADR-008: Security Audit Logging","text":"<p>Status: Accepted Date: 2026-01-12 PR: #76 Issue: #47 Milestone: v0.2.0</p>"},{"location":"adr/008-security-audit-logging/#context","title":"Context","text":"<p>Rate limiting systems control access to resources, making them security-sensitive. Compliance requirements (SOC 2, HIPAA) and security best practices require audit trails for:</p> <ul> <li>Who changed rate limits and when</li> <li>Entity lifecycle (creation, deletion)</li> <li>Configuration modifications</li> </ul> <p>Without audit logging, investigating security incidents or proving compliance is difficult.</p>"},{"location":"adr/008-security-audit-logging/#decision","title":"Decision","text":"<p>Store audit events in DynamoDB alongside rate limiting data, with automatic TTL-based expiration.</p> <p>Audit events logged:</p> Action Trigger Details <code>entity_created</code> <code>create_entity()</code> name, parent_id, metadata <code>entity_deleted</code> <code>delete_entity()</code> records_deleted count <code>limits_set</code> <code>set_limits()</code> resource, limits config <code>limits_deleted</code> <code>delete_limits()</code> resource <p>Key structure: <code>PK=AUDIT#{entity_id}, SK=#AUDIT#{timestamp}</code></p> <p>Optional <code>principal</code> parameter tracks who performed the action.</p>"},{"location":"adr/008-security-audit-logging/#consequences","title":"Consequences","text":"<p>Positive: - Full audit trail without external dependencies - Same access patterns as other data (single table) - Automatic cleanup via DynamoDB TTL (default 90 days) - Principal tracking enables accountability</p> <p>Negative: - Storage cost for audit records (mitigated by TTL) - No real-time alerting (would require separate system) - Audit records deleted with TTL, not archived (see #77 for S3 archival)</p>"},{"location":"adr/008-security-audit-logging/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>CloudTrail only: Rejected; doesn't capture application-level details (which limits changed)</li> <li>External audit service: Rejected; adds dependency, latency, cost</li> <li>Separate audit table: Rejected; loses transactional consistency with entity operations</li> </ul>"},{"location":"adr/009-version-migration-system/","title":"ADR-009: Schema Version Migration System","text":"<p>Status: Accepted Date: 2026-01-10 PR: #15 Milestone: v0.1.0</p>"},{"location":"adr/009-version-migration-system/#context","title":"Context","text":"<p>As the library evolves, DynamoDB schema changes may be required. Without version tracking:</p> <ul> <li>Clients can't detect incompatible infrastructure</li> <li>No mechanism to run schema migrations</li> <li>Lambda and client code can drift out of sync</li> <li>Rolling updates are unsafe</li> </ul>"},{"location":"adr/009-version-migration-system/#decision","title":"Decision","text":"<p>Implement version tracking in both DynamoDB and CloudFormation, with a migration framework for schema changes.</p> <p>Components:</p> <ol> <li>Version record: <code>PK=SYSTEM#, SK=#VERSION</code> stores current schema version</li> <li>Stack tags: CloudFormation stack tagged with <code>SchemaVersion</code></li> <li>Lambda env var: <code>SCHEMA_VERSION</code> for runtime compatibility checks</li> <li>Migration registry: Python framework for versioned migrations</li> </ol> <p>Client parameters: - <code>auto_update</code>: Auto-update Lambda on version mismatch (default: True) - <code>strict_version</code>: Fail if version mismatch (default: True) - <code>skip_version_check</code>: Bypass all checks (default: False)</p>"},{"location":"adr/009-version-migration-system/#consequences","title":"Consequences","text":"<p>Positive: - Safe rolling updates: detect incompatibility before operations - Automated Lambda updates when client library upgraded - Migration framework ready for future schema changes - Clear versioning across all components</p> <p>Negative: - Additional DynamoDB read on first operation (version check) - Complexity in version comparison logic - Migrations must be backward-compatible or coordinated</p>"},{"location":"adr/009-version-migration-system/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>No versioning: Rejected; silent failures on schema mismatch, unsafe upgrades</li> <li>Stack tags only: Rejected; doesn't help Lambda or clients detect issues at runtime</li> <li>External version store (SSM): Rejected; adds dependency, latency, different IAM permissions</li> </ul>"},{"location":"adr/010-total-consumed-counter/","title":"ADR-010: Total Consumed Counter for Accurate Delta Tracking","text":"<p>Status: Accepted Date: 2026-01-16 PR: #169 Issue: #179 Milestone: v0.4.0</p>"},{"location":"adr/010-total-consumed-counter/#context","title":"Context","text":"<p>The Lambda aggregator processes DynamoDB Streams to create usage snapshots. Initial implementation derived consumption from token bucket state changes:</p> <pre><code>consumption = old_tokens - new_tokens\n</code></pre> <p>This fails when token refill rate exceeds consumption rate. Example with 10M TPM limit: - Refill during 100ms operation: ~16,667 tokens - Actual consumption: 1,000 tokens - Calculated delta: -15,667 (wrong!)</p> <p>The fundamental problem: token bucket state conflates consumption with refill.</p>"},{"location":"adr/010-total-consumed-counter/#decision","title":"Decision","text":"<p>Add <code>total_consumed_milli</code> counter that tracks net consumption independently of refill.</p> <p>Implementation: - Counter stored as flat top-level attribute (not nested in <code>data.M</code>) - Consume: <code>counter += amount * 1000</code> - Release/adjust(negative): <code>counter -= amount * 1000</code> - Delta calculation: <code>new_counter - old_counter</code></p> <p>Why flat attribute: Enables atomic ADD operations in UpdateExpression without \"overlapping document paths\" error (same reason as ADR-006).</p>"},{"location":"adr/010-total-consumed-counter/#consequences","title":"Consequences","text":"<p>Positive: - Accurate consumption tracking regardless of refill rate - Works at any scale (tested with 10M+ TPM) - Simple delta calculation in aggregator - Net tracking handles both consume and release operations</p> <p>Negative: - Hybrid schema: most bucket fields nested, counter flat - Additional attribute in every bucket record - Existing buckets need migration (counter initialized to 0)</p>"},{"location":"adr/010-total-consumed-counter/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Track consumption in separate record: Rejected; doubles write operations, transaction complexity</li> <li>Calculate from refill formula: Rejected; requires knowing exact timing, error-prone</li> <li>Store last_consumed timestamp: Rejected; doesn't handle variable consumption rates</li> </ul>"},{"location":"adr/011-api-cli-parity/","title":"ADR-011: API/CLI Interface Parity","text":"<p>Status: Accepted Date: 2026-01-15 PRs: #141, #153, #188 Milestone: v0.3.0</p>"},{"location":"adr/011-api-cli-parity/#context","title":"Context","text":"<p>zae-limiter serves two distinct user personas:</p> <ol> <li>Developers integrate rate limiting into applications via Python API</li> <li>Operators manage infrastructure, debug issues, and run reports via CLI</li> </ol> <p>Early development added features to one interface without considering the other, creating gaps: - <code>get_status()</code> existed in API but no CLI equivalent - <code>audit list</code> existed in CLI but no API method - Naming inconsistencies between interfaces</p>"},{"location":"adr/011-api-cli-parity/#decision","title":"Decision","text":"<p>Maintain interface parity with explicit exceptions based on use case.</p> <p>Parity matrix:</p> Feature Type API CLI Rationale Infrastructure ops \u2705 \u2705 Both personas manage stacks Data queries \u2705 \u2705 Debugging from code or terminal Admin actions \u2705 \u2705 Automation needs both Runtime limiting \u2705 \u274c Only meaningful in application context Template exports \u274c \u2705 One-time ops task, not programmatic <p>Naming alignment: API method names map predictably to CLI commands.</p>"},{"location":"adr/011-api-cli-parity/#consequences","title":"Consequences","text":"<p>Positive: - Consistent user experience across interfaces - Features discoverable in both contexts - Automation scripts can use either interface - Documentation covers both without gaps</p> <p>Negative: - More implementation work per feature (two interfaces) - Must maintain naming discipline across interfaces - Some features feel forced in one interface (e.g., <code>cfn-template</code> as API)</p>"},{"location":"adr/011-api-cli-parity/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>CLI wraps API only: Rejected; some operations are CLI-native (interactive prompts, template export)</li> <li>API only: Rejected; operators need CLI for shell scripts and debugging</li> <li>No parity requirement: Rejected; leads to fragmented UX and documentation gaps</li> </ul>"},{"location":"adr/012-cloudformation-docs-parity/","title":"ADR-012: CloudFormation Documentation Parity","text":"<p>Status: Superseded by ADR-013 Date: 2026-01-19</p>"},{"location":"adr/012-cloudformation-docs-parity/#context","title":"Context","text":"<p>The CloudFormation template (<code>cfn_template.yaml</code>) defines all infrastructure resources for zae-limiter. The operator documentation (<code>docs/infra/cloudformation.md</code>) includes a Mermaid diagram and detailed documentation of parameters, resources, and outputs.</p> <p>When the template is modified (new parameters, resources, conditions, or outputs), the documentation can become stale, leading to: - Operators discovering features only by reading the template - Mermaid diagram missing new resource relationships - Parameter tables incomplete or outdated - Output tables missing new exports</p>"},{"location":"adr/012-cloudformation-docs-parity/#decision","title":"Decision","text":"<p>Whenever the CloudFormation template is modified, the documentation at <code>docs/infra/cloudformation.md</code> must be updated to reflect:</p> <ol> <li>Mermaid diagram - Add/remove resources and relationships</li> <li>Parameters table - Add/remove/update parameter descriptions</li> <li>Outputs table - Add/remove stack outputs</li> <li>Resource sections - Document new resource types</li> </ol> <p>This is enforced through the <code>docs-updater</code> agent which is invoked after infrastructure changes.</p>"},{"location":"adr/012-cloudformation-docs-parity/#consequences","title":"Consequences","text":"<p>Positive: - Operators can rely on documentation being current - Mermaid diagram provides accurate visual overview - Parameter and output tables are authoritative references</p> <p>Negative: - Additional work when modifying CloudFormation template - Requires discipline to invoke docs-updater agent</p>"},{"location":"adr/012-cloudformation-docs-parity/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Auto-generate docs from template: Rejected; loses narrative context and customization examples</li> <li>No parity requirement: Rejected; documentation drift degrades operator experience</li> </ul>"},{"location":"adr/013-module-documentation-parity/","title":"ADR-013: Bidirectional Source-Documentation Parity","text":"<p>Status: Proposed Date: 2026-01-19 Supersedes: ADR-012</p>"},{"location":"adr/013-module-documentation-parity/#context","title":"Context","text":"<p>Source code and documentation frequently drift out of sync. When <code>limiter.py</code> adds a <code>repository</code> parameter, the getting-started guide still shows the old constructor. When <code>cli.py</code> adds <code>--enable-tracing</code>, the CLI reference is missing it. When someone updates docs to describe a planned feature, users try to use APIs that don't exist yet.</p> <p>The project has narrow-scope ADRs (ADR-011 for API/CLI parity, ADR-012 for CloudFormation docs), but contributors must guess which documentation files need updating for changes outside those areas.</p>"},{"location":"adr/013-module-documentation-parity/#decision","title":"Decision","text":"<p>Maintain a bidirectional mapping between source modules and documentation. When either side changes, verify the other is synchronized.</p> Source Documentation What to Sync <code>limiter.py</code> <code>docs/api/limiter.md</code>, <code>docs/guide/basic-usage.md</code> Constructor signatures, method examples <code>models.py</code> <code>docs/api/models.md</code> Dataclass fields, factory methods <code>exceptions.py</code> <code>docs/api/exceptions.md</code> Exception classes, error fields <code>cli.py</code> <code>docs/cli.md</code> Commands, flags, usage examples <code>repository.py</code> <code>docs/contributing/architecture.md</code> Protocol methods, schema patterns <code>schema.py</code> <code>docs/contributing/architecture.md</code> Key patterns, GSI definitions <code>bucket.py</code> <code>docs/guide/token-bucket.md</code> Algorithm formulas, refill math <code>infra/cfn_template.yaml</code> <code>docs/infra/cloudformation.md</code> Mermaid diagram, parameters, outputs <code>infra/</code> (other) <code>docs/infra/deployment.md</code> CLI examples, StackOptions fields <code>aggregator/</code> <code>docs/guide/usage-snapshots.md</code> Stream processing, snapshot schema <code>__init__.py</code> <code>docs/api/index.md</code> Public exports, module docstring"},{"location":"adr/013-module-documentation-parity/#consequences","title":"Consequences","text":"<p>Positive: - Contributors know exactly which files to update for any change - Users can trust that documented examples match actual behavior - Reviewers can check documentation coverage during PR review</p> <p>Negative: - Every code change requires checking corresponding documentation - Matrix needs updating when new modules are added</p>"},{"location":"adr/013-module-documentation-parity/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Convention-based mapping (e.g., <code>foo.py</code> \u2192 <code>docs/foo.md</code>): Rejected; our docs structure doesn't mirror source structure, and implicit rules are easily forgotten</li> <li>Documentation-only reviews: Rejected; catching drift after the fact means users already hit broken examples</li> </ul>"},{"location":"adr/100-centralized-config/","title":"ADR-100: Centralized Configuration Access Patterns","text":"<p>Status: Proposed Date: 2026-01-18 Issue: #129 Milestone: v0.5.0</p>"},{"location":"adr/100-centralized-config/#context","title":"Context","text":"<p>zae-limiter is a distributed rate limiting library where multiple clients must behave consistently. Currently:</p> <ol> <li>Limits passed explicitly - Each <code>acquire()</code> call requires limits</li> <li>No global defaults - Cannot set system-wide or resource-level default limits</li> <li>No caching - <code>use_stored_limits=True</code> queries DynamoDB on every call</li> <li>Scattered config - Behavior settings are constructor-only, risking inconsistent fail-open/fail-closed behavior</li> </ol>"},{"location":"adr/100-centralized-config/#decision","title":"Decision","text":"<p>Implement centralized configuration with these architectural choices:</p> Decision ADR Summary Schema format ADR-101 Flat schema (no nested <code>data.M</code>) for atomic counters Config hierarchy ADR-102 Three levels: System &gt; Resource &gt; Entity Caching strategy ADR-103 60s TTL with negative caching API behavior ADR-104 Stored limits as default Read consistency ADR-105 Eventually consistent reads"},{"location":"adr/100-centralized-config/#consequences","title":"Consequences","text":"<p>Positive: - Consistent behavior across distributed clients - Negligible cost with caching (~0.00025 RCU/request at scale) - Enables per-resource and per-entity customization - Clean upgrade path to v0.6.0 full schema migration</p> <p>Negative: - Max 60s staleness for config changes - Additional complexity in resolution logic - Breaking change for explicit-limits-only users</p>"},{"location":"adr/100-centralized-config/#access-patterns-added","title":"Access Patterns Added","text":"Pattern Query Index Get system config <code>PK=SYSTEM#, SK begins_with #LIMIT#</code> Primary Get resource config <code>PK=RESOURCE#{resource}, SK begins_with #LIMIT#</code> Primary Get entity config <code>PK=ENTITY#{id}, SK begins_with #LIMIT#</code> Primary"},{"location":"adr/100-centralized-config/#implementation","title":"Implementation","text":"<p>See linked issues for implementation details:</p> <ul> <li>#130 - Store system/resource config</li> <li>#131 - System-level default limits</li> <li>#135 - Client-side config cache</li> <li>#180 - v0.6.0 full schema flattening</li> </ul>"},{"location":"adr/101-flat-schema-config/","title":"ADR-101: Flat Schema for Config Records","text":"<p>Status: Accepted Date: 2026-01-18 Issue: #129</p>"},{"location":"adr/101-flat-schema-config/#context","title":"Context","text":"<p>The codebase has three DynamoDB schema patterns:</p> Pattern Records Use Case Nested <code>data.M</code> Entities, Limits, Audit, Version No atomic counters needed Hybrid Buckets (<code>total_consumed_milli</code> flat) Mostly nested + one atomic counter Flat Snapshots Atomic upsert with ADD counters <p>DynamoDB rejects UpdateExpressions that combine <code>SET #data = if_not_exists(#data, :map)</code> with <code>ADD #data.counter :delta</code> due to \"overlapping document paths\" (issues #168, #179).</p> <p>New config records need atomic <code>config_version</code> counter increments for cache invalidation.</p>"},{"location":"adr/101-flat-schema-config/#decision","title":"Decision","text":"<p>Use flat schema (no nested <code>data.M</code>) for all new config records at System, Resource, and Entity levels. This matches the snapshot pattern established in v0.4.0.</p> <p>v0.6.0 recommendation: Flatten all existing record types (entities, limits, audit, version) for consistency. See #180.</p>"},{"location":"adr/101-flat-schema-config/#consequences","title":"Consequences","text":"<p>Positive: - Enables atomic <code>config_version</code> counter increments - Consistent with snapshot pattern (v0.4.0) - Sets standard: flat schema for all new records - Forward compatible with v0.6.0 full schema migration</p> <p>Negative: - v0.6.0 will require migration work to flatten existing records - Two schema patterns coexist until v0.6.0</p>"},{"location":"adr/101-flat-schema-config/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr/101-flat-schema-config/#nested-datam-schema","title":"Nested <code>data.M</code> Schema","text":"<p>Rejected: Inconsistent with flat snapshot pattern; DynamoDB prevents atomic counters with nested paths.</p>"},{"location":"adr/101-flat-schema-config/#hybrid-schema-like-buckets","title":"Hybrid Schema (like buckets)","text":"<p>Rejected: Adds complexity; better to standardize on flat for new records.</p>"},{"location":"adr/102-config-hierarchy/","title":"ADR-102: Three-Level Configuration Hierarchy","text":"<p>Status: Accepted Date: 2026-01-18 Issues: #129, #130, #131</p>"},{"location":"adr/102-config-hierarchy/#context","title":"Context","text":"<p>zae-limiter clients need consistent configuration across distributed instances. Currently:</p> <ol> <li>No global defaults - Cannot set system-wide default limits that apply to all resources</li> <li>No resource defaults - Cannot set per-resource limits without per-entity configuration</li> <li>Scattered config - Behavior settings (<code>on_unavailable</code>) are constructor-only</li> <li>Risk of inconsistency - Different clients may have different fail-open/fail-closed behavior</li> </ol> <p>Operators need to set global defaults while allowing per-resource and per-entity overrides.</p>"},{"location":"adr/102-config-hierarchy/#decision","title":"Decision","text":"<p>Implement a three-level configuration hierarchy with precedence: Entity &gt; Resource &gt; System &gt; Constructor defaults.</p> Level PK SK Purpose System <code>SYSTEM#</code> <code>#LIMIT#{limit_name}</code> Global defaults for ALL resources System <code>SYSTEM#</code> <code>#CONFIG</code> Behavior config (<code>on_unavailable</code>, etc.) Resource <code>RESOURCE#{resource}</code> <code>#LIMIT#{limit_name}</code> Per-resource overrides Entity <code>ENTITY#{id}</code> <code>#LIMIT#{resource}#{limit_name}</code> Per-entity+resource overrides <p>Key distinction: - System limits apply universally (no resource association) - Resource limits override system defaults for a specific resource - Entity limits override resource/system defaults for a specific entity+resource pair</p> <p>Config field scope:</p> Field System Resource Entity Limit fields (<code>capacity</code>, etc.) \u2705 \u2705 \u2705 <code>on_unavailable</code> \u2705 \u274c \u274c <code>auto_update</code>, <code>strict_version</code> \u2705 \u274c \u274c"},{"location":"adr/102-config-hierarchy/#consequences","title":"Consequences","text":"<p>Positive: - Consistent behavior across distributed clients - Enables per-resource overrides (expensive model \u2192 lower limits) - Premium users can have different limits via entity config - Clean separation: system = global, resource = per-model, entity = per-user</p> <p>Negative: - 3 levels to check per cache miss (mitigated by caching, see ADR-103) - More complex resolution logic</p>"},{"location":"adr/102-config-hierarchy/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr/102-config-hierarchy/#single-level-system-only","title":"Single-level (System only)","text":"<p>Rejected: No per-resource or per-entity customization; insufficient for real-world use cases.</p>"},{"location":"adr/102-config-hierarchy/#two-level-system-entity","title":"Two-level (System + Entity)","text":"<p>Rejected: Resource-level is common (different limits per model); would force entity-level duplication.</p>"},{"location":"adr/102-config-hierarchy/#system-config-keyed-by-resource-original-design","title":"System config keyed by resource (original design)","text":"<p>Rejected: Redundant with resource-level config; system should be truly global defaults.</p>"},{"location":"adr/103-config-caching/","title":"ADR-103: Client-Side Config Caching with TTL","text":"<p>Status: Proposed Date: 2026-01-18 Issue: #135</p>"},{"location":"adr/103-config-caching/#context","title":"Context","text":"<p>The three-level config hierarchy (ADR-102) requires fetching 3 DynamoDB items per <code>acquire()</code> call. Without caching:</p> <ul> <li>Cost: 1.5 RCU per request (unacceptable at scale)</li> <li>Latency: BatchGetItem round-trip on every request</li> </ul> <p>Config changes are infrequent (typically during deployment or admin operations), making aggressive caching appropriate.</p>"},{"location":"adr/103-config-caching/#decision","title":"Decision","text":"<p>Implement in-memory TTL caching per RateLimiter instance with 60-second TTL and negative caching.</p> <p>Negative caching: Cache \"no entity config exists\" to avoid repeated misses for the 95%+ of users without custom limits.</p> <p>Cache invalidation: - Automatic: TTL expiry (60s) - Manual: <code>limiter.invalidate_config_cache()</code> method</p> <p>No distributed invalidation: Config changes propagate via TTL expiry (max 60s staleness). This avoids infrastructure complexity (SNS/EventBridge) for infrequent operations.</p>"},{"location":"adr/103-config-caching/#consequences","title":"Consequences","text":"<p>Positive: - High-frequency traffic: 99.98% cache hit rate (100 req/sec \u00d7 60s = 6K hits per miss) - Negligible amortized cost: +0.00025 RCU per request at scale - Negative caching reduces cost for sparse traffic patterns</p> <p>Negative: - Max 60s staleness for config changes - No cross-process invalidation (each instance has independent cache) - Memory usage scales with unique entity\u00d7resource combinations</p>"},{"location":"adr/103-config-caching/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr/103-config-caching/#no-caching","title":"No Caching","text":"<p>Rejected: 1.5 RCU per acquire is unacceptable; poor latency.</p>"},{"location":"adr/103-config-caching/#distributed-cache-rediselasticache","title":"Distributed Cache (Redis/ElastiCache)","text":"<p>Rejected: Adds infrastructure dependency; 60s staleness is acceptable for config.</p>"},{"location":"adr/103-config-caching/#dynamodb-streams-for-invalidation","title":"DynamoDB Streams for Invalidation","text":"<p>Rejected: Requires Lambda infrastructure; complexity not justified for config updates.</p>"},{"location":"adr/104-stored-limits-default/","title":"ADR-104: Stored Limits as Default Behavior","text":"<p>Status: Accepted Date: 2026-01-18 Issue: #130</p>"},{"location":"adr/104-stored-limits-default/#context","title":"Context","text":"<p>Currently, <code>acquire()</code> requires explicit limits or opt-in via <code>use_stored_limits=True</code>:</p> <pre><code>async with limiter.acquire(\n    entity_id=\"user-1\",\n    resource=\"gpt-4\",\n    limits=[Limit.per_minute(\"tpm\", 10000)],  # Required\n    use_stored_limits=False,  # Default\n):\n    ...\n</code></pre> <p>This creates friction: users must pass limits on every call or remember to opt-in. With centralized config (ADR-102), stored limits become the natural default.</p>"},{"location":"adr/104-stored-limits-default/#decision","title":"Decision","text":"<p>Change the default behavior: always resolve limits from stored config (System &gt; Resource &gt; Entity hierarchy).</p> <p>Resolution order: 1. Entity config \u2192 if exists, use it 2. Resource config \u2192 if exists, use it 3. System config \u2192 fallback 4. Error if no config found anywhere</p> <p>Backward compatibility: - <code>limits</code> parameter accepted as override (useful for testing, migration) - <code>use_stored_limits=False</code> deprecated with warning in v0.5.0, removed in v1.0</p>"},{"location":"adr/104-stored-limits-default/#consequences","title":"Consequences","text":"<p>Positive: - Simpler API: no limits parameter needed in common case - Centralized control: ops can change limits without code deployment - Consistent behavior: all clients use same stored config</p> <p>Negative: - Breaking change for users relying on explicit limits only - Requires config to be set up before use (or <code>limits</code> override)</p>"},{"location":"adr/104-stored-limits-default/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr/104-stored-limits-default/#keep-opt-in-use_stored_limitstrue","title":"Keep Opt-In (<code>use_stored_limits=True</code>)","text":"<p>Rejected: Adds friction; stored config is the better default now that hierarchy exists.</p>"},{"location":"adr/104-stored-limits-default/#remove-limits-parameter-entirely","title":"Remove <code>limits</code> Parameter Entirely","text":"<p>Rejected: Useful for testing and gradual migration; keep as override option.</p>"},{"location":"adr/105-eventual-consistency/","title":"ADR-105: Eventually Consistent Reads for Config","text":"<p>Status: Accepted Date: 2026-01-18 Issue: #129</p>"},{"location":"adr/105-eventual-consistency/#context","title":"Context","text":"<p>DynamoDB offers two read consistency modes:</p> Consistency Cost Typical Latency Strongly consistent 1 RCU / 4KB Higher Eventually consistent 0.5 RCU / 4KB Lower <p>Config reads fetch 3 items per cache miss (System, Resource, Entity). With caching (ADR-103), we already accept 60s staleness for config changes.</p>"},{"location":"adr/105-eventual-consistency/#decision","title":"Decision","text":"<p>Use eventually consistent reads for all config fetches.</p> <p>Rationale: Since the caching layer accepts 60s staleness, sub-second DynamoDB eventual consistency is negligible. This reduces config fetch cost from 3 RCU to 1.5 RCU per cache miss.</p>"},{"location":"adr/105-eventual-consistency/#consequences","title":"Consequences","text":"<p>Positive: - 50% RCU cost reduction for config reads - Lower latency (eventually consistent reads are faster) - Aligned with caching semantics (staleness already accepted)</p> <p>Negative: - Theoretical sub-second staleness on config reads (negligible given 60s cache TTL)</p>"},{"location":"adr/105-eventual-consistency/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr/105-eventual-consistency/#strongly-consistent-reads","title":"Strongly Consistent Reads","text":"<p>Rejected: 2x cost for no practical benefit; caching already introduces 60s staleness window.</p>"},{"location":"adr/105-eventual-consistency/#mixed-consistency-strong-for-entity-eventual-for-systemresource","title":"Mixed Consistency (strong for entity, eventual for system/resource)","text":"<p>Rejected: Adds complexity; consistency should be uniform across config levels.</p>"},{"location":"adr/106-audit-entity-ids-for-config/","title":"ADR-106: Audit Entity IDs for Config Levels","text":"<p>Status: Accepted Date: 2026-01-19 Issue: #130</p>"},{"location":"adr/106-audit-entity-ids-for-config/#context","title":"Context","text":"<p>The audit logging system requires an <code>entity_id</code> for all audit events. This works naturally for entity-level operations where the entity_id is the actual entity being modified.</p> <p>However, system-level and resource-level config changes (introduced in #130) have no natural entity. System config applies globally, and resource config applies to a resource name rather than a specific entity.</p> <p>Additionally, operators need to query audit trails for config changes separately from entity changes. Mixing config audits with entity audits under arbitrary entity IDs would make compliance queries difficult.</p>"},{"location":"adr/106-audit-entity-ids-for-config/#decision","title":"Decision","text":"<p>Use special prefixes for config-level audit entity IDs: - System config: <code>$SYSTEM</code> - Resource config: <code>$RESOURCE:{resource_name}</code> (e.g., <code>$RESOURCE:gpt-4</code>)</p> <p>The <code>$</code> prefix is chosen because it cannot appear in valid entity IDs (which must start with alphanumeric characters per validation rules).</p>"},{"location":"adr/106-audit-entity-ids-for-config/#consequences","title":"Consequences","text":"<p>Positive: - Clear distinction between entity and config audit events - Operators can query all system config changes via <code>entity_id=$SYSTEM</code> - Resource config audits are grouped by resource name - No collision risk with real entity IDs</p> <p>Negative: - Introduces a reserved character convention that must be documented - Audit queries for \"all changes\" must now include both entity and <code>$</code>-prefixed patterns</p>"},{"location":"adr/106-audit-entity-ids-for-config/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr/106-audit-entity-ids-for-config/#use-literal-strings-without-prefix-eg-system","title":"Use literal strings without prefix (e.g., \"SYSTEM\")","text":"<p>Rejected: Could collide with actual entity IDs if a user creates an entity named \"SYSTEM\".</p>"},{"location":"adr/106-audit-entity-ids-for-config/#skip-audit-logging-for-config-changes","title":"Skip audit logging for config changes","text":"<p>Rejected: Config changes are security-sensitive operations that require audit trails for compliance.</p>"},{"location":"adr/107-iam-roles-for-application-access/","title":"ADR-107: IAM Roles for Application Access","text":"<p>Status: Accepted Date: 2026-01-19 Issue: #132</p>"},{"location":"adr/107-iam-roles-for-application-access/#context","title":"Context","text":"<p>Users deploying zae-limiter must create their own IAM policies to access the DynamoDB table. This requires understanding the exact DynamoDB actions needed for different use cases (applications vs. administrators vs. monitoring), leading to either overly permissive policies or trial-and-error debugging of permission errors.</p> <p>The CloudFormation stack currently only creates the Lambda execution role for the aggregator. Different deployment patterns require different access levels: applications need transactional write access for <code>acquire()</code>, administrators need full CRUD for configuration management, and monitoring systems need read-only access.</p>"},{"location":"adr/107-iam-roles-for-application-access/#decision","title":"Decision","text":"<p>The CloudFormation stack creates three optional IAM roles (AppRole, AdminRole, ReadOnlyRole) with least-privilege DynamoDB permissions, enabled by default and controlled via <code>--no-iam-roles</code> flag or <code>StackOptions.create_iam_roles=False</code>.</p>"},{"location":"adr/107-iam-roles-for-application-access/#consequences","title":"Consequences","text":"<p>Positive: - Easy onboarding with correct IAM permissions out-of-the-box - Least-privilege security enforced by default - Clear separation between app/admin/monitoring access patterns - Roles respect existing <code>permission_boundary</code> and <code>role_name_format</code> options - No new IAM permission requirements (stack already needs <code>iam:CreateRole</code> for Lambda)</p> <p>Negative: - Three additional IAM roles per stack increases IAM resource count - Users with existing IAM setup may have redundant roles (use <code>--no-iam-roles</code>)</p>"},{"location":"adr/107-iam-roles-for-application-access/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr/107-iam-roles-for-application-access/#inline-iam-policies-in-documentation-only","title":"Inline IAM policies in documentation only","text":"<p>Rejected because: Users still need to create and maintain policies manually, defeating the goal of easy onboarding.</p>"},{"location":"adr/107-iam-roles-for-application-access/#single-role-with-configurable-permissions","title":"Single role with configurable permissions","text":"<p>Rejected because: A single role can't satisfy least-privilege for different access patterns; applications shouldn't have config deletion rights.</p>"},{"location":"adr/107-iam-roles-for-application-access/#iam-policies-attached-to-user-provided-roles","title":"IAM policies attached to user-provided roles","text":"<p>Rejected because: Increases deployment complexity by requiring users to pre-create roles and pass ARNs as parameters.</p>"},{"location":"adr/108-repository-protocol/","title":"ADR-108: Repository Protocol Design","text":"<p>Status: Accepted Date: 2026-01-19 Issue: #150 Milestone: v0.5.0</p>"},{"location":"adr/108-repository-protocol/#context","title":"Context","text":"<p><code>RateLimiter</code> tightly couples business logic to DynamoDB by constructing its own <code>Repository</code> internally. This creates testing friction, backend lock-in, and misplaced infrastructure concerns. Users wanting alternative backends (Redis #149, SQLite #156, In-Memory #157) would need invasive changes, and third-party packages cannot implement backends without depending on zae-limiter.</p>"},{"location":"adr/108-repository-protocol/#decision","title":"Decision","text":"<p>Use Python's <code>typing.Protocol</code> with <code>@runtime_checkable</code> decorator to define <code>RepositoryProtocol</code>. This enables duck typing, third-party backends without zae-limiter dependency, and easy mock injection for testing.</p> <p>Key design choices:</p> <ol> <li>Protocol over ABC: Any object with matching methods satisfies the protocol\u2014no inheritance required</li> <li>Infrastructure ownership: Repository owns data access and infrastructure (<code>StackOptions</code>); RateLimiter owns business logic only</li> <li>Method categorization:</li> <li>Required: entity CRUD, bucket operations, transactions, limit config, lifecycle</li> <li>Optional: audit events, usage snapshots (backend-specific, not in protocol)</li> <li>Capability-gated: batch operations (detected via <code>capabilities</code> property)</li> <li>Infrastructure API: <code>ensure_infrastructure()</code> replaces <code>create_stack()</code>; <code>stack_options</code> passed to constructor, not method</li> </ol> <p>See #150 for implementation details and method signatures.</p>"},{"location":"adr/108-repository-protocol/#consequences","title":"Consequences","text":"<p>Positive: - Clean separation of concerns (data vs business logic) - Backend flexibility without breaking changes - Third-party extensibility without zae-limiter dependency - Testability via mock injection - Type safety with <code>@runtime_checkable</code></p> <p>Negative: - More verbose construction (two objects instead of one) - Deprecation period requires maintaining both constructor signatures - Optional methods require type narrowing to access</p>"},{"location":"adr/108-repository-protocol/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr/108-repository-protocol/#abstract-base-class-abc","title":"Abstract Base Class (ABC)","text":"<p>Rejected: Requires inheritance, preventing duck typing; less Pythonic for interfaces.</p>"},{"location":"adr/108-repository-protocol/#keep-stackoptions-on-ratelimiter","title":"Keep StackOptions on RateLimiter","text":"<p>Rejected: Conflates business logic with infrastructure; violates single responsibility.</p>"},{"location":"adr/108-repository-protocol/#separate-sync-and-async-protocols","title":"Separate Sync and Async Protocols","text":"<p>Rejected: Increases surface area; <code>SyncRateLimiter</code> can wrap async with <code>asyncio.run()</code>.</p>"},{"location":"adr/109-backend-capability-matrix/","title":"ADR-109: Backend Capability Matrix","text":"<p>Status: Accepted Date: 2026-01-19 Issue: #150 Milestone: v0.5.0</p>"},{"location":"adr/109-backend-capability-matrix/#context","title":"Context","text":"<p>The RepositoryProtocol extraction (#150) enables multiple storage backends (Redis #149, SQLite #156, In-Memory #157, Cosmos DB #158, Firestore #159, OCI NoSQL #160). Each backend has different capabilities. This ADR defines which features are required versus optional.</p>"},{"location":"adr/109-backend-capability-matrix/#decision","title":"Decision","text":"<p>Define a three-tier capability system:</p> Tier Requirement Examples Core Required for all backends Token bucket, entity CRUD, transactions Standard Expected for production Hierarchical limits, cascade, TTL Extended Backend-specific, optional Audit logging, usage snapshots, infrastructure <p>Core features (all backends MUST implement): entity CRUD, bucket operations, atomic transactions, limit configuration, lifecycle management.</p> <p>Standard features (production backends SHOULD implement): hierarchical entities via <code>parent_id</code>, cascade support, TTL management, optimistic locking.</p> <p>Extended features (declared via <code>BackendCapabilities</code>): audit logging, usage snapshots, infrastructure management, change streams, batch operations. Backends declare support via capability flags; RateLimiter checks before using.</p>"},{"location":"adr/109-backend-capability-matrix/#capability-matrix","title":"Capability Matrix","text":"Backend Audit Snapshots Infra Mgmt Streams Batch DynamoDB Yes Yes CloudFormation Yes Yes Redis Streams Consumer Manual Yes Pipeline SQLite Table Polling N/A No No In-Memory No No N/A No No Cosmos DB TBD TBD ARM/Terraform Yes Bulk Firestore TBD TBD Terraform Yes getAll <p>See #150 for method signatures and implementation details.</p>"},{"location":"adr/109-backend-capability-matrix/#consequences","title":"Consequences","text":"<p>Positive: - Clear contract for backend implementers - Users know what to expect from each backend - Core features guaranteed; extensibility without breaking changes</p> <p>Negative: - Feature disparity between backends - Documentation and testing matrix grows with each backend</p>"},{"location":"adr/109-backend-capability-matrix/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr/109-backend-capability-matrix/#all-features-required","title":"All features required","text":"<p>Rejected: Forces backends to implement unsuitable features (e.g., In-Memory with audit logging).</p>"},{"location":"adr/109-backend-capability-matrix/#no-capability-declaration","title":"No capability declaration","text":"<p>Rejected: Users can't discover available features.</p>"},{"location":"adr/109-backend-capability-matrix/#separate-protocols-per-tier","title":"Separate protocols per tier","text":"<p>Rejected: Overly complex; single protocol with capabilities is cleaner.</p>"},{"location":"adr/110-deprecation-constructor/","title":"ADR-110: Deprecation Strategy for RateLimiter Constructor","text":"<p>Status: Accepted Date: 2026-01-19 Issue: #150 Milestone: v0.5.0</p>"},{"location":"adr/110-deprecation-constructor/#context","title":"Context","text":"<p>The <code>RateLimiter</code> constructor conflates business logic (<code>on_unavailable</code>) with data access configuration (<code>name</code>, <code>region</code>, <code>endpoint_url</code>, <code>stack_options</code>). Per ADR-108, Repository should own data access and infrastructure, while RateLimiter owns only business logic. This requires deprecating the old constructor parameters.</p>"},{"location":"adr/110-deprecation-constructor/#decision","title":"Decision","text":"<p>Deprecate <code>name</code>, <code>region</code>, <code>endpoint_url</code>, and <code>stack_options</code> parameters on <code>RateLimiter</code> and <code>SyncRateLimiter</code>. Introduce a <code>repository</code> parameter accepting <code>RepositoryProtocol</code>.</p> <p>Deprecation rules: 1. Old parameters emit <code>DeprecationWarning</code> with <code>stacklevel=2</code> 2. Passing both <code>repository</code> and <code>name</code> raises <code>ValueError</code> 3. When neither provided, default to <code>Repository(name=\"limiter\")</code> for backward compatibility 4. Applies to both <code>RateLimiter</code> and <code>SyncRateLimiter</code></p> <p>Timeline:</p> Version Behavior v0.4.x Only old signature v0.5.0 Both work; old emits warning v0.6.0\u2013v1.x Warning remains v2.0.0 Old parameters removed <p>See #150 for migration guide and examples.</p>"},{"location":"adr/110-deprecation-constructor/#consequences","title":"Consequences","text":"<p>Positive: - Clear separation of concerns - Enables mock repositories for testing - Gradual migration with 1+ major version cycle</p> <p>Negative: - More verbose construction - Users must migrate before v2.0.0 - Docs must cover both patterns during transition</p>"},{"location":"adr/110-deprecation-constructor/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr/110-deprecation-constructor/#keep-parameters-add-repository-as-optional","title":"Keep Parameters, Add Repository as Optional","text":"<p>Rejected: Creates ambiguity; doesn't achieve separation goal.</p>"},{"location":"adr/110-deprecation-constructor/#remove-immediately-breaking-change","title":"Remove Immediately (Breaking Change)","text":"<p>Rejected: Violates semver; users face immediate breakage.</p>"},{"location":"adr/110-deprecation-constructor/#factory-method-ratelimiterfrom_repository","title":"Factory Method (<code>RateLimiter.from_repository()</code>)","text":"<p>Rejected: Two \"right\" ways; cleaner to have single constructor with deprecation.</p>"},{"location":"adr/111-flatten-all-records/","title":"ADR-111: Flatten All DynamoDB Records to Top-Level Attributes","text":"<p>Status: Accepted Date: 2026-01-25 Issue: #180</p>"},{"location":"adr/111-flatten-all-records/#context","title":"Context","text":"<p>The codebase accumulated three DynamoDB schema patterns: nested <code>data.M</code> maps (entities, limits, audit events, version records), hybrid (buckets with one flat counter), and flat (usage snapshots and config records). ADR-006 and ADR-010 introduced flat attributes to solve DynamoDB's \"overlapping document paths\" limitation. ADR-101 standardized flat schema for all new config records and explicitly recommended flattening existing record types in v0.6.0.</p> <p>The mixed patterns create inconsistency in serialization/deserialization code, complicate the aggregator Lambda (which must navigate nested paths), and make the codebase harder to maintain. Every new feature must decide which pattern to use, adding cognitive overhead.</p>"},{"location":"adr/111-flatten-all-records/#decision","title":"Decision","text":"<p>All DynamoDB record types must use flat schema (top-level attributes, no nested <code>data.M</code> wrapper). Deserialization reads flat format only. Pre-1.0.0 semver allows breaking changes without migration \u2014 existing nested records are not supported. Serialization produces only flat format.</p>"},{"location":"adr/111-flatten-all-records/#consequences","title":"Consequences","text":"<p>Positive: - Uniform schema across all record types eliminates pattern inconsistency - Simpler serialization code without <code>data.M</code> wrapper construction - Aggregator Lambda reads flat attributes directly instead of navigating nested paths - Enables atomic operations on any attribute without \"overlapping paths\" errors - No branching logic in deserializers</p> <p>Negative: - Existing nested <code>data.M</code> records from pre-0.6.0 deployments must be recreated (no migration path provided pre-1.0.0) - DynamoDB reserved words (<code>name</code>, <code>resource</code>, <code>action</code>, <code>timestamp</code>) require <code>ExpressionAttributeNames</code> aliases in all expressions</p>"},{"location":"adr/111-flatten-all-records/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr/111-flatten-all-records/#keep-nested-datam-for-existing-records","title":"Keep nested <code>data.M</code> for existing records","text":"<p>Rejected because: Perpetuates schema inconsistency; ADR-101 already committed to flattening in v0.6.0.</p>"},{"location":"adr/111-flatten-all-records/#big-bang-migration-write-stops-until-all-records-migrated","title":"Big-bang migration (write stops until all records migrated)","text":"<p>Rejected because: Requires downtime; pre-1.0.0 semver makes migration unnecessary \u2014 breaking changes are expected.</p>"},{"location":"adr/111-flatten-all-records/#flatten-only-on-read-no-serialization-changes","title":"Flatten only on read (no serialization changes)","text":"<p>Rejected because: Leaves old-format records indefinitely; new writes would still produce nested format, preventing convergence.</p>"},{"location":"adr/112-cascade-per-entity/","title":"ADR-112: Move Cascade from Per-Call to Per-Entity Configuration","text":"<p>Status: Accepted Date: 2026-01-25</p>"},{"location":"adr/112-cascade-per-entity/#context","title":"Context","text":"<p>Cascade mode controls whether <code>acquire()</code> consumes tokens from both a child entity and its parent. Currently cascade is a per-call boolean parameter on <code>acquire(cascade=True)</code>, requiring callers to remember to pass it on every call. This is error-prone: forgetting <code>cascade=True</code> silently skips parent enforcement, and different call sites for the same entity may pass inconsistent values.</p> <p>Cascade is a property of the entity's relationship to its parent \u2014 it describes how the child participates in the hierarchy, not a per-request decision. Storing it on the entity makes the behavior consistent and eliminates a class of caller errors.</p> <p>The library is pre-1.0, so breaking parameter removal is acceptable without a deprecation period.</p>"},{"location":"adr/112-cascade-per-entity/#decision","title":"Decision","text":"<p>The <code>cascade</code> parameter must be removed from <code>acquire()</code> and stored as a <code>cascade: bool = False</code> attribute on the child entity's <code>#META</code> record. <code>create_entity()</code> must accept <code>cascade</code> as a parameter. <code>_do_acquire()</code> must read <code>entity.cascade</code> to determine whether to include the parent in the transaction.</p> <p><code>cascade</code> is a DynamoDB reserved word and must use <code>ExpressionAttributeNames</code> aliases in all expressions, consistent with the existing handling of <code>name</code>, <code>resource</code>, <code>action</code>, and <code>timestamp</code> (ADR-111).</p> <p>Existing entities without the <code>cascade</code> attribute must deserialize with <code>cascade=False</code> (backward compatible default).</p>"},{"location":"adr/112-cascade-per-entity/#consequences","title":"Consequences","text":"<p>Positive: - Cascade behavior is consistent for all calls to the same entity - Callers cannot accidentally omit cascade - Entity metadata is self-describing \u2014 inspecting the entity reveals its cascade behavior - No new DynamoDB reads: <code>acquire()</code> already fetches the entity's <code>#META</code> record</p> <p>Negative: - Breaking change: all callers passing <code>cascade=True</code> to <code>acquire()</code> must move it to <code>create_entity()</code> - Changing cascade for an existing entity requires entity recreation or a new <code>update_entity()</code> API</p>"},{"location":"adr/112-cascade-per-entity/#alternatives-considered","title":"Alternatives Considered","text":""},{"location":"adr/112-cascade-per-entity/#keep-cascade-as-per-call-parameter","title":"Keep cascade as per-call parameter","text":"<p>Rejected because: Error-prone, inconsistent across call sites, and conflates a static entity property with a runtime decision.</p>"},{"location":"adr/112-cascade-per-entity/#store-as-enum-noneparentall_ancestors","title":"Store as enum (<code>none|parent|all_ancestors</code>)","text":"<p>Rejected because: The codebase only supports 2-level hierarchy. A boolean is sufficient; a future ADR can supersede if deeper cascading is needed.</p>"},{"location":"adr/112-cascade-per-entity/#store-on-parent-entity-instead-of-child","title":"Store on parent entity instead of child","text":"<p>Rejected because: The child decides whether to cascade up. Storing on the parent would require the parent to know about all children's behavior, adding coordination overhead.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section contains the complete API documentation for zae-limiter, auto-generated from source code docstrings.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>The main components of the API are:</p> Component Description <code>RateLimiter</code> Async rate limiter client <code>SyncRateLimiter</code> Synchronous rate limiter client <code>Limit</code> Rate limit configuration <code>CacheStats</code> Cache performance statistics <code>RateLimitExceeded</code> Exception when limit is exceeded"},{"location":"api/#quick-reference","title":"Quick Reference","text":""},{"location":"api/#creating-a-limiter","title":"Creating a Limiter","text":"<pre><code>from zae_limiter import RateLimiter, SyncRateLimiter\n\n# Async\nlimiter = RateLimiter(\n    name=\"limiter\",\n    region=\"us-east-1\",\n)\n\n# Sync\nlimiter = SyncRateLimiter(\n    name=\"limiter\",\n    region=\"us-east-1\",\n)\n</code></pre>"},{"location":"api/#defining-limits","title":"Defining Limits","text":"<pre><code>from zae_limiter import Limit\n\n# Factory methods\nLimit.per_second(\"rps\", 10)\nLimit.per_minute(\"rpm\", 100)\nLimit.per_hour(\"rph\", 1000)\nLimit.per_day(\"rpd\", 10000)\n\n# With burst capacity\nLimit.per_minute(\"tpm\", 10_000, burst=15_000)\n\n# Custom period\nLimit.custom(\"requests\", capacity=50, refill_period_seconds=30)\n</code></pre>"},{"location":"api/#acquiring-limits","title":"Acquiring Limits","text":"<pre><code>from zae_limiter import RateLimiter, Limit, RateLimitExceeded\n\nlimiter = RateLimiter(name=\"limiter\")\n\ntry:\n    async with limiter.acquire(\n        entity_id=\"user-123\",\n        resource=\"api\",\n        limits=[Limit.per_minute(\"rpm\", 100)],\n        consume={\"rpm\": 1},\n    ) as lease:\n        # Do work\n        await lease.adjust(rpm=5)  # Adjust if needed\nexcept RateLimitExceeded as e:\n    print(f\"Retry after: {e.retry_after_seconds}s\")\n</code></pre>"},{"location":"api/#handling-exceptions","title":"Handling Exceptions","text":"<pre><code>from zae_limiter import RateLimitExceeded, RateLimiterUnavailable\n\ntry:\n    async with limiter.acquire(...):\n        pass\nexcept RateLimitExceeded as e:\n    # Rate limit exceeded\n    print(e.retry_after_seconds)\n    print(e.violations)\n    print(e.as_dict())\nexcept RateLimiterUnavailable as e:\n    # DynamoDB unavailable\n    print(f\"Service unavailable: {e}\")\n</code></pre>"},{"location":"api/#module-structure","title":"Module Structure","text":"<pre><code>zae_limiter/\n\u251c\u2500\u2500 __init__.py        # Public API exports\n\u251c\u2500\u2500 limiter.py         # RateLimiter, SyncRateLimiter\n\u251c\u2500\u2500 models.py          # Limit, Entity, LimitStatus, BucketState\n\u251c\u2500\u2500 exceptions.py      # RateLimitExceeded, RateLimiterUnavailable\n\u251c\u2500\u2500 lease.py           # Lease context manager\n\u251c\u2500\u2500 bucket.py          # Token bucket algorithm\n\u251c\u2500\u2500 schema.py          # DynamoDB key builders\n\u251c\u2500\u2500 repository.py      # DynamoDB operations\n\u2514\u2500\u2500 cli.py             # CLI commands\n</code></pre>"},{"location":"api/#public-exports","title":"Public Exports","text":"<p>The following are exported from <code>zae_limiter</code>:</p> <pre><code>from zae_limiter import (\n    # Main classes\n    RateLimiter,\n    SyncRateLimiter,\n    Lease,\n    SyncLease,\n\n    # Models\n    Limit,\n    LimiterInfo,\n    Entity,\n    LimitStatus,\n    BucketState,\n    CacheStats,\n\n    # Audit\n    AuditEvent,\n    AuditAction,\n\n    # Enums\n    OnUnavailable,\n\n    # Exceptions - Base\n    ZAELimiterError,\n\n    # Exceptions - Rate Limit\n    RateLimitExceeded,\n\n    # Exceptions - Entity\n    EntityNotFoundError,\n    EntityExistsError,\n\n    # Exceptions - Infrastructure\n    RateLimiterUnavailable,\n    StackCreationError,\n    StackAlreadyExistsError,\n    InfrastructureNotFoundError,\n\n    # Exceptions - Version\n    VersionMismatchError,\n    IncompatibleSchemaError,\n)\n</code></pre>"},{"location":"api/#detailed-documentation","title":"Detailed Documentation","text":"<ul> <li>RateLimiter - Main rate limiter classes</li> <li>Models - Data models and configuration</li> <li>Exceptions - Exception types and handling</li> </ul>"},{"location":"api/exceptions/","title":"Exceptions","text":"<p>Exception types raised by zae-limiter.</p>"},{"location":"api/exceptions/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>ZAELimiterError (base)\n\u251c\u2500\u2500 RateLimitError\n\u2502   \u251c\u2500\u2500 RateLimitExceeded\n\u2502   \u2514\u2500\u2500 RateLimiterUnavailable\n\u251c\u2500\u2500 EntityError\n\u2502   \u251c\u2500\u2500 EntityNotFoundError\n\u2502   \u2514\u2500\u2500 EntityExistsError\n\u251c\u2500\u2500 InfrastructureError\n\u2502   \u251c\u2500\u2500 StackCreationError\n\u2502   \u251c\u2500\u2500 StackAlreadyExistsError\n\u2502   \u2514\u2500\u2500 InfrastructureNotFoundError\n\u2514\u2500\u2500 VersionError\n    \u251c\u2500\u2500 VersionMismatchError\n    \u2514\u2500\u2500 IncompatibleSchemaError\n</code></pre>"},{"location":"api/exceptions/#base-exception","title":"Base Exception","text":""},{"location":"api/exceptions/#zae_limiter.exceptions.ZAELimiterError","title":"ZAELimiterError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all zae-limiter errors.</p> <p>All exceptions raised by this library inherit from this class, allowing callers to catch all library-specific errors with a single except clause.</p>"},{"location":"api/exceptions/#rate-limit-exceptions","title":"Rate Limit Exceptions","text":""},{"location":"api/exceptions/#zae_limiter.exceptions.RateLimitExceeded","title":"RateLimitExceeded","text":"<pre><code>RateLimitExceeded(statuses)\n</code></pre> <p>               Bases: <code>RateLimitError</code></p> <p>Raised when one or more rate limits would be exceeded.</p> <p>Provides full visibility into ALL limits that were checked, both passed and failed, to help callers understand the full picture.</p> <p>Attributes:</p> Name Type Description <code>statuses</code> <p>Status of ALL limits checked (both passed and failed)</p> <code>violations</code> <p>Only the limits that were exceeded</p> <code>passed</code> <p>Only the limits that passed</p> <code>retry_after_seconds</code> <p>Time until ALL requested capacity is available</p> <code>primary_violation</code> <p>The violation with longest retry time (bottleneck)</p>"},{"location":"api/exceptions/#zae_limiter.exceptions.RateLimitExceeded.retry_after_header","title":"retry_after_header  <code>property</code>","text":"<pre><code>retry_after_header\n</code></pre> <p>Value for HTTP Retry-After header (integer seconds).</p>"},{"location":"api/exceptions/#zae_limiter.exceptions.RateLimitExceeded.as_dict","title":"as_dict","text":"<pre><code>as_dict()\n</code></pre> <p>Serialize for JSON API responses.</p> <p>Returns a dictionary suitable for returning in a 429 response body.</p>"},{"location":"api/exceptions/#zae_limiter.exceptions.RateLimiterUnavailable","title":"RateLimiterUnavailable","text":"<pre><code>RateLimiterUnavailable(message, cause=None, *, stack_name=None, entity_id=None, resource=None)\n</code></pre> <p>               Bases: <code>InfrastructureError</code></p> <p>Raised when DynamoDB is unavailable and on_unavailable=OnUnavailable.BLOCK.</p> <p>This indicates a transient infrastructure issue, not a rate limit. When using OnUnavailable.BLOCK (the default), your application should be prepared to catch this exception and handle degraded mode gracefully.</p> <p>Attributes:</p> Name Type Description <code>cause</code> <p>The underlying exception that caused the unavailability</p> <code>stack_name</code> <p>The stack/table that was being accessed</p> <code>entity_id</code> <p>The entity being rate limited (if applicable)</p> <code>resource</code> <p>The resource being rate limited (if applicable)</p>"},{"location":"api/exceptions/#entity-exceptions","title":"Entity Exceptions","text":""},{"location":"api/exceptions/#zae_limiter.exceptions.EntityNotFoundError","title":"EntityNotFoundError","text":"<pre><code>EntityNotFoundError(entity_id)\n</code></pre> <p>               Bases: <code>EntityError</code></p> <p>Raised when an entity is not found.</p>"},{"location":"api/exceptions/#zae_limiter.exceptions.EntityExistsError","title":"EntityExistsError","text":"<pre><code>EntityExistsError(entity_id)\n</code></pre> <p>               Bases: <code>EntityError</code></p> <p>Raised when trying to create an entity that already exists.</p>"},{"location":"api/exceptions/#infrastructure-exceptions","title":"Infrastructure Exceptions","text":""},{"location":"api/exceptions/#zae_limiter.exceptions.StackCreationError","title":"StackCreationError","text":"<pre><code>StackCreationError(stack_name, reason, events=None)\n</code></pre> <p>               Bases: <code>InfrastructureError</code></p> <p>Raised when CloudFormation stack creation fails.</p>"},{"location":"api/exceptions/#zae_limiter.exceptions.StackAlreadyExistsError","title":"StackAlreadyExistsError","text":"<pre><code>StackAlreadyExistsError(stack_name, reason, events=None)\n</code></pre> <p>               Bases: <code>StackCreationError</code></p> <p>Raised when stack already exists (informational).</p>"},{"location":"api/exceptions/#zae_limiter.exceptions.InfrastructureNotFoundError","title":"InfrastructureNotFoundError","text":"<pre><code>InfrastructureNotFoundError(stack_name)\n</code></pre> <p>               Bases: <code>InfrastructureError</code></p> <p>Raised when expected infrastructure doesn't exist.</p> <p>This typically means the CloudFormation stack or DynamoDB table hasn't been deployed yet.</p>"},{"location":"api/exceptions/#version-exceptions","title":"Version Exceptions","text":""},{"location":"api/exceptions/#zae_limiter.exceptions.VersionMismatchError","title":"VersionMismatchError","text":"<pre><code>VersionMismatchError(client_version, schema_version, lambda_version, message, can_auto_update=False)\n</code></pre> <p>               Bases: <code>VersionError</code></p> <p>Raised when client and infrastructure versions are incompatible.</p> <p>This error indicates that the client library version doesn't match the deployed infrastructure and auto-update is disabled or failed.</p>"},{"location":"api/exceptions/#zae_limiter.exceptions.IncompatibleSchemaError","title":"IncompatibleSchemaError","text":"<pre><code>IncompatibleSchemaError(client_version, schema_version, message, migration_guide_url=None)\n</code></pre> <p>               Bases: <code>VersionError</code></p> <p>Raised when schema version requires manual migration.</p> <p>This indicates a major version difference that cannot be automatically reconciled.</p>"},{"location":"api/exceptions/#exception-handling-examples","title":"Exception Handling Examples","text":""},{"location":"api/exceptions/#basic-handling","title":"Basic Handling","text":"<pre><code>from zae_limiter import RateLimitExceeded, RateLimiterUnavailable\n\ntry:\n    async with limiter.acquire(\n        entity_id=\"user-123\",\n        resource=\"api\",\n        limits=[Limit.per_minute(\"rpm\", 100)],\n        consume={\"rpm\": 1},\n    ):\n        await do_work()\nexcept RateLimitExceeded as e:\n    # Handle rate limit exceeded\n    print(f\"Rate limited. Retry after {e.retry_after_seconds}s\")\nexcept RateLimiterUnavailable as e:\n    # Handle service unavailable\n    print(f\"Service unavailable: {e}\")\n</code></pre>"},{"location":"api/exceptions/#http-api-response","title":"HTTP API Response","text":"<pre><code>from fastapi import HTTPException\nfrom fastapi.responses import JSONResponse\n\n@app.post(\"/api/endpoint\")\nasync def endpoint():\n    try:\n        async with limiter.acquire(...):\n            return await process_request()\n    except RateLimitExceeded as e:\n        return JSONResponse(\n            status_code=429,\n            content=e.as_dict(),\n            headers={\"Retry-After\": e.retry_after_header},\n        )\n    except RateLimiterUnavailable:\n        raise HTTPException(status_code=503, detail=\"Service temporarily unavailable\")\n</code></pre>"},{"location":"api/exceptions/#detailed-error-information","title":"Detailed Error Information","text":"<pre><code>try:\n    async with limiter.acquire(...):\n        pass\nexcept RateLimitExceeded as e:\n    # All limit statuses (both passed and failed)\n    for status in e.statuses:\n        print(f\"Limit: {status.limit_name}\")\n        print(f\"  Entity: {status.entity_id}\")\n        print(f\"  Available: {status.available}\")\n        print(f\"  Requested: {status.requested}\")\n        print(f\"  Exceeded: {status.exceeded}\")\n        print(f\"  Retry after: {status.retry_after_seconds}s\")\n\n    # Only the violations\n    print(f\"Violations: {len(e.violations)}\")\n    for v in e.violations:\n        print(f\"  - {v.limit_name}: {v.available} available\")\n\n    # Only the passed limits\n    print(f\"Passed: {len(e.passed)}\")\n\n    # Primary bottleneck (longest wait time)\n    print(f\"Bottleneck: {e.primary_violation.limit_name}\")\n    print(f\"Total retry after: {e.retry_after_seconds}s\")\n</code></pre>"},{"location":"api/exceptions/#as_dict-output","title":"as_dict() Output","text":"<p>The <code>as_dict()</code> method returns a dictionary suitable for API responses:</p> <pre><code>{\n    \"error\": \"rate_limit_exceeded\",\n    \"message\": \"Rate limit exceeded for user-123/api: [rpm]. Retry after 45.2s\",\n    \"retry_after_seconds\": 45.2,\n    \"retry_after_ms\": 45200,\n    \"limits\": [\n        {\n            \"entity_id\": \"user-123\",\n            \"resource\": \"api\",\n            \"limit_name\": \"rpm\",\n            \"capacity\": 100,\n            \"burst\": 100,\n            \"available\": -5,\n            \"requested\": 10,\n            \"exceeded\": True,\n            \"retry_after_seconds\": 45.2,\n        },\n        {\n            \"entity_id\": \"user-123\",\n            \"resource\": \"api\",\n            \"limit_name\": \"tpm\",\n            \"capacity\": 10000,\n            \"burst\": 10000,\n            \"available\": 8500,\n            \"requested\": 500,\n            \"exceeded\": False,\n            \"retry_after_seconds\": 0.0,\n        },\n    ],\n}\n</code></pre> <p>Single <code>limits</code> array</p> <p>All limits (both exceeded and passed) are returned in a single <code>limits</code> array. Use the <code>exceeded</code> field to distinguish between violations and passed limits.</p>"},{"location":"api/limiter/","title":"RateLimiter","text":"<p>The main rate limiter classes for async and sync usage.</p>"},{"location":"api/limiter/#ratelimiter-async","title":"RateLimiter (Async)","text":""},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter","title":"RateLimiter","text":"<pre><code>RateLimiter(repository=None, name=None, region=None, endpoint_url=None, stack_options=None, on_unavailable=BLOCK, auto_update=True, strict_version=True, skip_version_check=False, config_cache_ttl=60)\n</code></pre> <p>Async rate limiter backed by DynamoDB.</p> <p>Implements token bucket algorithm with support for: - Multiple limits per entity/resource - Two-level hierarchy (parent/child entities) - Cascade mode (consume from entity + parent) - Stored limit configs - Usage analytics</p> <p>Example (new API - preferred):     from zae_limiter import RateLimiter, Repository, StackOptions</p> <pre><code>repo = Repository(\n    name=\"my-app\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(),\n)\nlimiter = RateLimiter(repository=repo)\n</code></pre> <p>Example (old API - deprecated):     limiter = RateLimiter(         name=\"my-app\",         region=\"us-east-1\",         stack_options=StackOptions(),     )</p> <p>Parameters:</p> Name Type Description Default <code>repository</code> <code>RepositoryProtocol | None</code> <p>Repository instance (new API, preferred). Pass a Repository or any RepositoryProtocol implementation.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>DEPRECATED. Resource identifier (e.g., 'my-app'). Use Repository(name=...) instead.</p> <code>None</code> <code>region</code> <code>str | None</code> <p>DEPRECATED. AWS region. Use Repository(region=...) instead.</p> <code>None</code> <code>endpoint_url</code> <code>str | None</code> <p>DEPRECATED. DynamoDB endpoint URL. Use Repository(endpoint_url=...) instead.</p> <code>None</code> <code>stack_options</code> <code>StackOptions | None</code> <p>DEPRECATED. Infrastructure state. Use Repository(stack_options=...) instead.</p> <code>None</code> <code>on_unavailable</code> <code>OnUnavailable</code> <p>Behavior when DynamoDB is unavailable</p> <code>BLOCK</code> <code>auto_update</code> <code>bool</code> <p>Auto-update Lambda when version mismatch detected</p> <code>True</code> <code>strict_version</code> <code>bool</code> <p>Fail if version mismatch (when auto_update is False)</p> <code>True</code> <code>skip_version_check</code> <code>bool</code> <p>Skip all version checks (dangerous)</p> <code>False</code> <code>config_cache_ttl</code> <code>int</code> <p>TTL in seconds for config cache (default: 60, 0 to disable)</p> <code>60</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both repository and name/region/endpoint_url/stack_options are provided.</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>The resource identifier.</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.list_deployed","title":"list_deployed  <code>async</code> <code>classmethod</code>","text":"<pre><code>list_deployed(region=None, endpoint_url=None)\n</code></pre> <p>List all deployed rate limiter instances in a region.</p> <p>This is a class method that discovers existing deployments without requiring an initialized RateLimiter instance. It queries CloudFormation for stacks tagged with <code>ManagedBy=zae-limiter</code>.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>str | None</code> <p>AWS region (default: use boto3 defaults)</p> <code>None</code> <code>endpoint_url</code> <code>str | None</code> <p>CloudFormation endpoint (for LocalStack)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[LimiterInfo]</code> <p>List of LimiterInfo objects describing deployed instances.</p> <code>list[LimiterInfo]</code> <p>Sorted by user-friendly name. Excludes deleted stacks.</p> Example <p>Raises:</p> Type Description <code>ClientError</code> <p>If CloudFormation API call fails</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.list_deployed--discover-all-limiters-in-us-east-1","title":"Discover all limiters in us-east-1","text":"<p>limiters = await RateLimiter.list_deployed(region=\"us-east-1\") for limiter in limiters:     if limiter.is_healthy:         print(f\"\u2713 {limiter.user_name}: {limiter.version}\")     elif limiter.is_failed:         print(f\"\u2717 {limiter.user_name}: {limiter.stack_status}\")</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.close","title":"close  <code>async</code>","text":"<pre><code>close()\n</code></pre> <p>Close the underlying connections.</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.invalidate_config_cache","title":"invalidate_config_cache  <code>async</code>","text":"<pre><code>invalidate_config_cache()\n</code></pre> <p>Invalidate all cached config entries.</p> <p>Call this after modifying config (system defaults, resource defaults, or entity limits) to force an immediate refresh. Without manual invalidation, changes propagate within the TTL period (default: 60s).</p> Example"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.invalidate_config_cache--update-config-and-force-refresh","title":"Update config and force refresh","text":"<p>await limiter.set_system_defaults([Limit.tpm(100000)]) await limiter.invalidate_config_cache()</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.get_cache_stats","title":"get_cache_stats","text":"<pre><code>get_cache_stats()\n</code></pre> <p>Get config cache performance statistics.</p> <p>Returns statistics useful for monitoring and debugging cache behavior: - hits: Number of cache hits (avoided DynamoDB reads) - misses: Number of cache misses (DynamoDB reads performed) - size: Number of entries currently in cache - ttl: Cache TTL in seconds</p> <p>Returns:</p> Type Description <code>CacheStats</code> <p>CacheStats object with cache metrics</p> Example <p>stats = limiter.get_cache_stats() total = stats.hits + stats.misses hit_rate = stats.hits / total if total &gt; 0 else 0 print(f\"Cache hit rate: {hit_rate:.1%}\") print(f\"Cache entries: {stats.size}\")</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.is_available","title":"is_available  <code>async</code>","text":"<pre><code>is_available(timeout=1.0)\n</code></pre> <p>Check if the rate limiter backend (DynamoDB) is reachable.</p> <p>Performs a lightweight health check without requiring initialization. This method never raises exceptions - it returns False on any error.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum time in seconds to wait for response (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if DynamoDB table is reachable, False otherwise.</p> Example <p>limiter = RateLimiter(name=\"my-app\", region=\"us-east-1\") if await limiter.is_available():     async with limiter.acquire(...) as lease:         ... else:     # Handle degraded mode     pass</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.create_entity","title":"create_entity  <code>async</code>","text":"<pre><code>create_entity(entity_id, name=None, parent_id=None, cascade=False, metadata=None, principal=None)\n</code></pre> <p>Create a new entity.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Unique identifier for the entity</p> required <code>name</code> <code>str | None</code> <p>Human-readable name (defaults to entity_id)</p> <code>None</code> <code>parent_id</code> <code>str | None</code> <p>Parent entity ID (None for root/project entities)</p> <code>None</code> <code>cascade</code> <code>bool</code> <p>If True, acquire() will also consume from parent entity</p> <code>False</code> <code>metadata</code> <code>dict[str, str] | None</code> <p>Additional metadata to store</p> <code>None</code> <code>principal</code> <code>str | None</code> <p>Caller identity for audit logging (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>Entity</code> <p>The created Entity</p> <p>Raises:</p> Type Description <code>EntityExistsError</code> <p>If entity already exists</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.get_entity","title":"get_entity  <code>async</code>","text":"<pre><code>get_entity(entity_id)\n</code></pre> <p>Get an entity by ID.</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.delete_entity","title":"delete_entity  <code>async</code>","text":"<pre><code>delete_entity(entity_id, principal=None)\n</code></pre> <p>Delete an entity and all its related data.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>ID of the entity to delete</p> required <code>principal</code> <code>str | None</code> <p>Caller identity for audit logging (optional)</p> <code>None</code>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.get_children","title":"get_children  <code>async</code>","text":"<pre><code>get_children(parent_id)\n</code></pre> <p>Get all children of a parent entity.</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.get_audit_events","title":"get_audit_events  <code>async</code>","text":"<pre><code>get_audit_events(entity_id, limit=100, start_event_id=None)\n</code></pre> <p>Get audit events for an entity.</p> <p>Retrieves security audit events logged for administrative operations on the specified entity, ordered by most recent first.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>ID of the entity to query</p> required <code>limit</code> <code>int</code> <p>Maximum number of events to return (default: 100)</p> <code>100</code> <code>start_event_id</code> <code>str | None</code> <p>Event ID to start after (for pagination)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[AuditEvent]</code> <p>List of AuditEvent objects, ordered by most recent first</p> Example <p>events = await limiter.get_audit_events(\"proj-1\") for event in events:     print(f\"{event.timestamp}: {event.action} by {event.principal}\")</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.get_usage_snapshots","title":"get_usage_snapshots  <code>async</code>","text":"<pre><code>get_usage_snapshots(entity_id=None, resource=None, window_type=None, start_time=None, end_time=None, limit=100, next_key=None)\n</code></pre> <p>Query usage snapshots for historical consumption data.</p> <p>Usage snapshots are created by the aggregator Lambda from DynamoDB stream events. They track token consumption per entity/resource within time windows (hourly, daily).</p> <p>Supports two query modes: 1. Entity-scoped: Provide entity_id (optionally with resource filter) 2. Resource-scoped: Provide resource to query across all entities</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str | None</code> <p>Entity to query (uses primary key)</p> <code>None</code> <code>resource</code> <code>str | None</code> <p>Resource name filter (required if entity_id is None)</p> <code>None</code> <code>window_type</code> <code>str | None</code> <p>Filter by window type (\"hourly\", \"daily\")</p> <code>None</code> <code>start_time</code> <code>datetime | None</code> <p>Filter snapshots &gt;= this timestamp</p> <code>None</code> <code>end_time</code> <code>datetime | None</code> <p>Filter snapshots &lt;= this timestamp</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum items to fetch from DynamoDB per page (default: 100)</p> <code>100</code> <code>next_key</code> <code>dict[str, Any] | None</code> <p>Pagination cursor from previous call</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[list[UsageSnapshot], dict[str, Any] | None]</code> <p>Tuple of (snapshots, next_key). next_key is None if no more results.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither entity_id nor resource is provided</p> Note <p>The <code>limit</code> parameter controls the DynamoDB query batch size. Client-side filters (window_type, start_time, end_time) are applied after fetching, so the returned count may be less than <code>limit</code>. Use <code>next_key</code> to paginate through all matching results.</p> Example"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.get_usage_snapshots--get-hourly-snapshots-for-an-entity","title":"Get hourly snapshots for an entity","text":"<p>snapshots, cursor = await limiter.get_usage_snapshots(     entity_id=\"user-123\",     resource=\"gpt-4\",     window_type=\"hourly\",     start_time=datetime(2024, 1, 1),     end_time=datetime(2024, 1, 31), ) for snap in snapshots:     print(f\"{snap.window_start}: {snap.counters}\")</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.get_usage_snapshots--paginate-through-results","title":"Paginate through results","text":"<p>while cursor:     more, cursor = await limiter.get_usage_snapshots(         entity_id=\"user-123\",         next_key=cursor,     )</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.get_usage_summary","title":"get_usage_summary  <code>async</code>","text":"<pre><code>get_usage_summary(entity_id=None, resource=None, window_type=None, start_time=None, end_time=None)\n</code></pre> <p>Get aggregated usage summary across multiple snapshots.</p> <p>Fetches all matching snapshots and computes total and average consumption statistics. Useful for billing, reporting, and capacity planning.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str | None</code> <p>Entity to query</p> <code>None</code> <code>resource</code> <code>str | None</code> <p>Resource name filter (required if entity_id is None)</p> <code>None</code> <code>window_type</code> <code>str | None</code> <p>Filter by window type (\"hourly\", \"daily\")</p> <code>None</code> <code>start_time</code> <code>datetime | None</code> <p>Filter snapshots &gt;= this timestamp</p> <code>None</code> <code>end_time</code> <code>datetime | None</code> <p>Filter snapshots &lt;= this timestamp</p> <code>None</code> <p>Returns:</p> Type Description <code>UsageSummary</code> <p>UsageSummary with total and average consumption per limit type</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither entity_id nor resource is provided</p> Example <p>summary = await limiter.get_usage_summary(     entity_id=\"user-123\",     resource=\"gpt-4\",     window_type=\"hourly\",     start_time=datetime(2024, 1, 1),     end_time=datetime(2024, 1, 31), ) print(f\"Total tokens: {summary.total.get('tpm', 0)}\") print(f\"Average per hour: {summary.average.get('tpm', 0.0):.1f}\") print(f\"Snapshots: {summary.snapshot_count}\")</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.acquire","title":"acquire  <code>async</code>","text":"<pre><code>acquire(entity_id, resource, limits, consume, use_stored_limits=False, on_unavailable=None)\n</code></pre> <p>Acquire rate limit capacity.</p> <p>Limits are resolved using three-tier hierarchy: Entity &gt; Resource &gt; System. If no stored limits found, falls back to the <code>limits</code> parameter.</p> <p>Cascade behavior is controlled by the entity's <code>cascade</code> flag, set at entity creation time via <code>create_entity(cascade=True)</code>. When enabled, acquire() automatically consumes from both the entity and its parent.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity to acquire capacity for</p> required <code>resource</code> <code>str</code> <p>Resource being accessed (e.g., \"gpt-4\")</p> required <code>consume</code> <code>dict[str, int]</code> <p>Amounts to consume by limit name</p> required <code>limits</code> <code>list[Limit] | None</code> <p>Override limits (optional, falls back to stored config)</p> required <code>use_stored_limits</code> <code>bool</code> <p>DEPRECATED - limits are now always resolved from stored config. This parameter will be removed in v1.0.</p> <code>False</code> <code>on_unavailable</code> <code>OnUnavailable | None</code> <p>Override default on_unavailable behavior</p> <code>None</code> <p>Yields:</p> Type Description <code>AsyncIterator[Lease]</code> <p>Lease for managing additional consumption</p> <p>Raises:</p> Type Description <code>RateLimitExceeded</code> <p>If any limit would be exceeded</p> <code>RateLimiterUnavailable</code> <p>If DynamoDB unavailable and BLOCK</p> <code>ValidationError</code> <p>If no limits found at any level and no override provided</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.available","title":"available  <code>async</code>","text":"<pre><code>available(entity_id, resource, limits=None, use_stored_limits=False)\n</code></pre> <p>Check available capacity without consuming.</p> <p>Limits are resolved using three-tier hierarchy: Entity &gt; Resource &gt; System. If no stored limits found, falls back to the <code>limits</code> parameter.</p> <p>Returns minimum available across entity (and parent if cascade). Can return negative values if bucket is in debt.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity to check</p> required <code>resource</code> <code>str</code> <p>Resource to check</p> required <code>limits</code> <code>list[Limit] | None</code> <p>Override limits (optional, falls back to stored config)</p> <code>None</code> <code>use_stored_limits</code> <code>bool</code> <p>DEPRECATED - limits are now always resolved from stored config. This parameter will be removed in v1.0.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>Dict mapping limit_name -&gt; available tokens</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If no limits found at any level and no override provided</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.time_until_available","title":"time_until_available  <code>async</code>","text":"<pre><code>time_until_available(entity_id, resource, needed, limits=None, use_stored_limits=False)\n</code></pre> <p>Calculate seconds until requested capacity is available.</p> <p>Limits are resolved using three-tier hierarchy: Entity &gt; Resource &gt; System. If no stored limits found, falls back to the <code>limits</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity to check</p> required <code>resource</code> <code>str</code> <p>Resource to check</p> required <code>needed</code> <code>dict[str, int]</code> <p>Required amounts by limit name</p> required <code>limits</code> <code>list[Limit] | None</code> <p>Override limits (optional, falls back to stored config)</p> <code>None</code> <code>use_stored_limits</code> <code>bool</code> <p>DEPRECATED - limits are now always resolved from stored config. This parameter will be removed in v1.0.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>Seconds until available (0.0 if already available)</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If no limits found at any level and no override provided</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.set_limits","title":"set_limits  <code>async</code>","text":"<pre><code>set_limits(entity_id, limits, resource=DEFAULT_RESOURCE, principal=None)\n</code></pre> <p>Store limit configs for an entity.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity to set limits for</p> required <code>limits</code> <code>list[Limit]</code> <p>Limits to store</p> required <code>resource</code> <code>str</code> <p>Resource these limits apply to (or default)</p> <code>DEFAULT_RESOURCE</code> <code>principal</code> <code>str | None</code> <p>Caller identity for audit logging (optional)</p> <code>None</code>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.get_limits","title":"get_limits  <code>async</code>","text":"<pre><code>get_limits(entity_id, resource=DEFAULT_RESOURCE)\n</code></pre> <p>Get stored limit configs for an entity.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity to get limits for</p> required <code>resource</code> <code>str</code> <p>Resource to get limits for</p> <code>DEFAULT_RESOURCE</code> <p>Returns:</p> Type Description <code>list[Limit]</code> <p>List of stored limits (empty if none)</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.delete_limits","title":"delete_limits  <code>async</code>","text":"<pre><code>delete_limits(entity_id, resource=DEFAULT_RESOURCE, principal=None)\n</code></pre> <p>Delete stored limit configs for an entity.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity to delete limits for</p> required <code>resource</code> <code>str</code> <p>Resource to delete limits for</p> <code>DEFAULT_RESOURCE</code> <code>principal</code> <code>str | None</code> <p>Caller identity for audit logging (optional)</p> <code>None</code>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.set_resource_defaults","title":"set_resource_defaults  <code>async</code>","text":"<pre><code>set_resource_defaults(resource, limits, principal=None)\n</code></pre> <p>Store default limit configs for a resource.</p> <p>Resource defaults override system defaults for the specified resource.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>Resource name</p> required <code>limits</code> <code>list[Limit]</code> <p>Limits to store</p> required <code>principal</code> <code>str | None</code> <p>Caller identity for audit logging (optional)</p> <code>None</code>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.get_resource_defaults","title":"get_resource_defaults  <code>async</code>","text":"<pre><code>get_resource_defaults(resource)\n</code></pre> <p>Get stored default limit configs for a resource.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>Resource name</p> required <p>Returns:</p> Type Description <code>list[Limit]</code> <p>List of stored limits (empty if none)</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.delete_resource_defaults","title":"delete_resource_defaults  <code>async</code>","text":"<pre><code>delete_resource_defaults(resource, principal=None)\n</code></pre> <p>Delete stored default limit configs for a resource.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>Resource name</p> required <code>principal</code> <code>str | None</code> <p>Caller identity for audit logging (optional)</p> <code>None</code>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.list_resources_with_defaults","title":"list_resources_with_defaults  <code>async</code>","text":"<pre><code>list_resources_with_defaults()\n</code></pre> <p>List all resources that have default limit configs.</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.set_system_defaults","title":"set_system_defaults  <code>async</code>","text":"<pre><code>set_system_defaults(limits, on_unavailable=None, principal=None)\n</code></pre> <p>Store system-wide default limits and config.</p> <p>System defaults apply to ALL resources unless overridden at resource or entity level.</p> <p>Parameters:</p> Name Type Description Default <code>limits</code> <code>list[Limit]</code> <p>Limits to store (apply globally to all resources)</p> required <code>on_unavailable</code> <code>OnUnavailable | None</code> <p>Behavior when DynamoDB unavailable (optional)</p> <code>None</code> <code>principal</code> <code>str | None</code> <p>Caller identity for audit logging (optional)</p> <code>None</code>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.get_system_defaults","title":"get_system_defaults  <code>async</code>","text":"<pre><code>get_system_defaults()\n</code></pre> <p>Get system-wide default limits and config.</p> <p>Returns:</p> Type Description <code>tuple[list[Limit], OnUnavailable | None]</code> <p>Tuple of (limits, on_unavailable). on_unavailable may be None if not set.</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.delete_system_defaults","title":"delete_system_defaults  <code>async</code>","text":"<pre><code>delete_system_defaults(principal=None)\n</code></pre> <p>Delete all system-wide default limits and config.</p> <p>Parameters:</p> Name Type Description Default <code>principal</code> <code>str | None</code> <p>Caller identity for audit logging (optional)</p> <code>None</code>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.get_resource_capacity","title":"get_resource_capacity  <code>async</code>","text":"<pre><code>get_resource_capacity(resource, limit_name, parents_only=False)\n</code></pre> <p>Get aggregated capacity for a resource across all entities.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>Resource to query</p> required <code>limit_name</code> <code>str</code> <p>Limit name to query</p> required <code>parents_only</code> <code>bool</code> <p>If True, only include parent entities</p> <code>False</code> <p>Returns:</p> Type Description <code>ResourceCapacity</code> <p>ResourceCapacity with aggregated data</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.create_stack","title":"create_stack  <code>async</code>","text":"<pre><code>create_stack(stack_options=None)\n</code></pre> <p>Create CloudFormation stack for infrastructure.</p> <p>Parameters:</p> Name Type Description Default <code>stack_options</code> <code>StackOptions | None</code> <p>Stack configuration</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with stack_id, stack_name, and status</p> <p>Raises:</p> Type Description <code>StackCreationError</code> <p>If stack creation fails</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.delete_stack","title":"delete_stack  <code>async</code>","text":"<pre><code>delete_stack()\n</code></pre> <p>Delete the CloudFormation stack and all associated resources.</p> <p>This method permanently removes the CloudFormation stack, including:</p> <ul> <li>DynamoDB table and all stored data</li> <li>Lambda aggregator function (if deployed)</li> <li>IAM roles and CloudWatch log groups</li> <li>All other stack resources</li> </ul> <p>The method waits for deletion to complete before returning. If the stack doesn't exist, no error is raised.</p> <p>Raises:</p> Type Description <code>StackCreationError</code> <p>If deletion fails (e.g., permission denied, resources in use, or CloudFormation service error)</p> Example <p>Cleanup after integration testing::</p> <pre><code>limiter = RateLimiter(\n    name=\"test-limits\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(),\n)\n\nasync with limiter:\n    # Run tests...\n    pass\n\n# Clean up infrastructure\nawait limiter.delete_stack()\n</code></pre> Warning <p>This operation is irreversible. All rate limit state, entity data, and usage history will be permanently deleted.</p>"},{"location":"api/limiter/#zae_limiter.limiter.RateLimiter.get_status","title":"get_status  <code>async</code>","text":"<pre><code>get_status()\n</code></pre> <p>Get comprehensive status of the RateLimiter infrastructure.</p> <p>Consolidates connectivity, infrastructure state, version information, and table metrics into a single status object. This method does not raise exceptions for missing infrastructure - it gracefully handles all error cases and returns status information accordingly.</p> <p>Returns:</p> Type Description <code>Status</code> <p>Status object containing:</p> <code>Status</code> <ul> <li>Connectivity: available, latency_ms</li> </ul> <code>Status</code> <ul> <li>Infrastructure: stack_status, table_status, aggregator_enabled</li> </ul> <code>Status</code> <ul> <li>Identity: name, region</li> </ul> <code>Status</code> <ul> <li>Versions: schema_version, lambda_version, client_version</li> </ul> <code>Status</code> <ul> <li>Table metrics: table_item_count, table_size_bytes</li> </ul> <code>Status</code> <ul> <li>IAM Roles: app_role_arn, admin_role_arn, readonly_role_arn</li> </ul> Example <p>Check infrastructure health::</p> <pre><code>status = await limiter.get_status()\nif status.available:\n    print(f\"Ready! Latency: {status.latency_ms}ms\")\n    print(f\"Stack: {status.stack_status}\")\n    print(f\"Schema: {status.schema_version}\")\nelse:\n    print(\"DynamoDB is not reachable\")\n</code></pre> Note <p>This method measures actual DynamoDB latency by performing a lightweight operation. The latency_ms value reflects real round-trip time to the DynamoDB endpoint.</p>"},{"location":"api/limiter/#syncratelimiter","title":"SyncRateLimiter","text":""},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter","title":"SyncRateLimiter","text":"<pre><code>SyncRateLimiter(repository=None, name=None, region=None, endpoint_url=None, stack_options=None, on_unavailable=BLOCK, auto_update=True, strict_version=True, skip_version_check=False, config_cache_ttl=60)\n</code></pre> <p>Synchronous rate limiter backed by DynamoDB.</p> <p>Wraps RateLimiter, running async operations in an event loop.</p> <p>Example (new API - preferred):     from zae_limiter import SyncRateLimiter, Repository, StackOptions</p> <pre><code>repo = Repository(\n    name=\"my-app\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(),\n)\nlimiter = SyncRateLimiter(repository=repo)\n</code></pre> <p>Example (old API - deprecated):     limiter = SyncRateLimiter(         name=\"my-app\",         region=\"us-east-1\",         stack_options=StackOptions(),     )</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>The resource identifier.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the underlying connections.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.invalidate_config_cache","title":"invalidate_config_cache","text":"<pre><code>invalidate_config_cache()\n</code></pre> <p>Invalidate all cached config entries.</p> <p>Synchronous wrapper for :meth:<code>RateLimiter.invalidate_config_cache</code>. See the async version for full documentation.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.get_cache_stats","title":"get_cache_stats","text":"<pre><code>get_cache_stats()\n</code></pre> <p>Get config cache performance statistics.</p> <p>Synchronous wrapper for :meth:<code>RateLimiter.get_cache_stats</code>. See the async version for full documentation.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.is_available","title":"is_available","text":"<pre><code>is_available(timeout=1.0)\n</code></pre> <p>Check if the rate limiter backend (DynamoDB) is reachable.</p> <p>Performs a lightweight health check without requiring initialization. This method never raises exceptions - it returns False on any error.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum time in seconds to wait for response (default: 1.0)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if DynamoDB table is reachable, False otherwise.</p> Example <p>limiter = SyncRateLimiter(name=\"my-app\", region=\"us-east-1\") if limiter.is_available():     with limiter.acquire(...) as lease:         ... else:     # Handle degraded mode     pass</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.create_entity","title":"create_entity","text":"<pre><code>create_entity(entity_id, name=None, parent_id=None, cascade=False, metadata=None, principal=None)\n</code></pre> <p>Create a new entity.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.get_entity","title":"get_entity","text":"<pre><code>get_entity(entity_id)\n</code></pre> <p>Get an entity by ID.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.delete_entity","title":"delete_entity","text":"<pre><code>delete_entity(entity_id, principal=None)\n</code></pre> <p>Delete an entity and all its related data.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.get_children","title":"get_children","text":"<pre><code>get_children(parent_id)\n</code></pre> <p>Get all children of a parent entity.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.get_audit_events","title":"get_audit_events","text":"<pre><code>get_audit_events(entity_id, limit=100, start_event_id=None)\n</code></pre> <p>Get audit events for an entity.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.get_usage_snapshots","title":"get_usage_snapshots","text":"<pre><code>get_usage_snapshots(entity_id=None, resource=None, window_type=None, start_time=None, end_time=None, limit=100, next_key=None)\n</code></pre> <p>Query usage snapshots for historical consumption data.</p> <p>Synchronous wrapper for :meth:<code>RateLimiter.get_usage_snapshots</code>. See the async version for full documentation.</p> <p>Returns:</p> Type Description <code>tuple[list[UsageSnapshot], dict[str, Any] | None]</code> <p>Tuple of (snapshots, next_key). next_key is None if no more results.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.get_usage_summary","title":"get_usage_summary","text":"<pre><code>get_usage_summary(entity_id=None, resource=None, window_type=None, start_time=None, end_time=None)\n</code></pre> <p>Get aggregated usage summary across multiple snapshots.</p> <p>Synchronous wrapper for :meth:<code>RateLimiter.get_usage_summary</code>. See the async version for full documentation.</p> <p>Returns:</p> Type Description <code>UsageSummary</code> <p>UsageSummary with total and average consumption per limit type.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.acquire","title":"acquire","text":"<pre><code>acquire(entity_id, resource, limits, consume, use_stored_limits=False, on_unavailable=None)\n</code></pre> <p>Acquire rate limit capacity (synchronous).</p> <p>Limits are resolved using three-tier hierarchy: Entity &gt; Resource &gt; System. If no stored limits found, falls back to the <code>limits</code> parameter.</p> <p>Cascade behavior is controlled by the entity's <code>cascade</code> flag, set at entity creation time via <code>create_entity(cascade=True)</code>.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.available","title":"available","text":"<pre><code>available(entity_id, resource, limits=None, use_stored_limits=False)\n</code></pre> <p>Check available capacity without consuming.</p> <p>Limits are resolved using three-tier hierarchy: Entity &gt; Resource &gt; System. If no stored limits found, falls back to the <code>limits</code> parameter.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.time_until_available","title":"time_until_available","text":"<pre><code>time_until_available(entity_id, resource, needed, limits=None, use_stored_limits=False)\n</code></pre> <p>Calculate seconds until requested capacity is available.</p> <p>Limits are resolved using three-tier hierarchy: Entity &gt; Resource &gt; System. If no stored limits found, falls back to the <code>limits</code> parameter.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.set_limits","title":"set_limits","text":"<pre><code>set_limits(entity_id, limits, resource=DEFAULT_RESOURCE, principal=None)\n</code></pre> <p>Store limit configs for an entity.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.get_limits","title":"get_limits","text":"<pre><code>get_limits(entity_id, resource=DEFAULT_RESOURCE)\n</code></pre> <p>Get stored limit configs for an entity.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.delete_limits","title":"delete_limits","text":"<pre><code>delete_limits(entity_id, resource=DEFAULT_RESOURCE, principal=None)\n</code></pre> <p>Delete stored limit configs for an entity.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.set_resource_defaults","title":"set_resource_defaults","text":"<pre><code>set_resource_defaults(resource, limits, principal=None)\n</code></pre> <p>Store default limit configs for a resource.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.get_resource_defaults","title":"get_resource_defaults","text":"<pre><code>get_resource_defaults(resource)\n</code></pre> <p>Get stored default limit configs for a resource.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.delete_resource_defaults","title":"delete_resource_defaults","text":"<pre><code>delete_resource_defaults(resource, principal=None)\n</code></pre> <p>Delete stored default limit configs for a resource.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.list_resources_with_defaults","title":"list_resources_with_defaults","text":"<pre><code>list_resources_with_defaults()\n</code></pre> <p>List all resources that have default limit configs.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.set_system_defaults","title":"set_system_defaults","text":"<pre><code>set_system_defaults(limits, on_unavailable=None, principal=None)\n</code></pre> <p>Store system-wide default limits and config.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.get_system_defaults","title":"get_system_defaults","text":"<pre><code>get_system_defaults()\n</code></pre> <p>Get system-wide default limits and config.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.delete_system_defaults","title":"delete_system_defaults","text":"<pre><code>delete_system_defaults(principal=None)\n</code></pre> <p>Delete all system-wide default limits and config.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.get_resource_capacity","title":"get_resource_capacity","text":"<pre><code>get_resource_capacity(resource, limit_name, parents_only=False)\n</code></pre> <p>Get aggregated capacity for a resource across all entities.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.create_stack","title":"create_stack","text":"<pre><code>create_stack(stack_options=None)\n</code></pre> <p>Create CloudFormation stack for infrastructure.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.delete_stack","title":"delete_stack","text":"<pre><code>delete_stack()\n</code></pre> <p>Delete the CloudFormation stack and all associated resources.</p> <p>Synchronous wrapper for :meth:<code>RateLimiter.delete_stack</code>. See the async version for full documentation.</p> <p>Raises:</p> Type Description <code>StackCreationError</code> <p>If deletion fails</p> Example <p>Cleanup after testing::</p> <pre><code>limiter = SyncRateLimiter(\n    name=\"test-limits\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(),\n)\n\nwith limiter:\n    # Run tests...\n    pass\n\n# Clean up infrastructure\nlimiter.delete_stack()\n</code></pre> Warning <p>This operation is irreversible. All data will be permanently deleted.</p>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.get_status","title":"get_status","text":"<pre><code>get_status()\n</code></pre> <p>Get comprehensive status of the RateLimiter infrastructure.</p> <p>Synchronous wrapper for :meth:<code>RateLimiter.get_status</code>. See the async version for full documentation.</p> <p>Returns:</p> Type Description <code>Status</code> <p>Status object with connectivity, infrastructure, versions, and metrics.</p> Example <p>Check infrastructure health::</p> <pre><code>limiter = SyncRateLimiter(name=\"my-app\", region=\"us-east-1\")\n\nstatus = limiter.get_status()\nif status.available:\n    print(f\"Ready! Latency: {status.latency_ms}ms\")\n    print(f\"Schema: {status.schema_version}\")\nelse:\n    print(\"DynamoDB is not reachable\")\n</code></pre>"},{"location":"api/limiter/#zae_limiter.limiter.SyncRateLimiter.list_deployed","title":"list_deployed  <code>staticmethod</code>","text":"<pre><code>list_deployed(region=None, endpoint_url=None)\n</code></pre> <p>List all deployed rate limiter instances in a region.</p> <p>Synchronous wrapper for :meth:<code>RateLimiter.list_deployed</code>. See the async version for full documentation.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>str | None</code> <p>AWS region to search. Defaults to boto3 session default.</p> <code>None</code> <code>endpoint_url</code> <code>str | None</code> <p>AWS endpoint URL (e.g., LocalStack).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[LimiterInfo]</code> <p>List of LimiterInfo objects describing discovered stacks.</p> Example <p>Discover all limiters in a region::</p> <pre><code>limiters = SyncRateLimiter.list_deployed(region=\"us-east-1\")\nfor limiter in limiters:\n    print(f\"{limiter.user_name}: {limiter.stack_status}\")\n</code></pre>"},{"location":"api/limiter/#onunavailable","title":"OnUnavailable","text":""},{"location":"api/limiter/#zae_limiter.limiter.OnUnavailable","title":"OnUnavailable","text":"<p>               Bases: <code>Enum</code></p> <p>Behavior when DynamoDB is unavailable.</p>"},{"location":"api/models/","title":"Models","text":"<p>Data models for rate limit configuration and status.</p>"},{"location":"api/models/#limit","title":"Limit","text":""},{"location":"api/models/#zae_limiter.models.Limit","title":"Limit  <code>dataclass</code>","text":"<pre><code>Limit(name, capacity, burst, refill_amount, refill_period_seconds)\n</code></pre> <p>Token bucket rate limit configuration.</p> <p>Refill rate is stored as a fraction (refill_amount / refill_period_seconds) to avoid floating point precision issues.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique identifier for this limit type (e.g., \"rpm\", \"tpm\")</p> <code>capacity</code> <code>int</code> <p>Max tokens that refill over the period (sustained rate)</p> <code>burst</code> <code>int</code> <p>Max tokens in bucket (&gt;= capacity, allows bursting)</p> <code>refill_amount</code> <code>int</code> <p>Numerator of refill rate</p> <code>refill_period_seconds</code> <code>int</code> <p>Denominator of refill rate</p>"},{"location":"api/models/#zae_limiter.models.Limit.refill_rate","title":"refill_rate  <code>property</code>","text":"<pre><code>refill_rate\n</code></pre> <p>Tokens per second (for display/debugging).</p>"},{"location":"api/models/#zae_limiter.models.Limit.per_second","title":"per_second  <code>classmethod</code>","text":"<pre><code>per_second(name, capacity, burst=None)\n</code></pre> <p>Create a limit that refills <code>capacity</code> tokens per second.</p>"},{"location":"api/models/#zae_limiter.models.Limit.per_minute","title":"per_minute  <code>classmethod</code>","text":"<pre><code>per_minute(name, capacity, burst=None)\n</code></pre> <p>Create a limit that refills <code>capacity</code> tokens per minute.</p>"},{"location":"api/models/#zae_limiter.models.Limit.per_hour","title":"per_hour  <code>classmethod</code>","text":"<pre><code>per_hour(name, capacity, burst=None)\n</code></pre> <p>Create a limit that refills <code>capacity</code> tokens per hour.</p>"},{"location":"api/models/#zae_limiter.models.Limit.per_day","title":"per_day  <code>classmethod</code>","text":"<pre><code>per_day(name, capacity, burst=None)\n</code></pre> <p>Create a limit that refills <code>capacity</code> tokens per day.</p>"},{"location":"api/models/#zae_limiter.models.Limit.custom","title":"custom  <code>classmethod</code>","text":"<pre><code>custom(name, capacity, refill_amount, refill_period_seconds, burst=None)\n</code></pre> <p>Create a custom limit with explicit refill rate.</p> Sustain 100/sec with burst of 1000 <p>Limit.custom(\"requests\", capacity=100, refill_amount=100,             refill_period_seconds=1, burst=1000)</p>"},{"location":"api/models/#zae_limiter.models.Limit.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Serialize to dictionary for storage.</p>"},{"location":"api/models/#zae_limiter.models.Limit.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data)\n</code></pre> <p>Deserialize from dictionary.</p>"},{"location":"api/models/#entity","title":"Entity","text":""},{"location":"api/models/#zae_limiter.models.Entity","title":"Entity  <code>dataclass</code>","text":"<pre><code>Entity(id, name=None, parent_id=None, cascade=False, metadata=dict(), created_at=None)\n</code></pre> <p>An entity that can have rate limits applied.</p> <p>Entities can be parents (projects) or children (API keys). Children have a parent_id reference.</p> <p>Note: This model does not validate in post_init to support DynamoDB deserialization and avoid performance overhead. Validation is performed in Repository.create_entity() at the API boundary.</p>"},{"location":"api/models/#zae_limiter.models.Entity.is_parent","title":"is_parent  <code>property</code>","text":"<pre><code>is_parent\n</code></pre> <p>True if this entity has no parent (is a root/project).</p>"},{"location":"api/models/#zae_limiter.models.Entity.is_child","title":"is_child  <code>property</code>","text":"<pre><code>is_child\n</code></pre> <p>True if this entity has a parent.</p>"},{"location":"api/models/#limitstatus","title":"LimitStatus","text":""},{"location":"api/models/#zae_limiter.models.LimitStatus","title":"LimitStatus  <code>dataclass</code>","text":"<pre><code>LimitStatus(entity_id, resource, limit_name, limit, available, requested, exceeded, retry_after_seconds)\n</code></pre> <p>Status of a specific limit check.</p> <p>Returned in RateLimitExceeded to provide full visibility into all limits that were checked.</p> <p>Note: This is an internal model created by the limiter from validated inputs. No validation is performed here to avoid performance overhead.</p>"},{"location":"api/models/#zae_limiter.models.LimitStatus.deficit","title":"deficit  <code>property</code>","text":"<pre><code>deficit\n</code></pre> <p>How many tokens short we are (0 if not exceeded).</p>"},{"location":"api/models/#bucketstate","title":"BucketState","text":""},{"location":"api/models/#zae_limiter.models.BucketState","title":"BucketState  <code>dataclass</code>","text":"<pre><code>BucketState(entity_id, resource, limit_name, tokens_milli, last_refill_ms, capacity_milli, burst_milli, refill_amount_milli, refill_period_ms, total_consumed_milli=None)\n</code></pre> <p>Internal state of a token bucket.</p> <p>All token values are stored in millitokens (x1000) for precision.</p> <p>Note: This is an internal model. Validation is performed in from_limit() for user-provided inputs, not in post_init to support DynamoDB deserialization and avoid performance overhead on frequent operations.</p>"},{"location":"api/models/#zae_limiter.models.BucketState.tokens","title":"tokens  <code>property</code>","text":"<pre><code>tokens\n</code></pre> <p>Current tokens (not millitokens).</p>"},{"location":"api/models/#zae_limiter.models.BucketState.capacity","title":"capacity  <code>property</code>","text":"<pre><code>capacity\n</code></pre> <p>Capacity (not millitokens).</p>"},{"location":"api/models/#zae_limiter.models.BucketState.burst","title":"burst  <code>property</code>","text":"<pre><code>burst\n</code></pre> <p>Burst (not millitokens).</p>"},{"location":"api/models/#zae_limiter.models.BucketState.from_limit","title":"from_limit  <code>classmethod</code>","text":"<pre><code>from_limit(entity_id, resource, limit, now_ms)\n</code></pre> <p>Create a new bucket at full capacity from a Limit.</p> <p>Note: This is an internal factory method. Validation of entity_id and resource is performed at the API boundary (RateLimiter public methods) before calling this method.</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>str</code> <p>Entity identifier (pre-validated by caller)</p> required <code>resource</code> <code>str</code> <p>Resource name (pre-validated by caller)</p> required <code>limit</code> <code>Limit</code> <p>Limit configuration (validated via post_init)</p> required <code>now_ms</code> <code>int</code> <p>Current time in milliseconds</p> required"},{"location":"api/models/#auditevent","title":"AuditEvent","text":""},{"location":"api/models/#zae_limiter.models.AuditEvent","title":"AuditEvent  <code>dataclass</code>","text":"<pre><code>AuditEvent(event_id, timestamp, action, entity_id, principal=None, resource=None, details=dict())\n</code></pre> <p>Security audit event for tracking modifications.</p> <p>Audit events are logged for security-sensitive operations: - Entity creation and deletion - Limit configuration changes</p> <p>Attributes:</p> Name Type Description <code>event_id</code> <code>str</code> <p>Unique identifier for the event (timestamp-based)</p> <code>timestamp</code> <code>str</code> <p>ISO timestamp when the event occurred</p> <code>action</code> <code>str</code> <p>Type of action (see AuditAction constants)</p> <code>entity_id</code> <code>str</code> <p>ID of the entity affected</p> <code>principal</code> <code>str | None</code> <p>Caller identity who performed the action (optional)</p> <code>resource</code> <code>str | None</code> <p>Resource name for limit-related actions (optional)</p> <code>details</code> <code>dict[str, Any]</code> <p>Additional action-specific details</p>"},{"location":"api/models/#zae_limiter.models.AuditEvent.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Serialize to dictionary for storage.</p>"},{"location":"api/models/#zae_limiter.models.AuditEvent.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data)\n</code></pre> <p>Deserialize from dictionary.</p>"},{"location":"api/models/#auditaction","title":"AuditAction","text":""},{"location":"api/models/#zae_limiter.models.AuditAction","title":"AuditAction","text":"<p>Audit action type constants.</p>"},{"location":"api/models/#usagesnapshot","title":"UsageSnapshot","text":""},{"location":"api/models/#zae_limiter.models.UsageSnapshot","title":"UsageSnapshot  <code>dataclass</code>","text":"<pre><code>UsageSnapshot(entity_id, resource, window_start, window_end, window_type, counters, total_events)\n</code></pre> <p>Aggregated usage for a time window.</p> <p>Created by the aggregator Lambda from DynamoDB stream events. Tracks token consumption per limit type within a time window.</p> <p>Attributes:</p> Name Type Description <code>entity_id</code> <code>str</code> <p>Entity that consumed tokens</p> <code>resource</code> <code>str</code> <p>Resource being rate-limited (e.g., \"gpt-4\")</p> <code>window_start</code> <code>str</code> <p>ISO timestamp of window start (e.g., \"2024-01-01T14:00:00Z\")</p> <code>window_end</code> <code>str</code> <p>ISO timestamp of window end</p> <code>window_type</code> <code>str</code> <p>Window granularity (\"hourly\", \"daily\")</p> <code>counters</code> <code>dict[str, int]</code> <p>Consumption by limit name (e.g., {\"tpm\": 5000, \"rpm\": 10})</p> <code>total_events</code> <code>int</code> <p>Number of consumption events in this window</p>"},{"location":"api/models/#usagesummary","title":"UsageSummary","text":""},{"location":"api/models/#zae_limiter.models.UsageSummary","title":"UsageSummary  <code>dataclass</code>","text":"<pre><code>UsageSummary(snapshot_count, total, average, min_window_start, max_window_start)\n</code></pre> <p>Aggregated usage summary across multiple snapshots.</p> <p>Returned by <code>RateLimiter.get_usage_summary()</code> to provide total and average consumption statistics over a time range.</p> <p>Attributes:</p> Name Type Description <code>snapshot_count</code> <code>int</code> <p>Number of snapshots aggregated</p> <code>total</code> <code>dict[str, int]</code> <p>Sum of consumption by limit name (e.g., {\"tpm\": 50000, \"rpm\": 100})</p> <code>average</code> <code>dict[str, float]</code> <p>Average consumption per snapshot by limit name</p> <code>min_window_start</code> <code>str | None</code> <p>Earliest snapshot window start (ISO timestamp)</p> <code>max_window_start</code> <code>str | None</code> <p>Latest snapshot window start (ISO timestamp)</p> Example <p>summary = await limiter.get_usage_summary(     entity_id=\"user-123\",     resource=\"gpt-4\",     window_type=\"hourly\", ) print(f\"Total tokens: {summary.total.get('tpm', 0)}\") print(f\"Average per hour: {summary.average.get('tpm', 0.0):.1f}\")</p>"},{"location":"api/models/#limiterinfo","title":"LimiterInfo","text":""},{"location":"api/models/#zae_limiter.models.LimiterInfo","title":"LimiterInfo  <code>dataclass</code>","text":"<pre><code>LimiterInfo(stack_name, user_name, region, stack_status, creation_time, last_updated_time=None, version=None, lambda_version=None, schema_version=None)\n</code></pre> <p>Information about a deployed rate limiter instance.</p> <p>Represents a CloudFormation stack discovered in a region via <code>RateLimiter.list_deployed()</code> or the <code>zae-limiter list</code> CLI command. This is a READ-ONLY model describing observed infrastructure state.</p> Example <p>Attributes:</p> Name Type Description <code>stack_name</code> <code>str</code> <p>Full CloudFormation stack name (e.g., \"my-app\")</p> <code>user_name</code> <code>str</code> <p>User-friendly name (e.g., \"my-app\")</p> <code>region</code> <code>str</code> <p>AWS region where the stack is deployed</p> <code>stack_status</code> <code>str</code> <p>CloudFormation stack status (e.g., \"CREATE_COMPLETE\")</p> <code>creation_time</code> <code>str</code> <p>ISO 8601 timestamp of stack creation</p> <code>last_updated_time</code> <code>str | None</code> <p>ISO 8601 timestamp of last update (None if never updated)</p> <code>version</code> <code>str | None</code> <p>Value of zae-limiter:version tag (client version at deployment)</p> <code>lambda_version</code> <code>str | None</code> <p>Value of zae-limiter:lambda-version tag</p> <code>schema_version</code> <code>str | None</code> <p>Value of zae-limiter:schema-version tag</p>"},{"location":"api/models/#zae_limiter.models.LimiterInfo--discover-all-limiters-in-us-east-1","title":"Discover all limiters in us-east-1","text":"<p>limiters = await RateLimiter.list_deployed(region=\"us-east-1\") for limiter in limiters:     if limiter.is_failed:         print(f\"\u26a0\ufe0f  {limiter.user_name}: {limiter.stack_status}\")</p>"},{"location":"api/models/#zae_limiter.models.LimiterInfo.is_healthy","title":"is_healthy  <code>property</code>","text":"<pre><code>is_healthy\n</code></pre> <p>Stack is in a stable, operational state.</p>"},{"location":"api/models/#zae_limiter.models.LimiterInfo.is_in_progress","title":"is_in_progress  <code>property</code>","text":"<pre><code>is_in_progress\n</code></pre> <p>Stack operation is in progress.</p>"},{"location":"api/models/#zae_limiter.models.LimiterInfo.is_failed","title":"is_failed  <code>property</code>","text":"<pre><code>is_failed\n</code></pre> <p>Stack is in a failed or rollback state.</p>"},{"location":"api/models/#cachestats","title":"CacheStats","text":""},{"location":"api/models/#zae_limiter.config_cache.CacheStats","title":"CacheStats  <code>dataclass</code>","text":"<pre><code>CacheStats(hits=0, misses=0, size=0, ttl_seconds=0)\n</code></pre> <p>Statistics for cache performance monitoring.</p>"},{"location":"api/models/#zae_limiter.config_cache.CacheStats.as_dict","title":"as_dict","text":"<pre><code>as_dict()\n</code></pre> <p>Return stats as a dictionary.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to zae-limiter!</p>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<pre><code>git clone https://github.com/zeroae/zae-limiter.git\ncd zae-limiter\nuv sync --all-extras\npytest\n</code></pre>"},{"location":"contributing/#development-guides","title":"Development Guides","text":"Guide Description Development Setup Environment setup, running tests, code quality LocalStack Local AWS development environment Testing Test organization, pytest fixtures, CI Architecture DynamoDB schema, token bucket algorithm"},{"location":"contributing/#detailed-reference","title":"Detailed Reference","text":"<p>For comprehensive development instructions including:</p> <ul> <li>Build commands and linting</li> <li>Commit message conventions</li> <li>Code review guidelines</li> <li>Release process</li> </ul> <p>See CLAUDE.md in the repository root.</p>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create a feature branch from <code>main</code></li> <li>Make changes following project conventions</li> <li>Ensure CI passes (lint, type check, tests)</li> <li>Submit PR for review</li> </ol> <p>All changes must go through pull requests. Direct commits to <code>main</code> are not allowed.</p>"},{"location":"contributing/architecture/","title":"Architecture","text":"<p>This guide covers the internal architecture of zae-limiter, including the DynamoDB schema and token bucket implementation.</p>"},{"location":"contributing/architecture/#dynamodb-schema-single-table","title":"DynamoDB Schema (Single Table)","text":"<p>All data is stored in a single DynamoDB table using a composite key pattern:</p> Record Type PK SK Entity metadata <code>ENTITY#{id}</code> <code>#META</code> Bucket <code>ENTITY#{id}</code> <code>#BUCKET#{resource}#{limit_name}</code> Entity limit config <code>ENTITY#{id}</code> <code>#LIMIT#{resource}#{limit_name}</code> Resource limit config <code>RESOURCE#{resource}</code> <code>#LIMIT#{resource}#{limit_name}</code> System limit config <code>SYSTEM#</code> <code>#LIMIT#{resource}#{limit_name}</code> Usage snapshot <code>ENTITY#{id}</code> <code>#USAGE#{resource}#{window_key}</code> System version <code>SYSTEM#</code> <code>#VERSION</code> Audit events <code>AUDIT#{entity_id}</code> <code>#AUDIT#{timestamp}</code>"},{"location":"contributing/architecture/#global-secondary-indexes","title":"Global Secondary Indexes","text":"Index Purpose Key Pattern GSI1 Parent \u2192 Children lookup <code>GSI1PK=PARENT#{id}</code> \u2192 <code>GSI1SK=CHILD#{id}</code> GSI2 Resource aggregation <code>GSI2PK=RESOURCE#{name}</code> \u2192 buckets/usage"},{"location":"contributing/architecture/#access-patterns","title":"Access Patterns","text":"Pattern Query Get entity <code>PK=ENTITY#{id}, SK=#META</code> Get buckets <code>PK=ENTITY#{id}, SK begins_with #BUCKET#</code> Batch get buckets <code>BatchGetItem</code> with multiple PK/SK pairs Get children GSI1: <code>GSI1PK=PARENT#{id}</code> Resource capacity GSI2: <code>GSI2PK=RESOURCE#{name}, SK begins_with BUCKET#</code> Get version <code>PK=SYSTEM#, SK=#VERSION</code> Get audit events <code>PK=AUDIT#{entity_id}, SK begins_with #AUDIT#</code> Get usage snapshots <code>PK=ENTITY#{id}, SK begins_with #USAGE#</code> Get system limits <code>PK=SYSTEM#, SK begins_with #LIMIT#{resource}#</code> Get resource limits <code>PK=RESOURCE#{resource}, SK begins_with #LIMIT#{resource}#</code> Get entity limits <code>PK=ENTITY#{id}, SK begins_with #LIMIT#{resource}#</code>"},{"location":"contributing/architecture/#optimized-read-patterns","title":"Optimized Read Patterns","text":"<p>The <code>acquire()</code> operation uses <code>BatchGetItem</code> to fetch all required buckets in a single DynamoDB round trip (see Issue #133):</p> <pre><code># Before: N sequential GetItem calls\nfor entity_id, resource, limit_name in bucket_keys:\n    bucket = await get_bucket(entity_id, resource, limit_name)\n\n# After: 1 BatchGetItem call\nbuckets = await batch_get_buckets(bucket_keys)\n</code></pre> <p>This optimization is particularly beneficial for cascade scenarios where both entity and parent buckets are fetched together, reducing latency from 2\u00d7N round trips to 1.</p>"},{"location":"contributing/architecture/#item-structure","title":"Item Structure","text":"<p>Most record types use a nested <code>data</code> map for business attributes:</p> <pre><code># Entity and Audit records use nested data.M:\n{\n    \"PK\": \"ENTITY#user-1\",\n    \"SK\": \"#META\",\n    \"entity_id\": \"user-1\",\n    \"data\": {                    # Nested map\n        \"name\": \"User One\",\n        \"parent_id\": null,\n        \"metadata\": {...}\n    }\n}\n</code></pre> <p>Bucket records use a HYBRID schema: Most fields are in <code>data.M</code>, but <code>total_consumed_milli</code> is stored as a flat top-level attribute:</p> <pre><code># Bucket record (HYBRID structure):\n{\n    \"PK\": \"ENTITY#user-1\",\n    \"SK\": \"#BUCKET#gpt-4#tpm\",\n    \"entity_id\": \"user-1\",\n    \"data\": {\n        \"M\": {\n            \"resource\": \"gpt-4\",\n            \"limit_name\": \"tpm\",\n            \"tokens_milli\": 9500000,\n            \"last_refill_ms\": 1704067200000,\n            # ... other bucket fields\n        }\n    },\n    \"total_consumed_milli\": 500000,  # FLAT - net consumption counter\n    \"GSI2PK\": \"RESOURCE#gpt-4\",\n    \"ttl\": 1234567890\n}\n</code></pre> <p>The <code>total_consumed_milli</code> counter tracks net consumption (increases on consume, decreases on release) and is used by the aggregator Lambda to accurately calculate consumption deltas. This counter is independent of token bucket refill, solving the issue where <code>old_tokens - new_tokens</code> gives incorrect results when refill rate exceeds consumption rate. See Issue #179.</p> <p>Usage snapshots use a FLAT structure (no nested <code>data</code> map):</p> <pre><code># Usage snapshot (FLAT structure):\n{\n    \"PK\": \"ENTITY#user-1\",\n    \"SK\": \"#USAGE#gpt-4#2024-01-01T14:00:00Z\",\n    \"entity_id\": \"user-1\",\n    \"resource\": \"gpt-4\",        # Top-level attribute\n    \"window\": \"hourly\",         # Top-level attribute\n    \"window_start\": \"...\",      # Top-level attribute\n    \"tpm\": 5000,                # Counter at top-level\n    \"total_events\": 10,         # Counter at top-level\n    \"GSI2PK\": \"RESOURCE#gpt-4\",\n    \"ttl\": 1234567890\n}\n</code></pre> <p>Why snapshots are flat: DynamoDB has a limitation where you cannot SET a map path (<code>#data = if_not_exists(#data, :map)</code>) AND ADD to paths within it (<code>#data.counter</code>) in the same UpdateExpression - it fails with \"overlapping document paths\" error. Snapshots require atomic upsert (create-or-update) with ADD counters for usage aggregation, so they use a flat structure to enable single-call atomic updates.</p> <p>See: Issue #168</p> <p>Limit config records also use FLAT structure (v0.5.0+):</p> <pre><code># System/Resource/Entity limit config (FLAT structure):\n{\n    \"PK\": \"RESOURCE#gpt-4\",           # or SYSTEM# or ENTITY#{id}\n    \"SK\": \"#LIMIT#gpt-4#tpm\",\n    \"resource\": \"gpt-4\",               # Top-level\n    \"limit_name\": \"tpm\",               # Top-level\n    \"capacity\": 100000,                # Top-level\n    \"burst\": 100000,                   # Top-level\n    \"refill_amount\": 100000,           # Top-level\n    \"refill_period_seconds\": 60        # Top-level\n}\n</code></pre> <p>Limit configs use three-level precedence: Entity &gt; Resource &gt; System &gt; Constructor defaults.</p> <p>Key builders:</p> <ul> <li><code>pk_system()</code> - Returns <code>SYSTEM#</code></li> <li><code>pk_resource(resource)</code> - Returns <code>RESOURCE#{resource}</code></li> <li><code>pk_entity(entity_id)</code> - Returns <code>ENTITY#{entity_id}</code></li> <li><code>sk_limit(resource, limit_name)</code> - Returns <code>#LIMIT#{resource}#{limit_name}</code></li> </ul> <p>Audit entity IDs for config levels (see ADR-106):</p> <ul> <li>System config: Uses <code>$SYSTEM</code> as entity_id</li> <li>Resource config: Uses <code>$RESOURCE:{resource_name}</code> (e.g., <code>$RESOURCE:gpt-4</code>)</li> </ul>"},{"location":"contributing/architecture/#token-bucket-implementation","title":"Token Bucket Implementation","text":"<p>For a conceptual overview of the token bucket algorithm, see the User Guide. This section covers implementation details for contributors.</p>"},{"location":"contributing/architecture/#core-functions","title":"Core Functions","text":"<p>The algorithm is implemented in <code>bucket.py</code>:</p> Function Purpose Lines <code>refill_bucket()</code> Calculate refilled tokens with drift compensation 27-75 <code>try_consume()</code> Atomic check-and-consume operation 78-134 <code>force_consume()</code> Force consume (can go negative) 224-255 <code>calculate_retry_after()</code> Calculate wait time for deficit 137-159"},{"location":"contributing/architecture/#mathematical-formulas","title":"Mathematical Formulas","text":"<p>Refill calculation (lazy, on-demand):</p> <pre><code>tokens_to_add = (elapsed_ms \u00d7 refill_amount_milli) // refill_period_ms\n</code></pre> <p>Drift compensation (prevents accumulated rounding errors):</p> <pre><code>time_used_ms = (tokens_to_add \u00d7 refill_period_ms) // refill_amount_milli\nnew_last_refill = last_refill_ms + time_used_ms\n</code></pre> <p>The inverse calculation ensures we only \"consume\" the time that corresponds to whole tokens, preventing drift over many refill cycles.</p> <p>Retry-after calculation:</p> <pre><code>time_ms = (deficit_milli \u00d7 refill_period_ms) // refill_amount_milli\nretry_seconds = (time_ms + 1) / 1000.0  # +1ms rounds up\n</code></pre>"},{"location":"contributing/architecture/#integer-arithmetic-for-precision","title":"Integer Arithmetic for Precision","text":"<p>All token values are stored as millitokens (\u00d71000) to avoid floating-point precision issues in distributed systems:</p> <pre><code># User sees: 100 tokens/minute\n# Stored as: 100,000 millitokens/minute\ncapacity_milli = 100_000\n</code></pre> <p>Why integers matter in distributed systems:</p> <ul> <li>Floating-point operations can produce different results on different hardware</li> <li>DynamoDB stores numbers as strings, so precision loss can occur during serialization</li> <li>Rate limiting across multiple nodes requires identical calculations</li> </ul>"},{"location":"contributing/architecture/#refill-rate-storage","title":"Refill Rate Storage","text":"<p>Refill rates are stored as a fraction (amount/period) rather than a decimal:</p> <pre><code># 100 tokens per minute stored as:\nrefill_amount_milli = 100_000  # millitokens (numerator)\nrefill_period_ms = 60_000      # milliseconds (denominator)\n</code></pre> <p>This avoids representing <code>1.6667 tokens/second</code> as a float. Instead:</p> <pre><code># 100 tokens/minute = 100,000 millitokens / 60,000 ms\n# Integer division handles the math precisely\n</code></pre>"},{"location":"contributing/architecture/#lazy-refill-with-drift-compensation","title":"Lazy Refill with Drift Compensation","text":"<p>Tokens are calculated on-demand rather than via a background timer. The <code>refill_bucket()</code> function:</p> <ol> <li>Calculates elapsed time since last refill</li> <li>Computes tokens to add using integer division</li> <li>Tracks \"time consumed\" to prevent drift</li> </ol> <pre><code># From bucket.py:refill_bucket()\ntokens_to_add = (elapsed_ms * refill_amount_milli) // refill_period_ms\n\n# Drift compensation: only advance time for tokens actually added\ntime_used_ms = (tokens_to_add * refill_period_ms) // refill_amount_milli\nnew_last_refill = last_refill_ms + time_used_ms\n</code></pre> <p>Without drift compensation, repeated calls with small time intervals would accumulate rounding errors.</p>"},{"location":"contributing/architecture/#negative-buckets-debt","title":"Negative Buckets (Debt)","text":"<p>Buckets can go negative to support post-hoc reconciliation:</p> <pre><code># Estimate 500 tokens, actually used 2000\nasync with limiter.acquire(consume={\"tpm\": 500}) as lease:\n    actual = await call_llm()  # Returns 2000 tokens\n    await lease.adjust(tpm=2000 - 500)  # Bucket at -1500\n</code></pre> <p>The <code>force_consume()</code> function handles this:</p> <pre><code># From bucket.py:force_consume()\n# Consume can go negative - no bounds checking\nnew_tokens_milli = refill.new_tokens_milli - (amount * 1000)\n</code></pre> <p>The debt is repaid as tokens refill over time. A bucket at -1500 millitokens needs 1.5 minutes to reach 0 (at 1000 tokens/minute).</p>"},{"location":"contributing/architecture/#burst-capacity","title":"Burst Capacity","text":"<p>Burst allows temporary exceeding of sustained rate:</p> <pre><code># Sustained: 10k tokens/minute\n# Burst: 15k tokens (one-time)\nLimit.per_minute(\"tpm\", 10_000, burst=15_000)\n</code></pre> <p>When <code>burst &gt; capacity</code>, users can consume up to <code>burst</code> tokens immediately, then sustain at <code>capacity</code> rate.</p>"},{"location":"contributing/architecture/#design-decisions","title":"Design Decisions","text":"Decision Rationale Integer over float Identical results across distributed nodes; no precision drift Lazy over continuous No background timers; accurate retry_after; efficient Negative allowed Estimate-then-reconcile pattern; operations with unknown cost Fraction over decimal Exact representation of rates like 100/minute"},{"location":"contributing/architecture/#atomicity","title":"Atomicity","text":""},{"location":"contributing/architecture/#transactwriteitems","title":"TransactWriteItems","text":"<p>Multi-entity updates (like cascade mode) use DynamoDB transactions:</p> <pre><code># Single atomic operation:\n# 1. Consume from child entity\n# 2. Consume from parent entity\n# Both succeed or both fail\n</code></pre> <p>Transaction limits: max 100 items per transaction.</p>"},{"location":"contributing/architecture/#optimistic-locking","title":"Optimistic Locking","text":"<p>Entity metadata uses version numbers for optimistic locking:</p> <pre><code># Read entity with version 5\n# Update fails if version changed\ncondition_expression=\"version = :expected_version\"\n</code></pre>"},{"location":"contributing/architecture/#project-structure","title":"Project Structure","text":"<pre><code>src/zae_limiter/\n\u251c\u2500\u2500 __init__.py        # Public API exports\n\u251c\u2500\u2500 models.py          # Limit, Entity, LimitStatus, BucketState, StackOptions\n\u251c\u2500\u2500 exceptions.py      # RateLimitExceeded, RateLimiterUnavailable, etc.\n\u251c\u2500\u2500 naming.py          # Resource name validation (ZAEL- prefix retained for legacy discovery)\n\u251c\u2500\u2500 bucket.py          # Token bucket math (integer arithmetic)\n\u251c\u2500\u2500 schema.py          # DynamoDB key builders\n\u251c\u2500\u2500 repository.py      # DynamoDB operations\n\u251c\u2500\u2500 lease.py           # Lease context manager\n\u251c\u2500\u2500 limiter.py         # RateLimiter, SyncRateLimiter\n\u251c\u2500\u2500 cli.py             # CLI commands\n\u251c\u2500\u2500 version.py         # Version tracking and compatibility\n\u251c\u2500\u2500 migrations/        # Schema migration framework\n\u2514\u2500\u2500 infra/\n    \u251c\u2500\u2500 stack_manager.py    # CloudFormation stack operations\n    \u251c\u2500\u2500 lambda_builder.py   # Lambda deployment package builder\n    \u2514\u2500\u2500 cfn_template.yaml   # CloudFormation template\n</code></pre>"},{"location":"contributing/architecture/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li>Lease commits only on success: If any exception occurs in the context, changes are rolled back</li> <li>Bucket can go negative: <code>lease.adjust()</code> never throws, allows debt</li> <li>Cascade is per-entity config: Set <code>cascade=True</code> on <code>create_entity()</code> to auto-cascade to parent on every <code>acquire()</code></li> <li>Stored limits override defaults: When <code>use_stored_limits=True</code></li> <li>Transactions are atomic: Multi-entity updates succeed or fail together</li> </ol>"},{"location":"contributing/architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Development Setup - Setting up your environment</li> <li>Testing - Test organization and fixtures</li> </ul>"},{"location":"contributing/development/","title":"Development Setup","text":"<p>This guide covers setting up a local development environment for zae-limiter.</p>"},{"location":"contributing/development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or 3.12</li> <li>uv (recommended) or conda</li> <li>Docker (for LocalStack integration tests)</li> </ul>"},{"location":"contributing/development/#setup","title":"Setup","text":""},{"location":"contributing/development/#using-uv-recommended","title":"Using uv (Recommended)","text":"<pre><code>git clone --recurse-submodules https://github.com/zeroae/zae-limiter.git\ncd zae-limiter\nuv sync --all-extras\n</code></pre>"},{"location":"contributing/development/#using-conda","title":"Using conda","text":"<pre><code>git clone --recurse-submodules https://github.com/zeroae/zae-limiter.git\ncd zae-limiter\nconda create -n zae-limiter python=3.12\nconda activate zae-limiter\npip install -e \".[dev]\"\n</code></pre> <p>Already cloned?</p> <p>If you cloned without <code>--recurse-submodules</code>, run: <pre><code>git submodule update --init --recursive\n</code></pre></p>"},{"location":"contributing/development/#running-tests","title":"Running Tests","text":"<pre><code># Run all unit tests\npytest tests/unit/ -v\n\n# Run with coverage\npytest --cov=zae_limiter --cov-report=html\n\n# Run specific test file\npytest tests/unit/test_limiter.py -v\n</code></pre> <p>For integration tests with LocalStack, see the Testing Guide.</p>"},{"location":"contributing/development/#code-quality","title":"Code Quality","text":"<pre><code># Format code\nruff format .\n\n# Lint and auto-fix\nruff check --fix .\n\n# Type checking\nmypy src/zae_limiter\n</code></pre>"},{"location":"contributing/development/#commit-messages","title":"Commit Messages","text":"<p>Follow the ZeroAE commit conventions:</p> <pre><code># Examples\nfeat(limiter): add hierarchical rate limiting support\nfix(bucket): prevent integer overflow in refill calculation\ndocs(readme): add CloudFormation deployment guide\nrefactor(schema): simplify DynamoDB key structure\n</code></pre> <p>Project scopes: <code>limiter</code>, <code>bucket</code>, <code>cli</code>, <code>infra</code>, <code>ci</code>, <code>aggregator</code>, <code>models</code>, <code>schema</code>, <code>repository</code>, <code>lease</code>, <code>exceptions</code>, <code>test</code>, <code>benchmark</code></p>"},{"location":"contributing/development/#documentation","title":"Documentation","text":"<p>To preview documentation locally:</p> <pre><code>uv run mkdocs serve --livereload --dirty\n</code></pre> <p>Then open http://localhost:8000/zae-limiter/ in your browser.</p> <p>Live reload requires <code>--livereload</code></p> <p>Due to a known issue with Click 8.3.x, the <code>--livereload</code> flag must be explicitly passed for automatic browser refresh to work.</p>"},{"location":"contributing/development/#next-steps","title":"Next Steps","text":"<ul> <li>LocalStack - Local AWS development</li> <li>Testing - Test organization and fixtures</li> <li>Architecture - DynamoDB schema and design decisions</li> </ul>"},{"location":"contributing/localstack/","title":"LocalStack Development","text":"<p>LocalStack provides a local AWS environment for development and testing. This guide covers setting up zae-limiter with LocalStack.</p>"},{"location":"contributing/localstack/#why-localstack","title":"Why LocalStack?","text":"<ul> <li>Free - No AWS costs during development</li> <li>Fast - No network latency</li> <li>Isolated - No risk to production data</li> <li>Full stack - DynamoDB, Lambda, Streams, CloudFormation</li> </ul>"},{"location":"contributing/localstack/#quick-start","title":"Quick Start","text":""},{"location":"contributing/localstack/#1-start-localstack","title":"1. Start LocalStack","text":"Docker Compose (Preferred)DockerLocalStack CLI <p>The project includes a pre-configured <code>docker-compose.yml</code> at the repository root:</p> <pre><code># From the project root\ndocker compose up -d\n</code></pre> <p>This is the preferred method as it includes all required configuration for Lambda execution.</p> <pre><code>docker run -d \\\n  --name localstack \\\n  -p 4566:4566 \\\n  -e SERVICES=dynamodb,dynamodbstreams,lambda,cloudformation,logs,iam,cloudwatch,sqs,s3 \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v \"${TMPDIR:-/tmp}/localstack:/var/lib/localstack\" \\\n  localstack/localstack\n</code></pre> <p>Docker Socket Required</p> <p>The Docker socket mount (<code>-v /var/run/docker.sock:/var/run/docker.sock</code>) is required for LocalStack to spawn Lambda functions as Docker containers.</p> <pre><code>pip install localstack\nlocalstack start -d\n</code></pre>"},{"location":"contributing/localstack/#2-deploy-infrastructure","title":"2. Deploy Infrastructure","text":"<pre><code>zae-limiter deploy \\\n    --name limiter \\\n    --endpoint-url http://localhost:4566 \\\n    --region us-east-1\n</code></pre>"},{"location":"contributing/localstack/#3-use-in-code","title":"3. Use in Code","text":"<pre><code>from zae_limiter import RateLimiter, Limit\n\nlimiter = RateLimiter(\n    name=\"limiter\",\n    endpoint_url=\"http://localhost:4566\",\n    region=\"us-east-1\",\n)\n\nasync with limiter.acquire(\n    entity_id=\"test-user\",\n    resource=\"api\",\n    limits=[Limit.per_minute(\"requests\", 100)],\n    consume={\"requests\": 1},\n) as lease:\n    print(\"Rate limited request!\")\n</code></pre>"},{"location":"contributing/localstack/#declarative-infrastructure","title":"Declarative Infrastructure","text":"<p>For quick iteration, declare infrastructure in code:</p> <pre><code>from zae_limiter import RateLimiter, StackOptions\n\nlimiter = RateLimiter(\n    name=\"limiter\",\n    endpoint_url=\"http://localhost:4566\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(),  # Declare desired state\n)\n</code></pre>"},{"location":"contributing/localstack/#environment-variables","title":"Environment Variables","text":"<p>Configure via environment variables for easy switching:</p> <pre><code># .env.local\nAWS_ENDPOINT_URL=http://localhost:4566\nAWS_ACCESS_KEY_ID=test\nAWS_SECRET_ACCESS_KEY=test\nAWS_DEFAULT_REGION=us-east-1\n</code></pre> <pre><code>import os\n\nlimiter = RateLimiter(\n    name=\"limiter\",\n    endpoint_url=os.getenv(\"AWS_ENDPOINT_URL\"),\n    region=os.getenv(\"AWS_DEFAULT_REGION\", \"us-east-1\"),\n)\n</code></pre>"},{"location":"contributing/localstack/#debugging","title":"Debugging","text":""},{"location":"contributing/localstack/#check-stack-status","title":"Check Stack Status","text":"<pre><code># List stacks\naws --endpoint-url=http://localhost:4566 cloudformation list-stacks\n\n# Describe stack\naws --endpoint-url=http://localhost:4566 cloudformation describe-stacks \\\n    --stack-name limiter\n</code></pre>"},{"location":"contributing/localstack/#inspect-dynamodb","title":"Inspect DynamoDB","text":"<pre><code># List tables\naws --endpoint-url=http://localhost:4566 dynamodb list-tables\n\n# Scan table\naws --endpoint-url=http://localhost:4566 dynamodb scan \\\n    --table-name limiter\n</code></pre>"},{"location":"contributing/localstack/#view-lambda-logs","title":"View Lambda Logs","text":"<pre><code># List functions\naws --endpoint-url=http://localhost:4566 lambda list-functions\n\n# Get logs\naws --endpoint-url=http://localhost:4566 logs tail \\\n    /aws/lambda/zae-limiter-aggregator\n</code></pre>"},{"location":"contributing/localstack/#localstack-vs-dynamodb-local","title":"LocalStack vs DynamoDB Local","text":"Feature LocalStack DynamoDB Local DynamoDB Yes Yes Streams Yes Limited Lambda Yes No CloudFormation Yes No Cost Free Free Fidelity High Medium <p>Recommendation: Use LocalStack for full integration testing, DynamoDB Local for quick unit tests.</p>"},{"location":"contributing/localstack/#troubleshooting","title":"Troubleshooting","text":""},{"location":"contributing/localstack/#connection-refused","title":"Connection Refused","text":"<pre><code>Cannot connect to http://localhost:4566\n</code></pre> <p>Solution: Ensure LocalStack is running:</p> <pre><code>docker ps | grep localstack\n# or\ncurl http://localhost:4566/_localstack/health\n</code></pre>"},{"location":"contributing/localstack/#lambda-not-executing","title":"Lambda Not Executing","text":"<p>Check Lambda logs:</p> <pre><code>docker logs localstack 2&gt;&amp;1 | grep -i lambda\n</code></pre> <p>Ensure the Lambda service is enabled:</p> <pre><code>docker run -e SERVICES=dynamodb,dynamodbstreams,lambda,cloudformation,logs,iam,cloudwatch,sqs,s3 ...\n</code></pre>"},{"location":"contributing/localstack/#slow-performance","title":"Slow Performance","text":"<p>LocalStack can be slow on first request. Consider:</p> <ul> <li>Pre-warming containers</li> <li>Using persistence for faster restarts</li> <li>Reducing DEBUG level</li> </ul>"},{"location":"contributing/localstack/#next-steps","title":"Next Steps","text":"<ul> <li>Testing - pytest fixtures and CI configuration</li> <li>Development Setup - Local development environment</li> </ul>"},{"location":"contributing/testing/","title":"Testing","text":"<p>This guide covers test organization, pytest fixtures, and CI configuration for zae-limiter.</p>"},{"location":"contributing/testing/#test-organization","title":"Test Organization","text":"<p>Tests are organized by execution environment and scope:</p> <pre><code>tests/\n\u251c\u2500\u2500 conftest.py                  # Shared config (--run-aws flag)\n\u251c\u2500\u2500 unit/                        # Fast tests with mocked AWS (moto)\n\u2502   \u251c\u2500\u2500 test_limiter.py\n\u2502   \u251c\u2500\u2500 test_repository.py\n\u2502   \u2514\u2500\u2500 test_sync_limiter.py\n\u251c\u2500\u2500 integration/                 # LocalStack tests (repository-level)\n\u2502   \u2514\u2500\u2500 test_repository.py\n\u251c\u2500\u2500 e2e/                         # Full workflow tests (LocalStack + AWS)\n\u2502   \u251c\u2500\u2500 test_localstack.py\n\u2502   \u2514\u2500\u2500 test_aws.py\n\u2514\u2500\u2500 benchmark/                   # Performance benchmarks (pytest-benchmark)\n    \u251c\u2500\u2500 test_operations.py       # Mocked benchmarks\n    \u2514\u2500\u2500 test_localstack.py       # LocalStack benchmarks\n</code></pre>"},{"location":"contributing/testing/#test-categories","title":"Test Categories","text":"Category Directory Backend What to Test Speed Unit <code>tests/unit/</code> moto (mocked) Business logic, bucket math, schema, exceptions Fast (~seconds) Integration <code>tests/integration/</code> LocalStack Repository operations, transactions, GSI queries Medium E2E <code>tests/e2e/</code> LocalStack or AWS Full workflows: CLI, rate limiting, hierarchical limits Slow Benchmark <code>tests/benchmark/</code> moto or LocalStack Latency (p50/p95/p99), throughput, cascade overhead Variable"},{"location":"contributing/testing/#pytest-markers","title":"Pytest Markers","text":"Marker Description How to Run (none) Unit tests <code>pytest tests/unit/</code> <code>@pytest.mark.integration</code> Requires LocalStack <code>pytest -m integration</code> <code>@pytest.mark.e2e</code> End-to-end workflows <code>pytest -m e2e</code> <code>@pytest.mark.aws</code> Real AWS (requires <code>--run-aws</code>) <code>pytest -m aws --run-aws</code> <code>@pytest.mark.benchmark</code> Performance benchmarks <code>pytest -m benchmark</code> <code>@pytest.mark.slow</code> Tests with &gt;30s waits Skip with <code>-m \"not slow\"</code>"},{"location":"contributing/testing/#pytest-fixtures","title":"pytest Fixtures","text":""},{"location":"contributing/testing/#localstack-endpoint-fixture","title":"LocalStack Endpoint Fixture","text":"<pre><code>import os\nimport pytest\n\n@pytest.fixture\ndef localstack_endpoint():\n    \"\"\"Get LocalStack endpoint from environment.\"\"\"\n    return os.getenv(\"AWS_ENDPOINT_URL\", \"http://localhost:4566\")\n</code></pre>"},{"location":"contributing/testing/#function-scoped-limiter-isolated","title":"Function-Scoped Limiter (Isolated)","text":"<pre><code>import uuid\nimport pytest\nfrom zae_limiter import RateLimiter, StackOptions\n\n@pytest.fixture(scope=\"function\")\nasync def limiter(localstack_endpoint):\n    \"\"\"\n    Create a rate limiter connected to LocalStack with automatic cleanup.\n\n    This fixture:\n    1. Creates a unique stack for test isolation\n    2. Yields the limiter for test use\n    3. Deletes the stack in teardown\n    \"\"\"\n    # Unique name prevents test interference\n    name = f\"test-{uuid.uuid4().hex[:8]}\"\n\n    limiter = RateLimiter(\n        name=name,\n        endpoint_url=localstack_endpoint,\n        region=\"us-east-1\",\n        stack_options=StackOptions(enable_aggregator=False),\n    )\n\n    async with limiter:\n        yield limiter\n\n    # Cleanup: delete the CloudFormation stack\n    await limiter.delete_stack()\n\n\n@pytest.mark.integration\nasync def test_rate_limiting(limiter):\n    async with limiter.acquire(\n        entity_id=\"test-user\",\n        resource=\"api\",\n        limits=[Limit.per_minute(\"requests\", 10)],\n        consume={\"requests\": 1},\n    ):\n        pass  # Success\n</code></pre>"},{"location":"contributing/testing/#session-scoped-limiter-faster","title":"Session-Scoped Limiter (Faster)","text":"<p>For test suites where stack creation overhead is significant:</p> <pre><code>@pytest.fixture(scope=\"session\")\nasync def shared_limiter(localstack_endpoint):\n    \"\"\"\n    Session-scoped limiter for faster test execution.\n\n    Trade-off: Tests share state, less isolation.\n    \"\"\"\n    limiter = RateLimiter(\n        name=\"integration-test-shared\",\n        endpoint_url=localstack_endpoint,\n        region=\"us-east-1\",\n        stack_options=StackOptions(enable_aggregator=False),\n    )\n\n    async with limiter:\n        yield limiter\n\n    await limiter.delete_stack()\n</code></pre>"},{"location":"contributing/testing/#sync-fixture-example","title":"Sync Fixture Example","text":"<pre><code>@pytest.fixture(scope=\"function\")\ndef sync_limiter(localstack_endpoint):\n    \"\"\"Synchronous rate limiter with cleanup.\"\"\"\n    from zae_limiter import SyncRateLimiter, StackOptions\n    import uuid\n\n    name = f\"test-sync-{uuid.uuid4().hex[:8]}\"\n\n    limiter = SyncRateLimiter(\n        name=name,\n        endpoint_url=localstack_endpoint,\n        region=\"us-east-1\",\n        stack_options=StackOptions(enable_aggregator=False),\n    )\n\n    with limiter:\n        yield limiter\n\n    limiter.delete_stack()\n</code></pre>"},{"location":"contributing/testing/#running-tests","title":"Running Tests","text":""},{"location":"contributing/testing/#unit-tests-only-no-docker","title":"Unit Tests Only (No Docker)","text":"<pre><code>pytest tests/unit/ -v\n</code></pre>"},{"location":"contributing/testing/#integration-tests-requires-localstack","title":"Integration Tests (Requires LocalStack)","text":"<pre><code># Start LocalStack\ndocker compose up -d\n\n# Set environment variables\nexport AWS_ENDPOINT_URL=http://localhost:4566\nexport AWS_ACCESS_KEY_ID=test\nexport AWS_SECRET_ACCESS_KEY=test\nexport AWS_DEFAULT_REGION=us-east-1\n\n# Run integration tests\npytest tests/integration/ -v\n\n# Stop LocalStack\ndocker compose down\n</code></pre>"},{"location":"contributing/testing/#e2e-tests","title":"E2E Tests","text":"<pre><code># LocalStack E2E\npytest tests/e2e/test_localstack.py -v\n\n# Real AWS E2E (costs money!)\npytest tests/e2e/test_aws.py --run-aws -v\n</code></pre>"},{"location":"contributing/testing/#benchmarks","title":"Benchmarks","text":"<p>Performance benchmarks measure operation latency, throughput, and DynamoDB capacity. Benchmarks are essential for detecting performance regressions when optimizing operations like config caching and cascade resolution.</p> <p>Quick Start:</p> <pre><code># Mocked benchmarks (fast - no Docker needed)\nuv run pytest tests/benchmark/test_operations.py -v\n\n# LocalStack benchmarks (realistic latency - requires Docker)\ndocker compose up -d\nexport AWS_ENDPOINT_URL=http://localhost:4566\nexport AWS_ACCESS_KEY_ID=test\nexport AWS_SECRET_ACCESS_KEY=test\nexport AWS_DEFAULT_REGION=us-east-1\nuv run pytest tests/benchmark/test_localstack.py -v\ndocker compose down\n\n# Export results to JSON for comparison\nuv run pytest tests/benchmark/ -v --benchmark-json=benchmark.json\n</code></pre> <p>Benchmark Categories:</p> Test File Backend Purpose Speed <code>test_operations.py</code> moto (mocked) Fast iteration, baseline measurements &lt; 10s <code>test_localstack.py</code> DynamoDB emulation Realistic network latency, real-world metrics 30-60s <code>test_latency.py</code> moto p50/p95/p99 latency breakdown &lt; 10s <code>test_throughput.py</code> moto Sequential/concurrent throughput &lt; 30s <code>test_capacity.py</code> moto RCU/WCU tracking &lt; 10s <code>test_aws.py</code> Real AWS Production metrics (optional) 60-120s <p>Performance Workflow:</p> <ol> <li> <p>Establish baseline before optimization: <pre><code>uv run pytest tests/benchmark/test_operations.py -v \\\n  --benchmark-json=baseline.json\n</code></pre></p> </li> <li> <p>Implement optimization (e.g., config caching, BatchGetItem)</p> </li> <li> <p>Compare against baseline: <pre><code>uv run pytest tests/benchmark/test_operations.py -v \\\n  --benchmark-compare=baseline.json\n</code></pre></p> </li> </ol> <p>Key Benchmarks:</p> Benchmark Purpose Typical Overhead <code>test_acquire_release_single_limit</code> Baseline operation ~1ms (mocked) <code>test_acquire_with_cached_config</code> Config cache hit &lt; 5% overhead <code>test_acquire_cold_config</code> Config cache miss &lt; 15% overhead <code>test_cascade_with_batchgetitem_optimization</code> Cascade optimization 10-20% improvement <code>test_cascade_with_config_cache_optimization</code> Combined optimizations 20-30% improvement <p>Interpreting Results:</p> <pre><code>test_operations.py::TestAcquireReleaseBenchmarks::test_acquire_release_single_limit\n  mean \u00b1 std dev: 1.23 \u00b1 0.15 ms [min: 0.98 ms, max: 1.65 ms] (PASS)\n</code></pre> <ul> <li><code>PASS</code>: Performance stable (no regression)</li> <li><code>FAIL</code>: Regression detected (&lt; -5% typical threshold)</li> <li>Positive/negative % = improvement/degradation vs baseline</li> </ul> <p>Storing Baselines:</p> <p>After establishing a good baseline, save it for future comparison:</p> <pre><code># Save baseline with version\ncp baseline.json docs/benchmark-v0.11.0.json\ngit add docs/benchmark-v0.11.0.json\n\n# Compare future runs\npytest tests/benchmark/ -v --benchmark-compare=docs/benchmark-v0.11.0.json\n</code></pre> <p>Adding New Benchmarks:</p> <p>When adding performance-sensitive code:</p> <ol> <li>Create test in appropriate benchmark file</li> <li>Include clear docstring explaining what's measured</li> <li>Compare against related baseline test</li> <li>Use <code>@pytest.mark.benchmark</code> marker for filtering</li> <li>Run locally and verify results</li> <li>Document expected performance targets</li> </ol> <p>Example:</p> <pre><code>@pytest.mark.benchmark\ndef test_acquire_with_new_optimization(self, benchmark, sync_limiter):\n    \"\"\"Measure acquire with new optimization.\n\n    Expected: 10% improvement over baseline due to [reason].\n    \"\"\"\n    limits = [Limit.per_minute(\"rpm\", 1_000_000)]\n\n    def operation():\n        with sync_limiter.acquire(\n            entity_id=\"bench-opt\",\n            resource=\"api\",\n            limits=limits,\n            consume={\"rpm\": 1},\n        ):\n            pass\n\n    benchmark(operation)\n</code></pre>"},{"location":"contributing/testing/#ci-configuration","title":"CI Configuration","text":"<p>Example GitHub Actions workflow for integration tests:</p> <pre><code># .github/workflows/ci.yml\njobs:\n  integration:\n    runs-on: ubuntu-latest\n    services:\n      localstack:\n        image: localstack/localstack\n        ports:\n          - 4566:4566\n        env:\n          SERVICES: dynamodb,dynamodbstreams,lambda,cloudformation,logs,iam,cloudwatch,sqs\n        options: &gt;-\n          --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock\n    steps:\n      - uses: actions/checkout@v4\n      - run: pip install -e \".[dev]\"\n      - run: pytest -m integration\n        env:\n          AWS_ENDPOINT_URL: http://localhost:4566\n          AWS_ACCESS_KEY_ID: test\n          AWS_SECRET_ACCESS_KEY: test\n</code></pre>"},{"location":"contributing/testing/#when-to-add-tests","title":"When to Add Tests","text":"<ul> <li>New business logic (bucket calculations, limit validation) \u2192 <code>unit/</code></li> <li>New DynamoDB operations (queries, transactions, GSI) \u2192 <code>integration/</code></li> <li>New user-facing features (CLI commands, rate limiting workflows) \u2192 <code>e2e/</code></li> <li>AWS-specific behavior (alarms, DLQ, CloudWatch metrics) \u2192 <code>e2e/test_aws.py</code></li> <li>Performance-sensitive code (new operations, optimizations) \u2192 <code>benchmark/</code></li> </ul>"},{"location":"contributing/testing/#test-coverage","title":"Test Coverage","text":"<pre><code>pytest --cov=zae_limiter --cov-report=html\nopen htmlcov/index.html\n</code></pre>"},{"location":"contributing/testing/#next-steps","title":"Next Steps","text":"<ul> <li>LocalStack - Local AWS development environment</li> <li>Architecture - Understanding the codebase</li> </ul>"},{"location":"guide/basic-usage/","title":"Basic Usage","text":"<p>This guide covers common rate limiting patterns with zae-limiter.</p>"},{"location":"guide/basic-usage/#the-acquire-context-manager","title":"The Acquire Context Manager","text":"<p>The <code>acquire()</code> method is the primary API for rate limiting:</p> <pre><code>async with limiter.acquire(\n    entity_id=\"user-123\",      # Who is being rate limited\n    resource=\"gpt-4\",          # What resource they're accessing\n    limits=[...],              # Rate limit definitions\n    consume={\"rpm\": 1},        # How much to consume\n) as lease:\n    # Your code here\n    pass\n</code></pre> <p>Behavior:</p> <ul> <li>On entry: Checks limits and consumes tokens</li> <li>On success: Commits the consumption</li> <li>On exception: Rolls back the consumption</li> </ul>"},{"location":"guide/basic-usage/#multiple-limits","title":"Multiple Limits","text":"<p>Track multiple limits in a single call:</p> <pre><code>async with limiter.acquire(\n    entity_id=\"api-key-123\",\n    resource=\"gpt-4\",\n    limits=[\n        Limit.per_minute(\"rpm\", 100),       # 100 requests/minute\n        Limit.per_minute(\"tpm\", 10_000),    # 10,000 tokens/minute\n        Limit.per_hour(\"rph\", 1_000),       # 1,000 requests/hour\n    ],\n    consume={\"rpm\": 1, \"tpm\": 500, \"rph\": 1},\n) as lease:\n    response = await call_llm()\n</code></pre> <p>All limits are checked atomically. If any limit is exceeded, the request is rejected.</p> <p>Performance Tip</p> <p>Combining multiple limits into a single <code>acquire()</code> call is more efficient than separate calls. See Batch Operation Patterns for details.</p>"},{"location":"guide/basic-usage/#burst-capacity","title":"Burst Capacity","text":"<p>Allow temporary bursts above the sustained rate:</p> <pre><code># Sustain 10k tokens/minute, but allow bursts up to 15k\nlimits = [\n    Limit.per_minute(\"tpm\", 10_000, burst=15_000),\n]\n</code></pre> <p>The bucket starts full at <code>burst</code> capacity and refills at <code>capacity</code> tokens per period. See Token Bucket Algorithm for details on how burst and capacity interact.</p>"},{"location":"guide/basic-usage/#adjusting-consumption","title":"Adjusting Consumption","text":"<p>Use <code>lease.adjust()</code> to modify consumption after the fact:</p> <pre><code>async with limiter.acquire(\n    entity_id=\"key-123\",\n    resource=\"gpt-4\",\n    limits=[Limit.per_minute(\"tpm\", 10_000)],\n    consume={\"tpm\": 500},  # Initial estimate\n) as lease:\n    response = await call_llm()\n\n    # Adjust based on actual usage\n    actual_tokens = response.usage.total_tokens\n    await lease.adjust(tpm=actual_tokens - 500)\n</code></pre> <p>Negative Adjustments</p> <p><code>adjust()</code> can go negative, allowing the bucket to go into debt. This is useful for post-hoc reconciliation when actual usage exceeds estimates. See Token Bucket Algorithm - Negative Buckets for how debt works.</p>"},{"location":"guide/basic-usage/#check-capacity-without-consuming","title":"Check Capacity Without Consuming","text":""},{"location":"guide/basic-usage/#check-available-tokens","title":"Check Available Tokens","text":"<pre><code>available = await limiter.available(\n    entity_id=\"key-123\",\n    resource=\"gpt-4\",\n    limits=[Limit.per_minute(\"tpm\", 10_000)],\n)\nprint(f\"Available tokens: {available['tpm']}\")\n</code></pre>"},{"location":"guide/basic-usage/#check-time-until-available","title":"Check Time Until Available","text":"<pre><code>wait_seconds = await limiter.time_until_available(\n    entity_id=\"key-123\",\n    resource=\"gpt-4\",\n    limits=[Limit.per_minute(\"tpm\", 10_000)],\n    needed={\"tpm\": 5_000},\n)\n\nif wait_seconds &gt; 0:\n    print(f\"Need to wait {wait_seconds}s for capacity\")\n</code></pre>"},{"location":"guide/basic-usage/#automatic-limit-resolution","title":"Automatic Limit Resolution","text":"<p>zae-limiter automatically resolves limits from stored configurations using a three-level hierarchy. See Configuration Hierarchy for full details.</p> <p>Resolution order (highest to lowest precedence):</p> <ol> <li>Entity level - Specific limits for an entity+resource pair</li> <li>Resource level - Default limits for a resource (all entities)</li> <li>System level - Global defaults (all resources)</li> <li>Override parameter - Fallback if no stored config exists</li> </ol> <pre><code># Set system-wide defaults (lowest precedence)\nawait limiter.set_system_defaults(\n    limits=[Limit.per_minute(\"rpm\", 100)],\n)\n\n# Set resource defaults (overrides system for this resource)\nawait limiter.set_resource_defaults(\n    resource=\"gpt-4\",\n    limits=[Limit.per_minute(\"rpm\", 50)],\n)\n\n# Set entity-specific limits (highest precedence)\nawait limiter.set_limits(\n    entity_id=\"user-premium\",\n    resource=\"gpt-4\",\n    limits=[\n        Limit.per_minute(\"rpm\", 500),        # 5x normal\n        Limit.per_minute(\"tpm\", 50_000),     # 5x normal\n    ],\n)\n\n# Limits are resolved automatically - no special flag needed\nasync with limiter.acquire(\n    entity_id=\"user-premium\",\n    resource=\"gpt-4\",\n    limits=None,  # Auto-resolves to entity-level (500 rpm)\n    consume={\"rpm\": 1},\n) as lease:\n    ...\n\n# Free user falls back to resource defaults (50 rpm)\nasync with limiter.acquire(\n    entity_id=\"user-free\",\n    resource=\"gpt-4\",\n    limits=None,  # Auto-resolves to resource-level\n    consume={\"rpm\": 1},\n) as lease:\n    ...\n\n# Override stored config for a specific call\nasync with limiter.acquire(\n    entity_id=\"user-premium\",\n    resource=\"gpt-4\",\n    limits=[Limit.per_minute(\"rpm\", 10)],  # Explicit override\n    consume={\"rpm\": 1},\n) as lease:\n    ...\n</code></pre> <p>v0.5.0 Breaking Change</p> <p>Prior to v0.5.0, you needed <code>use_stored_limits=True</code> to enable limit lookup. This parameter is now deprecated - limits are always resolved automatically.</p>"},{"location":"guide/basic-usage/#entity-management","title":"Entity Management","text":""},{"location":"guide/basic-usage/#create-entities","title":"Create Entities","text":"<pre><code># Create a standalone entity\nawait limiter.create_entity(\n    entity_id=\"user-123\",\n    name=\"John Doe\",\n)\n\n# Create a child entity (API key under a project)\nawait limiter.create_entity(\n    entity_id=\"key-abc\",\n    parent_id=\"project-1\",\n    name=\"Production API Key\",\n)\n</code></pre>"},{"location":"guide/basic-usage/#get-entity-information","title":"Get Entity Information","text":"<pre><code>entity = await limiter.get_entity(\"user-123\")\nprint(f\"Name: {entity.name}\")\nprint(f\"Parent: {entity.parent_id}\")\n</code></pre>"},{"location":"guide/basic-usage/#error-handling","title":"Error Handling","text":""},{"location":"guide/basic-usage/#ratelimitexceeded-details","title":"RateLimitExceeded Details","text":"<pre><code>try:\n    async with limiter.acquire(...):\n        ...\nexcept RateLimitExceeded as e:\n    # All limit statuses\n    for status in e.statuses:\n        print(f\"{status.limit_name}: {status.available}/{status.limit.capacity}\")\n\n    # Only violations\n    for v in e.violations:\n        print(f\"Exceeded: {v.limit_name}\")\n\n    # Primary bottleneck\n    print(f\"Bottleneck: {e.primary_violation.limit_name}\")\n\n    # For API responses\n    return e.as_dict()\n</code></pre>"},{"location":"guide/basic-usage/#service-unavailable","title":"Service Unavailable","text":"<pre><code>from zae_limiter import RateLimiterUnavailable\n\ntry:\n    async with limiter.acquire(...):\n        ...\nexcept RateLimiterUnavailable as e:\n    # DynamoDB is unavailable\n    # Behavior depends on on_unavailable setting\n    print(f\"Service unavailable: {e}\")\n</code></pre>"},{"location":"guide/basic-usage/#config-cache","title":"Config Cache","text":"<p>zae-limiter caches config data (system defaults, resource defaults, entity limits) to reduce DynamoDB reads. The cache has a 60-second TTL by default.</p>"},{"location":"guide/basic-usage/#configuring-cache-ttl","title":"Configuring Cache TTL","text":"<pre><code>from zae_limiter import RateLimiter, Repository\n\n# Default: 60-second cache TTL\nlimiter = RateLimiter(\n    repository=Repository(name=\"my-app\", region=\"us-east-1\"),\n    config_cache_ttl=60,\n)\n\n# Disable caching (for testing)\nlimiter = RateLimiter(\n    repository=Repository(name=\"my-app\", region=\"us-east-1\"),\n    config_cache_ttl=0,\n)\n</code></pre>"},{"location":"guide/basic-usage/#manual-cache-invalidation","title":"Manual Cache Invalidation","text":"<p>After modifying config, force immediate refresh:</p> <pre><code>await limiter.set_system_defaults([Limit.per_minute(\"rpm\", 1000)])\nawait limiter.invalidate_config_cache()  # Optional: force refresh\n</code></pre>"},{"location":"guide/basic-usage/#monitoring-cache-performance","title":"Monitoring Cache Performance","text":"<pre><code>stats = limiter.get_cache_stats()\nprint(f\"Hits: {stats.hits}, Misses: {stats.misses}\")\nprint(f\"Cache entries: {stats.size}\")\n</code></pre> <p>See Config Cache Tuning for advanced configuration.</p>"},{"location":"guide/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Hierarchical Limits - Parent/child rate limiting</li> <li>LLM Integration - Token estimation patterns</li> <li>Unavailability Handling - Handling service outages</li> </ul>"},{"location":"guide/config-hierarchy/","title":"Configuration Hierarchy","text":"<p>zae-limiter supports a three-level configuration hierarchy for rate limits, allowing you to set defaults at the system level and override them at the resource or entity level.</p>"},{"location":"guide/config-hierarchy/#overview","title":"Overview","text":"flowchart TD     A[Acquire Request] --&gt; B{Entity config?}     B --&gt;|Yes| C[Use Entity Limits]     B --&gt;|No| D{Resource config?}     D --&gt;|Yes| E[Use Resource Limits]     D --&gt;|No| F{System config?}     F --&gt;|Yes| G[Use System Limits]     F --&gt;|No| H[Use Constructor Defaults] <p>Precedence order (highest to lowest):</p> <ol> <li>Entity limits - Per-entity + resource overrides (e.g., premium user on gpt-4)</li> <li>Resource defaults - Per-resource limits (e.g., all users on gpt-4)</li> <li>System defaults - Global limits for all resources</li> <li>Constructor defaults - Fallback from code</li> </ol>"},{"location":"guide/config-hierarchy/#when-to-use-each-level","title":"When to Use Each Level","text":"Level Use Case Example System Universal defaults across all resources 100 RPM baseline for all models Resource Per-model or per-API limits gpt-4 gets 50 RPM, claude-3 gets 200 RPM Entity Premium tiers, custom contracts Enterprise customer gets 10x limits"},{"location":"guide/config-hierarchy/#python-api","title":"Python API","text":""},{"location":"guide/config-hierarchy/#system-defaults","title":"System Defaults","text":"<p>System defaults apply to all resources unless overridden:</p> <pre><code>from zae_limiter import RateLimiter, Limit, OnUnavailable\n\nlimiter = RateLimiter(name=\"my-app\", region=\"us-east-1\")\n\n# Set system-wide defaults (no resource parameter)\nawait limiter.set_system_defaults(\n    limits=[\n        Limit.per_minute(\"rpm\", 100),\n        Limit.per_minute(\"tpm\", 10_000),\n    ],\n    on_unavailable=OnUnavailable.ALLOW,  # Optional: behavior on DynamoDB failure\n)\n\n# Get system defaults\nlimits, on_unavailable = await limiter.get_system_defaults()\nprint(f\"on_unavailable: {on_unavailable}\")\nfor limit in limits:\n    print(f\"  {limit.name}: {limit.capacity}/min\")\n\n# Delete system defaults\nawait limiter.delete_system_defaults()\n</code></pre>"},{"location":"guide/config-hierarchy/#resource-defaults","title":"Resource Defaults","text":"<p>Resource defaults override system defaults for a specific resource:</p> <pre><code># Set resource-specific defaults (overrides system)\nawait limiter.set_resource_defaults(\n    resource=\"gpt-4\",\n    limits=[\n        Limit.per_minute(\"rpm\", 50),       # Lower RPM for expensive model\n        Limit.per_minute(\"tpm\", 100_000),  # Higher TPM\n    ],\n)\n\n# Get resource defaults\ndefaults = await limiter.get_resource_defaults(\"gpt-4\")\n\n# List all resources with configured defaults\nresources = await limiter.list_resources_with_defaults()\n# ['gpt-4', 'claude-3', ...]\n\n# Delete resource defaults\nawait limiter.delete_resource_defaults(\"gpt-4\")\n</code></pre>"},{"location":"guide/config-hierarchy/#entity-limits","title":"Entity Limits","text":"<p>Entity limits override both system and resource defaults for a specific entity+resource pair:</p> <pre><code># Set entity-specific limits (highest precedence)\nawait limiter.set_limits(\n    entity_id=\"user-premium\",\n    resource=\"gpt-4\",\n    limits=[\n        Limit.per_minute(\"rpm\", 500),       # 10x normal\n        Limit.per_minute(\"tpm\", 1_000_000), # 10x normal\n    ],\n)\n\n# Get entity limits\nlimits = await limiter.get_limits(\n    entity_id=\"user-premium\",\n    resource=\"gpt-4\",\n)\n\n# Delete entity limits\nawait limiter.delete_limits(\n    entity_id=\"user-premium\",\n    resource=\"gpt-4\",\n)\n</code></pre>"},{"location":"guide/config-hierarchy/#cli-commands","title":"CLI Commands","text":""},{"location":"guide/config-hierarchy/#system-defaults_1","title":"System Defaults","text":"<pre><code># Set system-wide defaults (applies to ALL resources)\nzae-limiter system set-defaults -l rpm:100 -l tpm:10000 --on-unavailable allow\n\n# Get system defaults\nzae-limiter system get-defaults\n\n# Delete system defaults\nzae-limiter system delete-defaults --yes\n</code></pre>"},{"location":"guide/config-hierarchy/#resource-defaults_1","title":"Resource Defaults","text":"<pre><code># Set resource-specific defaults\nzae-limiter resource set-defaults gpt-4 -l rpm:50 -l tpm:100000\n\n# Get resource defaults\nzae-limiter resource get-defaults gpt-4\n\n# List resources with defaults\nzae-limiter resource list\n\n# Delete resource defaults\nzae-limiter resource delete-defaults gpt-4 --yes\n</code></pre>"},{"location":"guide/config-hierarchy/#entity-limits_1","title":"Entity Limits","text":"<pre><code># Set entity-specific limits\nzae-limiter entity set-limits user-premium --resource gpt-4 -l rpm:500 -l tpm:1000000\n\n# Get entity limits\nzae-limiter entity get-limits user-premium --resource gpt-4\n\n# Delete entity limits\nzae-limiter entity delete-limits user-premium --resource gpt-4 --yes\n</code></pre>"},{"location":"guide/config-hierarchy/#example-multi-tier-pricing","title":"Example: Multi-Tier Pricing","text":"<p>Here's how to implement a typical SaaS pricing model:</p> <pre><code>async def setup_rate_limits(limiter: RateLimiter):\n    # 1. System defaults: Free tier baseline (all resources)\n    await limiter.set_system_defaults(\n        limits=[\n            Limit.per_minute(\"rpm\", 10),\n            Limit.per_minute(\"tpm\", 1_000),\n        ],\n        on_unavailable=OnUnavailable.BLOCK,\n    )\n\n    # 2. Resource defaults: Different limits per model\n    await limiter.set_resource_defaults(\n        resource=\"gpt-4\",\n        limits=[\n            Limit.per_minute(\"rpm\", 5),    # Expensive model: lower limits\n            Limit.per_minute(\"tpm\", 500),\n        ],\n    )\n\n    await limiter.set_resource_defaults(\n        resource=\"gpt-3.5-turbo\",\n        limits=[\n            Limit.per_minute(\"rpm\", 20),   # Cheap model: higher limits\n            Limit.per_minute(\"tpm\", 5_000),\n        ],\n    )\n\n    # 3. Entity limits: Premium customers\n    await limiter.set_limits(\n        entity_id=\"enterprise-customer\",\n        resource=\"gpt-4\",\n        limits=[\n            Limit.per_minute(\"rpm\", 100),  # 20x free tier\n            Limit.per_minute(\"tpm\", 100_000),\n        ],\n    )\n</code></pre> <p>With this setup:</p> User Resource RPM TPM Free user gpt-4 5 500 Free user gpt-3.5-turbo 20 5,000 Free user any other 10 1,000 Enterprise gpt-4 100 100,000 Enterprise gpt-3.5-turbo 20 5,000 (resource default)"},{"location":"guide/config-hierarchy/#system-config-on_unavailable","title":"System Config: on_unavailable","text":"<p>The <code>on_unavailable</code> setting controls behavior when DynamoDB is unreachable:</p> Value Behavior Use Case <code>ALLOW</code> Allow requests (fail open) High availability is critical <code>BLOCK</code> Block requests (fail closed) Rate limiting is critical <pre><code># Set at system level\nawait limiter.set_system_defaults(\n    limits=[...],\n    on_unavailable=OnUnavailable.ALLOW,\n)\n\n# Or via CLI\nzae-limiter system set-defaults -l rpm:100 --on-unavailable allow\n</code></pre> <p>Note</p> <p><code>on_unavailable</code> is only configurable at the system level, not per-resource or per-entity.</p>"},{"location":"guide/config-hierarchy/#replacement-semantics","title":"Replacement Semantics","text":"<p>When you set limits at any level, the new limits completely replace the previous ones:</p> <pre><code># Initial: rpm=100, tpm=10000\nawait limiter.set_system_defaults([\n    Limit.per_minute(\"rpm\", 100),\n    Limit.per_minute(\"tpm\", 10_000),\n])\n\n# After this: rpm is GONE, only tpm=20000 exists\nawait limiter.set_system_defaults([\n    Limit.per_minute(\"tpm\", 20_000),\n])\n</code></pre> <p>To add or update a single limit while preserving others, read first:</p> <pre><code># Read existing\ncurrent, on_unavail = await limiter.get_system_defaults()\n\n# Modify\ncurrent_dict = {l.name: l for l in current}\ncurrent_dict[\"rpm\"] = Limit.per_minute(\"rpm\", 200)\n\n# Write back\nawait limiter.set_system_defaults(list(current_dict.values()))\n</code></pre>"},{"location":"guide/config-hierarchy/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Usage - Rate limiting patterns</li> <li>Hierarchical Limits - Parent/child entity relationships</li> <li>Unavailability Handling - Handling DynamoDB outages</li> </ul>"},{"location":"guide/hierarchical/","title":"Hierarchical Limits","text":"<p>zae-limiter supports two-level hierarchies for rate limiting, enabling patterns like:</p> <ul> <li>Project \u2192 API Keys: Limit total project usage while also limiting individual keys</li> <li>Organization \u2192 Users: Organization-wide limits with per-user quotas</li> <li>Tenant \u2192 Services: Multi-tenant limits with service-level controls</li> </ul>"},{"location":"guide/hierarchical/#creating-a-hierarchy","title":"Creating a Hierarchy","text":"<pre><code># Create parent entity (project)\nawait limiter.create_entity(\n    entity_id=\"project-1\",\n    name=\"Production Project\",\n)\n\n# Create child entities (API keys) with cascade enabled\nawait limiter.create_entity(\n    entity_id=\"key-abc\",\n    parent_id=\"project-1\",\n    name=\"Web Application Key\",\n    cascade=True,  # Enforce parent limits on every acquire\n)\n\nawait limiter.create_entity(\n    entity_id=\"key-xyz\",\n    parent_id=\"project-1\",\n    name=\"Mobile App Key\",\n    cascade=True,\n)\n</code></pre>"},{"location":"guide/hierarchical/#cascade-mode","title":"Cascade Mode","text":"<p>Create entities with <code>cascade=True</code> to apply rate limits to both the child and parent on every <code>acquire()</code> call:</p> <pre><code># Cascade is set once at entity creation\nawait limiter.create_entity(\n    entity_id=\"key-abc\",\n    parent_id=\"project-1\",\n    cascade=True,  # All acquire() calls will also check parent\n)\n\n# acquire() automatically cascades to parent \u2014 no flag needed\nasync with limiter.acquire(\n    entity_id=\"key-abc\",\n    resource=\"gpt-4\",\n    limits=[\n        Limit.per_minute(\"tpm\", 10_000),  # Per-key limit\n    ],\n    consume={\"tpm\": 500},\n) as lease:\n    await call_api()\n</code></pre> <p>Performance Impact</p> <p>Cascade mode adds overhead: +1 GetEntity + parent bucket operations. Only enable when hierarchical enforcement is needed. See Batch Operation Patterns for optimization strategies.</p> <p>What happens:</p> <ol> <li>Check if <code>key-abc</code> has capacity (10k tpm)</li> <li>Check if <code>project-1</code> has capacity (uses same limits)</li> <li>If both pass, consume from both atomically</li> <li>If either fails, reject with details about which limit was exceeded</li> </ol>"},{"location":"guide/hierarchical/#different-limits-per-level","title":"Different Limits Per Level","text":"<p>Set different limits for parents and children:</p> <pre><code># Set project-level limits (higher)\nawait limiter.set_limits(\n    entity_id=\"project-1\",\n    limits=[\n        Limit.per_minute(\"tpm\", 100_000),  # 100k for entire project\n    ],\n)\n\n# Set key-level limits (lower)\nawait limiter.set_limits(\n    entity_id=\"key-abc\",\n    limits=[\n        Limit.per_minute(\"tpm\", 10_000),   # 10k per key\n    ],\n)\n\n# acquire() auto-cascades because key-abc was created with cascade=True\nasync with limiter.acquire(\n    entity_id=\"key-abc\",\n    resource=\"gpt-4\",\n    limits=[Limit.per_minute(\"tpm\", 5_000)],  # Default\n    consume={\"tpm\": 500},\n    use_stored_limits=True,  # Uses stored limits for both levels\n) as lease:\n    await call_api()\n</code></pre>"},{"location":"guide/hierarchical/#understanding-cascade-behavior","title":"Understanding Cascade Behavior","text":""},{"location":"guide/hierarchical/#without-cascade","title":"Without Cascade","text":"<pre><code># Entity created without cascade (default)\nawait limiter.create_entity(entity_id=\"key-abc\", parent_id=\"project-1\")\n\n# Only checks/consumes from key-abc\nasync with limiter.acquire(\n    entity_id=\"key-abc\",\n    resource=\"gpt-4\",\n    limits=[Limit.per_minute(\"tpm\", 10_000)],\n    consume={\"tpm\": 500},\n) as lease:\n    ...\n</code></pre>"},{"location":"guide/hierarchical/#with-cascade","title":"With Cascade","text":"<pre><code># Entity created with cascade enabled\nawait limiter.create_entity(entity_id=\"key-abc\", parent_id=\"project-1\", cascade=True)\n\n# Checks/consumes from BOTH key-abc AND project-1\nasync with limiter.acquire(\n    entity_id=\"key-abc\",\n    resource=\"gpt-4\",\n    limits=[Limit.per_minute(\"tpm\", 10_000)],\n    consume={\"tpm\": 500},\n) as lease:\n    ...\n</code></pre>"},{"location":"guide/hierarchical/#error-handling-with-hierarchies","title":"Error Handling with Hierarchies","text":"<p>When an entity has cascade enabled, <code>RateLimitExceeded</code> includes statuses for all entities:</p> <pre><code>try:\n    async with limiter.acquire(\n        entity_id=\"key-abc\",  # Has cascade=True from create_entity()\n        ...\n    ):\n        ...\nexcept RateLimitExceeded as e:\n    for status in e.statuses:\n        print(f\"Entity: {status.entity_id}\")\n        print(f\"  Limit: {status.limit_name}\")\n        print(f\"  Available: {status.available}\")\n        print(f\"  Exceeded: {status.exceeded}\")\n</code></pre>"},{"location":"guide/hierarchical/#use-cases","title":"Use Cases","text":""},{"location":"guide/hierarchical/#multi-tenant-saas","title":"Multi-Tenant SaaS","text":"<pre><code># Tenant has 1M tokens/day\nawait limiter.set_limits(\n    entity_id=\"tenant-acme\",\n    limits=[Limit.per_day(\"tpd\", 1_000_000)],\n)\n\n# Create user under tenant with cascade enabled\nawait limiter.create_entity(entity_id=\"user-123\", parent_id=\"tenant-acme\", cascade=True)\n\n# Each user gets 100k tokens/day\nawait limiter.set_limits(\n    entity_id=\"user-123\",\n    limits=[Limit.per_day(\"tpd\", 100_000)],\n)\n\n# Rate limit user \u2014 auto-cascades to tenant\nasync with limiter.acquire(\n    entity_id=\"user-123\",\n    use_stored_limits=True,\n    ...\n):\n    ...\n</code></pre>"},{"location":"guide/hierarchical/#api-key-management","title":"API Key Management","text":"<pre><code># Project limit: 10k RPM\nawait limiter.set_limits(\n    entity_id=\"project-prod\",\n    limits=[Limit.per_minute(\"rpm\", 10_000)],\n)\n\n# Production key: 5k RPM (half of project)\nawait limiter.set_limits(\n    entity_id=\"key-prod\",\n    limits=[Limit.per_minute(\"rpm\", 5_000)],\n)\n\n# Staging key: 1k RPM\nawait limiter.set_limits(\n    entity_id=\"key-staging\",\n    limits=[Limit.per_minute(\"rpm\", 1_000)],\n)\n</code></pre>"},{"location":"guide/hierarchical/#limitations","title":"Limitations","text":"<ul> <li>Two levels only: Parent \u2192 Child (no grandparents)</li> <li>Single parent: Each entity can have at most one parent</li> <li>Cascade is per-entity: Set <code>cascade=True</code> on <code>create_entity()</code> to enable; it applies to all <code>acquire()</code> calls for that entity</li> </ul>"},{"location":"guide/hierarchical/#next-steps","title":"Next Steps","text":"<ul> <li>LLM Integration - Token estimation patterns</li> <li>Unavailability Handling - Handling service outages</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"guide/llm-integration/","title":"LLM Integration","text":"<p>LLM APIs are a natural fit for zae-limiter's estimate-then-reconcile pattern, especially when token counts are unknown until after the call completes. This guide covers patterns for integrating with LLM providers.</p> <p>Understanding the Algorithm</p> <p>This guide assumes familiarity with how rate limiting works. If you're new to token buckets, start with Token Bucket Algorithm to understand concepts like negative buckets (debt) that enable the estimate-then-reconcile pattern.</p>"},{"location":"guide/llm-integration/#the-challenge","title":"The Challenge","text":"<p>LLM APIs present unique rate limiting challenges:</p> <ol> <li>Token counts are unknown upfront - You don't know how many tokens a response will use</li> <li>Multiple limits - Providers often limit both requests and tokens</li> <li>Variable costs - Different models have different token limits</li> <li>Streaming responses - Token count only known after stream completes</li> </ol>"},{"location":"guide/llm-integration/#basic-pattern-estimate-and-reconcile","title":"Basic Pattern: Estimate and Reconcile","text":"<pre><code>async with limiter.acquire(\n    entity_id=\"api-key-123\",\n    resource=\"gpt-4\",\n    limits=[\n        Limit.per_minute(\"rpm\", 100),       # Requests per minute\n        Limit.per_minute(\"tpm\", 10_000),    # Tokens per minute\n    ],\n    consume={\"rpm\": 1, \"tpm\": 500},  # Estimate 500 tokens\n) as lease:\n    response = await openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n    )\n\n    # Reconcile with actual usage\n    actual_tokens = response.usage.total_tokens\n    await lease.adjust(tpm=actual_tokens - 500)\n</code></pre>"},{"location":"guide/llm-integration/#estimation-strategies","title":"Estimation Strategies","text":""},{"location":"guide/llm-integration/#fixed-estimate","title":"Fixed Estimate","text":"<p>Simple but may over/under-estimate:</p> <pre><code>consume={\"tpm\": 500}  # Always estimate 500 tokens\n</code></pre>"},{"location":"guide/llm-integration/#input-based-estimate","title":"Input-Based Estimate","text":"<p>Estimate based on input length:</p> <pre><code>import tiktoken\n\ndef estimate_tokens(messages: list, model: str = \"gpt-4\") -&gt; int:\n    \"\"\"Estimate tokens for input messages.\"\"\"\n    encoding = tiktoken.encoding_for_model(model)\n    total = 0\n    for msg in messages:\n        total += len(encoding.encode(msg[\"content\"]))\n        total += 4  # Message overhead\n    total += 2  # Completion priming\n    return total\n\n# Use in rate limiting\ninput_tokens = estimate_tokens(messages)\nestimated_output = 500  # Rough estimate for output\ntotal_estimate = input_tokens + estimated_output\n\nasync with limiter.acquire(\n    ...\n    consume={\"tpm\": total_estimate},\n) as lease:\n    response = await call_llm()\n    actual = response.usage.total_tokens\n    await lease.adjust(tpm=actual - total_estimate)\n</code></pre>"},{"location":"guide/llm-integration/#max-tokens-estimate","title":"Max Tokens Estimate","text":"<p>Use max_tokens as upper bound:</p> <pre><code>max_tokens = 1000\n\nasync with limiter.acquire(\n    ...\n    consume={\"tpm\": input_tokens + max_tokens},\n) as lease:\n    response = await openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=messages,\n        max_tokens=max_tokens,\n    )\n    actual = response.usage.total_tokens\n    await lease.adjust(tpm=actual - (input_tokens + max_tokens))\n</code></pre>"},{"location":"guide/llm-integration/#handling-streaming-responses","title":"Handling Streaming Responses","text":"<p>For streaming responses, token count is only available after the stream completes:</p> <pre><code>async with limiter.acquire(\n    entity_id=\"api-key-123\",\n    resource=\"gpt-4\",\n    limits=[\n        Limit.per_minute(\"rpm\", 100),\n        Limit.per_minute(\"tpm\", 10_000),\n    ],\n    consume={\"rpm\": 1, \"tpm\": estimated_tokens},\n) as lease:\n    chunks = []\n    async for chunk in await openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=messages,\n        stream=True,\n    ):\n        chunks.append(chunk)\n        yield chunk  # Stream to client\n\n    # Get final usage from last chunk (OpenAI includes it)\n    if chunks[-1].usage:\n        actual = chunks[-1].usage.total_tokens\n        await lease.adjust(tpm=actual - estimated_tokens)\n</code></pre>"},{"location":"guide/llm-integration/#per-model-rate-limits","title":"Per-Model Rate Limits","text":"<p>Different models have different limits. Use the <code>resource</code> parameter:</p> <pre><code>MODEL_LIMITS = {\n    \"gpt-4\": [\n        Limit.per_minute(\"rpm\", 100),\n        Limit.per_minute(\"tpm\", 10_000),\n    ],\n    \"gpt-4-turbo\": [\n        Limit.per_minute(\"rpm\", 500),\n        Limit.per_minute(\"tpm\", 150_000),\n    ],\n    \"gpt-3.5-turbo\": [\n        Limit.per_minute(\"rpm\", 3500),\n        Limit.per_minute(\"tpm\", 90_000),\n    ],\n}\n\nasync def rate_limited_completion(\n    entity_id: str,\n    model: str,\n    messages: list,\n    estimated_tokens: int,\n):\n    limits = MODEL_LIMITS.get(model, MODEL_LIMITS[\"gpt-3.5-turbo\"])\n\n    async with limiter.acquire(\n        entity_id=entity_id,\n        resource=model,  # Different bucket per model\n        limits=limits,\n        consume={\"rpm\": 1, \"tpm\": estimated_tokens},\n    ) as lease:\n        response = await openai.chat.completions.create(\n            model=model,\n            messages=messages,\n        )\n        actual = response.usage.total_tokens\n        await lease.adjust(tpm=actual - estimated_tokens)\n        return response\n</code></pre>"},{"location":"guide/llm-integration/#negative-buckets-debt","title":"Negative Buckets (Debt)","text":"<p>zae-limiter allows buckets to go negative, which is useful when actual usage exceeds estimates:</p> <pre><code># Estimate: 500 tokens\nasync with limiter.acquire(\n    consume={\"tpm\": 500},\n    ...\n) as lease:\n    response = await call_llm()\n\n    # Actual: 2000 tokens\n    # Adjustment: 2000 - 500 = 1500\n    await lease.adjust(tpm=1500)\n    # Bucket now at -1500 tokens (in debt)\n</code></pre> <p>The debt is repaid as tokens refill over time. This ensures accurate accounting while allowing requests to complete.</p>"},{"location":"guide/llm-integration/#pre-flight-capacity-check","title":"Pre-Flight Capacity Check","text":"<p>Check capacity before making expensive calls:</p> <pre><code>async def call_with_capacity_check(\n    entity_id: str,\n    model: str,\n    messages: list,\n    estimated_tokens: int,\n):\n    limits = MODEL_LIMITS[model]\n\n    # Check available capacity\n    available = await limiter.available(\n        entity_id=entity_id,\n        resource=model,\n        limits=limits,\n    )\n\n    if available[\"tpm\"] &lt; estimated_tokens:\n        # Not enough capacity - check when it will be available\n        wait_time = await limiter.time_until_available(\n            entity_id=entity_id,\n            resource=model,\n            limits=limits,\n            needed={\"tpm\": estimated_tokens},\n        )\n        raise RetryAfter(seconds=wait_time)\n\n    # Proceed with rate-limited call\n    async with limiter.acquire(...):\n        ...\n</code></pre>"},{"location":"guide/llm-integration/#integration-with-retry-libraries","title":"Integration with Retry Libraries","text":"<p>Combine with retry libraries like <code>tenacity</code>:</p> <pre><code>from tenacity import retry, retry_if_exception_type, wait_fixed\n\n@retry(\n    retry=retry_if_exception_type(RateLimitExceeded),\n    wait=wait_fixed(1),\n)\nasync def resilient_llm_call(entity_id: str, messages: list):\n    async with limiter.acquire(\n        entity_id=entity_id,\n        resource=\"gpt-4\",\n        limits=[Limit.per_minute(\"rpm\", 100)],\n        consume={\"rpm\": 1},\n    ):\n        return await openai.chat.completions.create(\n            model=\"gpt-4\",\n            messages=messages,\n        )\n</code></pre> <p>Or use the retry information from the exception:</p> <pre><code>async def smart_retry_llm_call(entity_id: str, messages: list):\n    while True:\n        try:\n            async with limiter.acquire(...):\n                return await call_llm()\n        except RateLimitExceeded as e:\n            await asyncio.sleep(e.retry_after_seconds)\n</code></pre>"},{"location":"guide/llm-integration/#next-steps","title":"Next Steps","text":"<ul> <li>Unavailability Handling - Handling service outages</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"guide/token-bucket/","title":"Token Bucket Algorithm","text":"<p>Understanding how rate limiting works helps you choose the right limits for your application. This page explains the token bucket algorithm and how zae-limiter implements it.</p>"},{"location":"guide/token-bucket/#the-classic-algorithm","title":"The Classic Algorithm","text":"<p>The token bucket algorithm controls request rates using a simple metaphor: imagine a bucket that holds tokens.</p> <ul> <li>Tokens accumulate over time at a fixed rate (the refill rate)</li> <li>Requests consume tokens from the bucket</li> <li>Requests are allowed if enough tokens are available</li> <li>Requests are rejected if the bucket is empty</li> </ul> flowchart LR     subgraph bucket[\"Token Bucket\"]         T[Tokens]     end     Time --&gt;|refill| T     T --&gt;|consume| Request     Request --&gt; Check{Enough tokens?}     Check --&gt;|Yes| Allow[Allow request]     Check --&gt;|No| Reject[Reject + retry_after] <p>This creates a natural rate limit: requests can burst up to the bucket's capacity, but sustained traffic is limited by the refill rate.</p>"},{"location":"guide/token-bucket/#how-zae-limiter-implements-it","title":"How zae-limiter Implements It","text":"<p>zae-limiter uses a modified token bucket optimized for distributed systems. Here's how it differs from the classic algorithm:</p> Aspect Classic Algorithm zae-limiter Token storage Floating-point numbers Integers (millitokens x1000) Refill timing Continuous background process Lazy (calculated on-demand) Minimum tokens 0 (never negative) Can go negative (debt) Precision May drift due to float errors Drift-compensated integers <p>These modifications enable:</p> <ul> <li>Distributed precision: Integer math produces identical results across all nodes</li> <li>Efficiency: No background timers or processes needed</li> <li>Estimate-then-reconcile: Negative buckets allow post-hoc cost adjustment</li> </ul>"},{"location":"guide/token-bucket/#key-concepts","title":"Key Concepts","text":""},{"location":"guide/token-bucket/#capacity-and-burst","title":"Capacity and Burst","text":"<p>Every limit has two key parameters:</p> <ul> <li>Capacity: The sustained rate (tokens that refill per period)</li> <li>Burst: The maximum bucket size (can be larger than capacity)</li> </ul> <pre><code># 10,000 tokens/minute sustained, 15,000 burst\nLimit.per_minute(\"tpm\", capacity=10_000, burst=15_000)\n</code></pre> graph TD     A[\"Bucket starts full at burst (15k)\"]     A --&gt;|\"consume 15k\"| B[\"Bucket empty (0)\"]     B --&gt;|\"wait 1 minute\"| C[\"Refills 10k tokens\"]     C --&gt;|\"wait 30 more seconds\"| D[\"Refills to burst (15k)\"]     D --&gt;|\"steady state\"| E[\"10k tokens/minute\"]      style A fill:#90EE90     style B fill:#FFB6C1     style C fill:#87CEEB     style D fill:#90EE90     style E fill:#87CEEB <p>Key insight: The bucket is larger (15k) but refills at the same rate (10k/minute). After fully depleting the burst, it takes 1.5 minutes to return to full capacity\u2014not 1 minute.</p> <p>When to use burst &gt; capacity:</p> <ul> <li>Startup surge: Handle initial traffic before steady state</li> <li>Bursty workloads: Allow temporary spikes followed by quiet periods</li> <li>User experience: Don't reject the first request just because a minute hasn't passed</li> </ul>"},{"location":"guide/token-bucket/#lazy-refill","title":"Lazy Refill","text":"<p>Unlike traditional implementations that continuously add tokens, zae-limiter calculates refills on-demand:</p> <pre><code>When a request arrives:\n1. Calculate elapsed time since last refill\n2. Add tokens based on elapsed time\n3. Check if enough tokens are available\n4. Consume tokens if allowed\n</code></pre> <p>Why this matters:</p> <ul> <li>Accurate <code>retry_after</code>: Time calculations are exact, not approximations</li> <li>No drift: Integer math with drift compensation prevents accumulated errors</li> <li>Efficient: No background processes consuming resources</li> </ul> <p>The refill formula:</p> <pre><code>tokens_to_add = elapsed_time \u00d7 refill_rate\n             = elapsed_ms \u00d7 refill_amount / refill_period\n</code></pre>"},{"location":"guide/token-bucket/#negative-buckets-debt","title":"Negative Buckets (Debt)","text":"<p>zae-limiter allows buckets to go negative, creating a \"debt\" that must be repaid before more tokens are available. This enables the estimate-then-reconcile pattern for operations with unknown cost.</p> sequenceDiagram     participant App     participant Bucket      Note over Bucket: tokens: 1,000      App-&gt;&gt;Bucket: consume(estimate=500)     Note over Bucket: tokens: 500      App-&gt;&gt;App: execute operation...     Note over App: actual cost: 2,000 units      App-&gt;&gt;Bucket: adjust(+1,500)     Note over Bucket: tokens: -1,000     Note over Bucket: Bucket is now in debt!      Note over Bucket: (time passes, tokens refill)     Note over Bucket: tokens: 0 \u2192 1,000 <p>Why allow negative tokens?</p> <p>Many operations have costs that are unknown until completion:</p> Domain Unknown cost Database queries Rows scanned, data returned File transfers Bytes transferred after compression Batch processing Items processed per batch API calls Metered usage calculated after <p>Negative buckets let you:</p> <ol> <li>Estimate cost upfront (consume)</li> <li>Execute the operation</li> <li>Reconcile based on actual cost (adjust)</li> </ol> <p>The debt is automatically repaid as tokens refill over time.</p> <pre><code>async with limiter.acquire(\n    entity_id=\"user-123\",\n    resource=\"api\",\n    limits=[Limit.per_minute(\"rpm\", 100)],\n    consume={\"rpm\": 5},  # Estimate 5 units\n) as lease:\n    result = await execute_operation()\n    actual = result.units_consumed  # 12\n    await lease.adjust(rpm=actual - 5)  # Add 7 to debt\n</code></pre> <p>See LLM Integration for a specific application of this pattern.</p>"},{"location":"guide/token-bucket/#practical-implications","title":"Practical Implications","text":""},{"location":"guide/token-bucket/#why-estimates-can-be-wrong","title":"Why estimates can be wrong","text":"<p>Because buckets can go negative, your initial estimate doesn't need to be perfect. Underestimate and adjust later:</p> <ul> <li>Estimate too low? Adjust adds to consumption</li> <li>Estimate too high? Adjust can return tokens (negative adjustment)</li> </ul>"},{"location":"guide/token-bucket/#why-retry_after-is-accurate","title":"Why <code>retry_after</code> is accurate","text":"<p>The lazy refill with drift compensation means <code>retry_after</code> tells you exactly when tokens will be available:</p> <pre><code>try:\n    await limiter.check(entity_id, resource, limits, consume={\"tpm\": 1000})\nexcept RateLimitExceeded as e:\n    # This is the exact time to wait\n    await asyncio.sleep(e.retry_after_seconds)\n    # Now the request will succeed\n</code></pre>"},{"location":"guide/token-bucket/#choosing-the-right-limits","title":"Choosing the right limits","text":"Scenario Capacity Burst Rationale Steady API traffic 100 rpm 100 No bursting needed Bursty batch jobs 100 rpm 500 Allow 5x burst, then sustain LLM tokens 10k tpm 15k Handle variable response sizes Database queries 1k rows/min 5k Allow large result sets occasionally New user onboarding 10 rpm 50 Let users explore, then limit"},{"location":"guide/token-bucket/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Usage - Common rate limiting patterns</li> <li>LLM Integration - Token estimation strategies</li> <li>Architecture - Implementation details for contributors</li> </ul>"},{"location":"guide/unavailability/","title":"Unavailability Handling","text":"<p>zae-limiter provides configurable behavior when DynamoDB is unavailable. This guide covers the <code>on_unavailable</code> modes and how to choose the right one for your application.</p> <p>Scope</p> <p>This page covers infrastructure unavailability (DynamoDB errors, timeouts, throttling).</p> <p>For handling rate limit violations, see Basic Usage.</p>"},{"location":"guide/unavailability/#available-modes","title":"Available Modes","text":"Mode Behavior Use Case <code>BLOCK</code> Reject requests Security-critical, billing <code>ALLOW</code> Allow requests User experience priority"},{"location":"guide/unavailability/#what-triggers-on_unavailable-logic","title":"What Triggers on_unavailable Logic","text":"<p>The <code>on_unavailable</code> mode only applies to infrastructure errors. These exceptions always propagate regardless of mode:</p> <ul> <li><code>RateLimitExceeded</code> \u2014 Rate limit violated (business logic)</li> <li><code>ValidationError</code> \u2014 Invalid configuration (user error)</li> </ul> <p>Infrastructure errors that trigger on_unavailable:</p> <ul> <li>Connection timeouts</li> <li>DynamoDB throttling</li> <li>Network failures</li> <li>Service unavailable errors</li> </ul>"},{"location":"guide/unavailability/#block-default","title":"BLOCK (Default)","text":"<p>When DynamoDB is unavailable, reject all rate-limited requests by raising <code>RateLimiterUnavailable</code>.</p> <p>Exception Handling Required</p> <p>When using <code>BLOCK</code> mode (the default), your application must catch <code>RateLimiterUnavailable</code> to handle infrastructure failures gracefully. This exception inherits from <code>InfrastructureError</code>, not <code>RateLimitExceeded</code>.</p> <pre><code>from zae_limiter import RateLimiter, OnUnavailable, RateLimiterUnavailable\n\nlimiter = RateLimiter(\n    name=\"limiter\",\n    on_unavailable=OnUnavailable.BLOCK,  # Default\n)\n\ntry:\n    async with limiter.acquire(...):\n        await do_work()\nexcept RateLimiterUnavailable as e:\n    # DynamoDB is unavailable - handle degraded mode\n    return JSONResponse(\n        status_code=503,\n        content={\"error\": \"Service temporarily unavailable\"},\n    )\n</code></pre> <p>When to use:</p> <ul> <li>Billing/metering systems where accuracy is critical</li> <li>Security-sensitive operations</li> <li>When over-consumption has significant costs</li> <li>Compliance requirements</li> </ul>"},{"location":"guide/unavailability/#allow","title":"ALLOW","text":"<p>When DynamoDB is unavailable, allow requests to proceed:</p> <pre><code>limiter = RateLimiter(\n    name=\"limiter\",\n    on_unavailable=OnUnavailable.ALLOW,\n)\n\n# Requests proceed even if DynamoDB is down\nasync with limiter.acquire(...):\n    await do_work()  # Runs without rate limiting\n</code></pre> <p>When to use:</p> <ul> <li>User experience is the priority</li> <li>Brief outages are acceptable</li> <li>Rate limiting is a soft limit</li> <li>Development/staging environments</li> </ul>"},{"location":"guide/unavailability/#no-op-lease-behavior","title":"No-Op Lease Behavior","text":"<p>When <code>ALLOW</code> activates due to infrastructure failure:</p> <ul> <li>A no-op lease is returned with no bucket entries</li> <li><code>lease.consume()</code>, <code>lease.adjust()</code>, and <code>lease.release()</code> silently do nothing</li> <li>Your code cannot detect degraded mode from the lease itself</li> </ul> <p>To detect and log degraded operations, wrap with custom error handling:</p> <pre><code>async def acquire_with_metrics(limiter, **kwargs):\n    \"\"\"Wrapper that tracks degraded operations.\"\"\"\n    try:\n        async with limiter.acquire(**kwargs) as lease:\n            yield lease\n    except Exception as e:\n        # BLOCK caught the error - we're in degraded mode\n        # This only runs if you use BLOCK and catch manually\n        metrics.increment(\"rate_limiter.degraded\")\n        logger.warning(f\"Rate limiter degraded: {e}\")\n        raise\n</code></pre>"},{"location":"guide/unavailability/#per-request-override","title":"Per-Request Override","text":"<p>Override the default mode for specific requests:</p> <pre><code># Default to BLOCK\nlimiter = RateLimiter(\n    name=\"limiter\",\n    on_unavailable=OnUnavailable.BLOCK,\n)\n\n# But allow this specific request to proceed\nasync with limiter.acquire(\n    entity_id=\"user-123\",\n    resource=\"api\",\n    limits=[...],\n    consume={\"requests\": 1},\n    on_unavailable=OnUnavailable.ALLOW,  # Override for this call\n) as lease:\n    await do_work()\n</code></pre>"},{"location":"guide/unavailability/#handling-unavailable-errors","title":"Handling Unavailable Errors","text":"<p>The <code>RateLimiterUnavailable</code> exception includes details about the failure:</p> <pre><code>from zae_limiter import RateLimiterUnavailable\n\ntry:\n    async with limiter.acquire(...):\n        await do_work()\nexcept RateLimiterUnavailable as e:\n    # Log the underlying error\n    logger.error(f\"Rate limiter unavailable: {e}\")\n\n    # Decide how to handle\n    if is_critical_operation:\n        raise HTTPException(status_code=503)\n    else:\n        # Proceed without rate limiting\n        await do_work()\n</code></pre>"},{"location":"guide/unavailability/#best-practices","title":"Best Practices","text":""},{"location":"guide/unavailability/#1-choose-based-on-risk","title":"1. Choose Based on Risk","text":"<pre><code># High-risk: billing, security\nbilling_limiter = RateLimiter(\n    name=\"billing\",\n    on_unavailable=OnUnavailable.BLOCK,\n)\n\n# Lower-risk: general API\napi_limiter = RateLimiter(\n    name=\"api\",\n    on_unavailable=OnUnavailable.ALLOW,\n)\n</code></pre>"},{"location":"guide/unavailability/#2-graceful-degradation","title":"2. Graceful Degradation","text":"<p>Implement fallback behavior:</p> <pre><code>async def resilient_operation(entity_id: str):\n    try:\n        async with limiter.acquire(\n            entity_id=entity_id,\n            on_unavailable=OnUnavailable.BLOCK,\n            ...\n        ):\n            return await premium_operation()\n    except RateLimiterUnavailable:\n        # Fall back to degraded mode\n        logger.warning(\"Rate limiter unavailable, using fallback\")\n        return await basic_operation()\n</code></pre>"},{"location":"guide/unavailability/#3-health-checks","title":"3. Health Checks","text":"<p>Use <code>is_available()</code> to check rate limiter connectivity:</p> <pre><code>async def health_check():\n    checks = {}\n\n    # Check rate limiter connectivity\n    if await limiter.is_available():\n        checks[\"rate_limiter\"] = \"healthy\"\n    else:\n        checks[\"rate_limiter\"] = \"unhealthy\"\n\n    return checks\n</code></pre> <p>The <code>is_available()</code> method:</p> <ul> <li>Returns <code>True</code> if DynamoDB is reachable, <code>False</code> otherwise</li> <li>Never raises exceptions</li> <li>Uses a configurable timeout (default 1 second)</li> <li>Works without requiring initialization</li> </ul> <pre><code># FastAPI health endpoint example\n@app.get(\"/health\")\nasync def health():\n    return {\n        \"status\": \"healthy\" if await limiter.is_available() else \"degraded\",\n    }\n\n# Pre-flight check before operations\nif not await limiter.is_available():\n    logger.warning(\"Rate limiter unavailable, using fallback\")\n</code></pre>"},{"location":"guide/unavailability/#observability","title":"Observability","text":"<p>For monitoring rate limiter health and setting up alerts, see the Monitoring Guide.</p>"},{"location":"guide/unavailability/#next-steps","title":"Next Steps","text":"<ul> <li>Operations Guide - Troubleshooting and operational procedures</li> <li>Deployment - Infrastructure setup</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"guide/usage-snapshots/","title":"Usage Snapshots","text":"<p>Usage snapshots provide historical consumption data for billing, reporting, and capacity planning. This guide explains how to query and use snapshot data.</p>"},{"location":"guide/usage-snapshots/#what-are-usage-snapshots","title":"What Are Usage Snapshots?","text":"<p>Usage snapshots are time-aggregated consumption records created by the Lambda aggregator from DynamoDB stream events. When you call <code>acquire()</code> and consume tokens, the aggregator automatically records this consumption into hourly and daily snapshots.</p> <p>Key characteristics:</p> <ul> <li>Automatic: Created by the Lambda aggregator without additional API calls</li> <li>Aggregated: Consumption is summed within time windows (hourly, daily)</li> <li>Retroactive: Data is available after the Lambda processes stream events</li> <li>Queryable: Access via API or CLI for reporting and analysis</li> </ul>"},{"location":"guide/usage-snapshots/#when-to-use-usage-snapshots","title":"When to Use Usage Snapshots","text":"Use Case Feature Billing reports Query consumption by entity/resource for a billing period Usage dashboards Display consumption trends over time Capacity planning Analyze usage patterns to inform limit adjustments Audit compliance Historical record of resource consumption <p>Snapshots vs Real-time Limits</p> <p>Use <code>get_usage_snapshots()</code> for historical analysis. For real-time limit status, use <code>get_status()</code> or check the <code>LimitStatus</code> returned by <code>acquire()</code>.</p>"},{"location":"guide/usage-snapshots/#querying-snapshots","title":"Querying Snapshots","text":""},{"location":"guide/usage-snapshots/#python-api","title":"Python API","text":"<p>Query snapshots for an entity:</p> <pre><code>from datetime import datetime\nfrom zae_limiter import RateLimiter\n\nlimiter = RateLimiter(name=\"my-app\", region=\"us-east-1\")\n\n# Get hourly snapshots for a user\nsnapshots, next_key = await limiter.get_usage_snapshots(\n    entity_id=\"user-123\",\n    resource=\"gpt-4\",\n    window_type=\"hourly\",\n    start_time=datetime(2024, 1, 1),\n    end_time=datetime(2024, 1, 31),\n)\n\nfor snap in snapshots:\n    print(f\"{snap.window_start}: {snap.counters}\")\n</code></pre> <p>Query snapshots across all entities for a resource:</p> <pre><code># Get all consumption for gpt-4 across all users\nsnapshots, _ = await limiter.get_usage_snapshots(\n    resource=\"gpt-4\",\n    window_type=\"daily\",\n)\n</code></pre>"},{"location":"guide/usage-snapshots/#pagination","title":"Pagination","text":"<p>For large result sets, use the <code>next_key</code> cursor:</p> <pre><code>all_snapshots = []\nnext_key = None\n\nwhile True:\n    snapshots, next_key = await limiter.get_usage_snapshots(\n        entity_id=\"user-123\",\n        next_key=next_key,\n    )\n    all_snapshots.extend(snapshots)\n    if next_key is None:\n        break\n</code></pre> <p>Pagination Behavior</p> <p>The <code>limit</code> parameter controls the DynamoDB query batch size, not the guaranteed result count. Client-side filters (<code>window_type</code>, <code>start_time</code>, <code>end_time</code>) are applied after fetching, so the returned count may be less than <code>limit</code>. Always use <code>next_key</code> to ensure you retrieve all matching results.</p>"},{"location":"guide/usage-snapshots/#usage-summary","title":"Usage Summary","text":"<p>For aggregated statistics, use <code>get_usage_summary()</code>:</p> <pre><code>summary = await limiter.get_usage_summary(\n    entity_id=\"user-123\",\n    resource=\"gpt-4\",\n    window_type=\"hourly\",\n    start_time=datetime(2024, 1, 1),\n    end_time=datetime(2024, 1, 31),\n)\n\nprint(f\"Total snapshots: {summary.snapshot_count}\")\nprint(f\"Total tokens: {summary.total['tpm']}\")\nprint(f\"Average per hour: {summary.average['tpm']}\")\nprint(f\"Time range: {summary.min_window_start} to {summary.max_window_start}\")\n</code></pre>"},{"location":"guide/usage-snapshots/#cli-commands","title":"CLI Commands","text":"<p>List snapshots:</p> <pre><code># Query by entity\nzae-limiter usage list --name my-app --entity-id user-123 --resource gpt-4\n\n# Query by resource across all entities\nzae-limiter usage list --name my-app --resource gpt-4 --window-type hourly\n\n# Filter by time range\nzae-limiter usage list --name my-app --entity-id user-123 \\\n    --start-time 2024-01-01T00:00:00Z --end-time 2024-01-31T23:59:59Z\n</code></pre> <p>Get summary:</p> <pre><code>zae-limiter usage summary --name my-app --entity-id user-123 --resource gpt-4\n</code></pre>"},{"location":"guide/usage-snapshots/#ascii-chart-visualization","title":"ASCII Chart Visualization","text":"<p>Display usage trends as ASCII charts with the <code>--plot</code> flag:</p> <pre><code># Install the plot extra\npip install 'zae-limiter[plot]'\n\n# Display usage as ASCII charts\nzae-limiter usage list --entity-id user-123 --plot\n\n# With resource filter\nzae-limiter usage list --entity-id user-123 --resource gpt-4 --plot\n</code></pre> <p>Example output:</p> <pre><code>Usage Plot: gpt-4 (hourly)\nEntity: user-123\n================================================================================\n\nRPM                             TPM\n----------------------------    -------------------------------\n51  \u2524        \u256d\u2500\u256e                5,141  \u2524        \u256d\u256e\n47  \u2524        \u2502 \u2570\u256e               4,719  \u2524        \u2502\u2570\u2500\u256e\n43  \u2524        \u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u256e         4,297  \u2524       \u256d\u256f  \u2570\u2500\u2500\u2500\u2500\u2500\u256e\n38  \u2524       \u256d\u256f        \u2570\u256e        3,875  \u2524       \u2502         \u2570\u256e\n34  \u2524       \u2502          \u2502        3,453  \u2524       \u2502          \u2502\n30  \u2524      \u256d\u256f          \u2570\u256e       3,031  \u2524      \u256d\u256f          \u2570\u256e\n26  \u2524     \u256d\u256f            \u2570\u2500\u2500     2,610  \u2524     \u256d\u256f            \u2570\u2500\u2500\n 5  \u253c\u2500\u2500\u2500\u2500\u2500\u256f                       500  \u253c\u2500\u2500\u2500\u2500\u2500\u256f\n\nTime range: 2024-01-15T00:00:00Z to 2024-01-15T23:00:00Z\nData points: 24\n\nTotal: 24 snapshots\n</code></pre> <p>Features:</p> <ul> <li>Side-by-side layout: Multiple counters displayed in pairs for compact viewing</li> <li>Context header: Shows resource, window type, and entity ID</li> <li>Auto-downsampling: Large datasets (&gt;60 points) are automatically averaged to fit terminal width</li> <li>Right-aligned Y-axis: Proper alignment for values with thousands separators</li> </ul> <p>This is useful for quickly spotting usage trends and patterns directly in the terminal.</p>"},{"location":"guide/usage-snapshots/#snapshot-data-model","title":"Snapshot Data Model","text":"<p>Each snapshot contains:</p> Field Description <code>entity_id</code> The entity that consumed tokens <code>resource</code> The resource being rate limited <code>window_start</code> Start of the time window (ISO timestamp) <code>window_end</code> End of the time window (ISO timestamp) <code>window_type</code> \"hourly\" or \"daily\" <code>counters</code> Dict of limit_name \u2192 total consumption <code>total_events</code> Number of rate limit events in the window <p>Example snapshot:</p> <pre><code>UsageSnapshot(\n    entity_id=\"user-123\",\n    resource=\"gpt-4\",\n    window_start=\"2024-01-15T10:00:00Z\",\n    window_end=\"2024-01-15T10:59:59.999999Z\",\n    window_type=\"hourly\",\n    counters={\"tpm\": 5000, \"rpm\": 25},\n    total_events=25,\n)\n</code></pre>"},{"location":"guide/usage-snapshots/#query-modes","title":"Query Modes","text":""},{"location":"guide/usage-snapshots/#entity-scoped-queries","title":"Entity-Scoped Queries","text":"<p>Provide <code>entity_id</code> to query snapshots for a specific entity:</p> <pre><code># Uses primary key (PK) - most efficient\nsnapshots, _ = await limiter.get_usage_snapshots(\n    entity_id=\"user-123\",\n    resource=\"gpt-4\",  # Optional: filter by resource\n)\n</code></pre>"},{"location":"guide/usage-snapshots/#resource-scoped-queries","title":"Resource-Scoped Queries","text":"<p>Provide only <code>resource</code> to query across all entities:</p> <pre><code># Uses GSI2 - aggregates across entities\nsnapshots, _ = await limiter.get_usage_snapshots(\n    resource=\"gpt-4\",\n)\n</code></pre> <p>This is useful for:</p> <ul> <li>Total consumption reports per resource</li> <li>Comparing usage across users/projects</li> <li>Capacity planning at the resource level</li> </ul>"},{"location":"guide/usage-snapshots/#relationship-to-other-features","title":"Relationship to Other Features","text":"flowchart LR     A[acquire] --&gt;|stream| B[DynamoDB Stream]     B --&gt;|triggers| C[Lambda Aggregator]     C --&gt;|creates| D[Usage Snapshots]     D --&gt;|query| E[get_usage_snapshots]     D --&gt;|aggregate| F[get_usage_summary] Feature Purpose Data <code>acquire()</code> Real-time rate limiting Current token state <code>get_status()</code> Check current limits Live bucket state <code>get_audit_events()</code> Security audit trail Who changed what <code>get_usage_snapshots()</code> Historical consumption Aggregated usage over time"},{"location":"guide/usage-snapshots/#best-practices","title":"Best Practices","text":"<ol> <li>Use window_type filtering: Query hourly vs daily based on your reporting needs</li> <li>Paginate large queries: Always handle <code>next_key</code> for complete results</li> <li>Use summary for totals: <code>get_usage_summary()</code> is more efficient than summing snapshots client-side</li> <li>Consider time ranges: Narrow your queries with <code>start_time</code>/<code>end_time</code> for better performance</li> </ol>"},{"location":"infra/auditing/","title":"Audit Logging","text":"<p>zae-limiter provides built-in audit logging for security-sensitive operations, enabling compliance tracking, troubleshooting, and incident investigation.</p>"},{"location":"infra/auditing/#overview","title":"Overview","text":"<p>The audit system automatically logs:</p> Action Trigger Details Captured <code>entity_created</code> Creating an entity name, parent_id, metadata <code>entity_deleted</code> Deleting an entity number of records deleted <code>limits_set</code> Configuring limits all limit configurations <code>limits_deleted</code> Removing limits resource name <p>Each audit event includes:</p> <ul> <li>Unique event ID - ULID (time-sortable, collision-free)</li> <li>Timestamp - ISO 8601 UTC</li> <li>Entity ID - The affected entity</li> <li>Principal - Who performed the action (optional)</li> <li>Details - Action-specific context</li> </ul>"},{"location":"infra/auditing/#audit-event-structure","title":"Audit Event Structure","text":"<pre><code>from zae_limiter import AuditEvent, AuditAction\n\n# Example audit event\nevent = AuditEvent(\n    event_id=\"01HQXYZ123ABC456DEF789GHI\",\n    timestamp=\"2024-01-15T10:30:00.000000+00:00\",\n    action=AuditAction.LIMITS_SET,\n    entity_id=\"api-key-123\",\n    principal=\"admin@example.com\",\n    resource=\"gpt-4\",\n    details={\n        \"limits\": [\n            {\"name\": \"rpm\", \"capacity\": 100, \"burst\": 150}\n        ]\n    }\n)\n</code></pre>"},{"location":"infra/auditing/#auditaction-constants","title":"AuditAction Constants","text":"Constant Value Description <code>ENTITY_CREATED</code> <code>\"entity_created\"</code> New entity was created <code>ENTITY_DELETED</code> <code>\"entity_deleted\"</code> Entity was deleted <code>LIMITS_SET</code> <code>\"limits_set\"</code> Limits were configured <code>LIMITS_DELETED</code> <code>\"limits_deleted\"</code> Limits were removed"},{"location":"infra/auditing/#principal-tracking","title":"Principal Tracking","text":"<p>Track who performed each action by passing the <code>principal</code> parameter to entity and limit management methods:</p> <pre><code>from zae_limiter import RateLimiter, Limit\n\nlimiter = RateLimiter(name=\"limiter\", region=\"us-east-1\")\n\n# Track who created the entity\nawait limiter.create_entity(\n    entity_id=\"api-key-123\",\n    name=\"Production Key\",\n    principal=\"admin@example.com\",  # Tracks the caller\n)\n\n# Track who configured limits\nawait limiter.set_limits(\n    entity_id=\"api-key-123\",\n    limits=[Limit.per_minute(\"rpm\", 100)],\n    principal=\"ops-team@example.com\",\n)\n\n# Track who deleted limits\nawait limiter.delete_limits(\n    entity_id=\"api-key-123\",\n    resource=\"gpt-4\",\n    principal=\"ops-team@example.com\",\n)\n\n# Track who deleted the entity\nawait limiter.delete_entity(\n    entity_id=\"api-key-123\",\n    principal=\"admin@example.com\",\n)\n</code></pre>"},{"location":"infra/auditing/#auto-detection-of-aws-caller-identity","title":"Auto-Detection of AWS Caller Identity","text":"<p>When <code>principal</code> is not provided, zae-limiter automatically detects the AWS caller identity (ARN) using STS <code>GetCallerIdentity</code>. This means audit events automatically capture who made changes without requiring explicit principal tracking:</p> <pre><code># No principal specified - AWS ARN is auto-detected\nawait limiter.create_entity(\n    entity_id=\"api-key-123\",\n    name=\"Production Key\",\n)\n# Audit event will have principal like:\n# \"arn:aws:iam::123456789012:user/admin\"\n# or \"arn:aws:sts::123456789012:assumed-role/MyRole/session\"\n</code></pre> <p>Best Practice</p> <p>For human-readable audit trails, explicitly pass a <code>principal</code> that identifies the user or service (e.g., email address or service name). Auto-detection is useful as a fallback when the caller identity is not available at the application level.</p> <p>Valid principal formats:</p> <ul> <li>Email: <code>user@example.com</code></li> <li>Service: <code>auth-service-v2</code></li> <li>Any identifier: alphanumeric start, then alphanumeric/underscore/hyphen/dot/colon/@</li> </ul>"},{"location":"infra/auditing/#querying-audit-events","title":"Querying Audit Events","text":"<p>Retrieve audit events for an entity using the <code>get_audit_events()</code> method:</p> <pre><code>from zae_limiter import RateLimiter\n\nlimiter = RateLimiter(name=\"limiter\", region=\"us-east-1\")\n\n# Get recent audit events (most recent first)\nevents = await limiter.get_audit_events(\n    entity_id=\"api-key-123\",\n    limit=100,\n)\n\nfor event in events:\n    print(f\"{event.timestamp}: {event.action} by {event.principal}\")\n</code></pre>"},{"location":"infra/auditing/#synchronous-api","title":"Synchronous API","text":"<p>For synchronous code, use <code>SyncRateLimiter</code>:</p> <pre><code>from zae_limiter import SyncRateLimiter\n\nlimiter = SyncRateLimiter(name=\"limiter\", region=\"us-east-1\")\n\nevents = limiter.get_audit_events(entity_id=\"api-key-123\", limit=100)\nfor event in events:\n    print(f\"{event.timestamp}: {event.action} by {event.principal}\")\n</code></pre>"},{"location":"infra/auditing/#pagination","title":"Pagination","text":"<p>Use <code>start_event_id</code> for pagination through large result sets:</p> <pre><code># First page\nevents = await limiter.get_audit_events(\n    entity_id=\"api-key-123\",\n    limit=50,\n)\n\n# Next page (use last event's ID)\nif events:\n    more_events = await limiter.get_audit_events(\n        entity_id=\"api-key-123\",\n        limit=50,\n        start_event_id=events[-1].event_id,\n    )\n</code></pre>"},{"location":"infra/auditing/#cli-access","title":"CLI Access","text":"<p>Query audit events from the command line:</p> <pre><code># List audit events for an entity\nzae-limiter audit list --name limiter --entity-id api-key-123\n\n# Limit results\nzae-limiter audit list --entity-id api-key-123 --limit 10\n\n# Paginate\nzae-limiter audit list --entity-id api-key-123 --start-event-id 01HXYZ...\n</code></pre>"},{"location":"infra/auditing/#retention-and-ttl","title":"Retention and TTL","text":"<p>Audit events auto-expire after 90 days by default. This is configurable via the <code>ttl_seconds</code> parameter when logging events.</p> <p>DynamoDB TTL handles deletion automatically:</p> <ul> <li>Events are marked with an expiration timestamp</li> <li>DynamoDB deletes expired items within 48 hours of TTL</li> <li>Expired events are automatically archived to S3 (when enabled)</li> </ul>"},{"location":"infra/auditing/#s3-archival","title":"S3 Archival","text":"<p>When audit archival is enabled (default), expired audit events are automatically archived to S3 before being deleted from DynamoDB. This enables long-term retention for compliance requirements.</p>"},{"location":"infra/auditing/#how-it-works","title":"How It Works","text":"<ol> <li>TTL Expiration: DynamoDB marks audit events for deletion after 90 days</li> <li>Stream Trigger: The Lambda aggregator receives TTL deletion events via DynamoDB Streams</li> <li>Archive to S3: Events are written to S3 in compressed JSONL format</li> <li>Glacier Transition: After configurable days (default: 90), archives transition to Glacier Instant Retrieval</li> </ol>"},{"location":"infra/auditing/#configuration","title":"Configuration","text":""},{"location":"infra/auditing/#cli-deployment","title":"CLI Deployment","text":"<pre><code># Default: archival enabled, 90-day Glacier transition\nzae-limiter deploy --name limiter --region us-east-1\n\n# Custom Glacier transition period\nzae-limiter deploy --name limiter --audit-archive-glacier-days 180\n\n# Disable archival entirely\nzae-limiter deploy --name limiter --no-audit-archival\n</code></pre>"},{"location":"infra/auditing/#programmatic-deployment","title":"Programmatic Deployment","text":"<pre><code>from zae_limiter import RateLimiter, StackOptions\n\n# With custom archival settings\nlimiter = RateLimiter(\n    name=\"limiter\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(\n        enable_audit_archival=True,  # Default\n        audit_archive_glacier_days=180,  # Custom transition period\n    ),\n)\n</code></pre>"},{"location":"infra/auditing/#s3-bucket-structure","title":"S3 Bucket Structure","text":"<p>Archives are stored in an S3 bucket with a name auto-generated by CloudFormation, with date-based partitioning for efficient querying. The bucket name is available in the CloudFormation stack outputs (<code>AuditArchiveBucketName</code>).</p> <pre><code>s3://&lt;auto-generated-bucket-name&gt;/\n  audit/\n    year=YYYY/\n      month=MM/\n        day=DD/\n          audit-{request_id}-{timestamp}.jsonl.gz\n</code></pre> <p>Each file contains newline-delimited JSON records (gzip compressed):</p> <pre><code>{\"event_id\": \"01HQXYZ...\", \"action\": \"limits_set\", \"entity_id\": \"api-key-123\", ...}\n{\"event_id\": \"01HQXYZ...\", \"action\": \"entity_created\", \"entity_id\": \"api-key-456\", ...}\n</code></pre>"},{"location":"infra/auditing/#s3-bucket-security","title":"S3 Bucket Security","text":"<p>The archive bucket is created with security best practices:</p> <ul> <li>Server-side encryption: AES256 (S3-managed keys)</li> <li>Public access blocked: All public access settings disabled</li> <li>Lifecycle policy: Automatic transition to Glacier Instant Retrieval</li> </ul>"},{"location":"infra/auditing/#querying-archived-events","title":"Querying Archived Events","text":"<p>Use Amazon Athena to query archived audit events:</p> <pre><code>-- Create external table (one-time setup)\nCREATE EXTERNAL TABLE audit_archive (\n    event_id STRING,\n    timestamp STRING,\n    action STRING,\n    entity_id STRING,\n    principal STRING,\n    resource STRING,\n    details STRING\n)\nPARTITIONED BY (year STRING, month STRING, day STRING)\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\nLOCATION 's3://&lt;audit-archive-bucket-name&gt;/audit/'\nTBLPROPERTIES ('compressionType'='gzip');\n\n-- Load partitions\nMSCK REPAIR TABLE audit_archive;\n\n-- Query audit events\nSELECT * FROM audit_archive\nWHERE year = '2024' AND month = '01'\nAND action = 'limits_set';\n</code></pre>"},{"location":"infra/auditing/#cloudformation-resources","title":"CloudFormation Resources","text":"<p>When audit archival is enabled, the following resources are created:</p> Resource Name Pattern Purpose S3 Bucket <code>{stack-name}-audit-archive</code> Archive storage IAM Policy (inline) Lambda S3:PutObject permission"},{"location":"infra/auditing/#cloudformation-outputs","title":"CloudFormation Outputs","text":"Output Description <code>AuditArchiveBucketName</code> S3 bucket name for archives <code>AuditArchiveBucketArn</code> S3 bucket ARN"},{"location":"infra/auditing/#dynamodb-access-patterns","title":"DynamoDB Access Patterns","text":"<p>Audit events are stored in the same DynamoDB table with the following schema:</p> Key Format Description PK <code>AUDIT#{entity_id}</code> Groups events by entity SK <code>#AUDIT#{event_id}</code> Sorts by event ID (chronological)"},{"location":"infra/auditing/#direct-dynamodb-queries","title":"Direct DynamoDB Queries","text":"<p>For advanced use cases, query audit events directly:</p> <pre><code>import boto3\n\ndynamodb = boto3.resource(\"dynamodb\")\ntable = dynamodb.Table(\"limiter\")\n\n# Query all audit events for an entity\nresponse = table.query(\n    KeyConditionExpression=\"PK = :pk AND begins_with(SK, :sk)\",\n    ExpressionAttributeValues={\n        \":pk\": \"AUDIT#api-key-123\",\n        \":sk\": \"#AUDIT#\",\n    },\n    ScanIndexForward=False,  # Most recent first\n    Limit=100,\n)\n\nfor item in response[\"Items\"]:\n    data = item[\"data\"]\n    print(f\"{data['timestamp']}: {data['action']}\")\n</code></pre>"},{"location":"infra/auditing/#use-cases","title":"Use Cases","text":""},{"location":"infra/auditing/#compliance-auditing","title":"Compliance Auditing","text":"<p>Answer \"who changed what, when?\" for SOC2, HIPAA, or internal audits:</p> <pre><code>from zae_limiter import RateLimiter\n\nlimiter = RateLimiter(name=\"limiter\", region=\"us-east-1\")\n\n# Find all changes to a specific entity\nevents = await limiter.get_audit_events(entity_id=\"sensitive-api-key\")\n\nfor event in events:\n    print(f\"\"\"\n    Time: {event.timestamp}\n    Action: {event.action}\n    By: {event.principal or 'unknown'}\n    Details: {event.details}\n    \"\"\")\n</code></pre>"},{"location":"infra/auditing/#troubleshooting","title":"Troubleshooting","text":"<p>Investigate when limits were changed:</p> <pre><code>from zae_limiter import RateLimiter, AuditAction\n\nlimiter = RateLimiter(name=\"limiter\", region=\"us-east-1\")\n\n# Filter for limit changes\nevents = await limiter.get_audit_events(entity_id=\"api-key-123\")\nlimit_changes = [e for e in events if e.action in (\n    AuditAction.LIMITS_SET,\n    AuditAction.LIMITS_DELETED,\n)]\n\nfor event in limit_changes:\n    print(f\"{event.timestamp}: {event.action}\")\n    if event.details.get(\"limits\"):\n        for limit in event.details[\"limits\"]:\n            print(f\"  - {limit['name']}: {limit['capacity']}\")\n</code></pre>"},{"location":"infra/auditing/#security-incident-response","title":"Security Incident Response","text":"<p>Track entity deletions during an incident window:</p> <pre><code>from zae_limiter import RateLimiter, AuditAction\n\nlimiter = RateLimiter(name=\"limiter\", region=\"us-east-1\")\n\nevents = await limiter.get_audit_events(entity_id=\"compromised-key\")\ndeletions = [\n    e for e in events\n    if e.action == AuditAction.ENTITY_DELETED\n]\n\nfor event in deletions:\n    print(f\"Deleted at {event.timestamp} by {event.principal}\")\n</code></pre>"},{"location":"infra/auditing/#next-steps","title":"Next Steps","text":"<ul> <li>Production Deployment - Security best practices</li> <li>Monitoring - Observability and alerting</li> <li>API Reference - AuditEvent and AuditAction details</li> </ul>"},{"location":"infra/cloudformation/","title":"CloudFormation Template","text":"<p>This guide covers the CloudFormation template used by zae-limiter and how to customize it.</p>"},{"location":"infra/cloudformation/#template-overview","title":"Template Overview","text":"<p>The template creates:</p> flowchart TB     subgraph stack[CloudFormation Stack]         subgraph data[Data Layer]             table[(DynamoDB Table)]             stream([DynamoDB Stream])             s3[(S3 Bucket&lt;br/&gt;audit archive)]         end          subgraph compute[Compute Layer]             role[IAM Role]             lambda[[Lambda Aggregator]]         end          subgraph iam[Application IAM Roles]             appRole[AppRole]             adminRole[AdminRole]             readonlyRole[ReadOnlyRole]         end          subgraph observe[Observability]             logs[(CloudWatch Logs)]             alarms([CloudWatch Alarms])             dlq([Dead Letter Queue])         end     end      table --&gt; stream     stream --&gt; lambda     lambda --&gt; table     lambda --&gt; s3     role --&gt; lambda     lambda --&gt; logs     lambda -.-&gt;|on failure| dlq     alarms -.-&gt; lambda     alarms -.-&gt; dlq      appRole -.-&gt; table     adminRole -.-&gt; table     readonlyRole -.-&gt; table      click table \"#dynamodb-table\"     click stream \"#stream-configuration\"     click s3 \"#s3-audit-archive-bucket\"     click lambda \"#lambda-aggregator\"     click role \"#iam-permissions\"     click appRole \"#application-iam-roles\"     click adminRole \"#application-iam-roles\"     click readonlyRole \"#application-iam-roles\"     click dlq \"#add-dead-letter-queue\"     click alarms \"#add-cloudwatch-alarms\""},{"location":"infra/cloudformation/#export-template","title":"Export Template","text":"<pre><code># Export to file\nzae-limiter cfn-template &gt; template.yaml\n\n# View template\nzae-limiter cfn-template | less\n</code></pre>"},{"location":"infra/cloudformation/#template-parameters","title":"Template Parameters","text":"<p>The DynamoDB table name is automatically derived from the CloudFormation stack name using the <code>AWS::StackName</code> pseudo-parameter. This ensures consistency between stack and resource names.</p> Parameter Type Default Description <code>SnapshotWindows</code> String <code>hourly,daily</code> Comma-separated list of snapshot windows <code>SnapshotRetentionDays</code> Number <code>90</code> Days to retain usage snapshots (1-3650) <code>LambdaMemorySize</code> Number <code>256</code> Memory for aggregator Lambda (128-3008 MB) <code>LambdaTimeout</code> Number <code>60</code> Timeout for aggregator Lambda (1-900 seconds) <code>LambdaDurationThreshold</code> Number <code>54000</code> Duration alarm threshold in ms (90% of timeout) <code>EnableAggregator</code> String <code>true</code> Whether to deploy the aggregator Lambda <code>SchemaVersion</code> String <code>1.0.0</code> Schema version for infrastructure <code>PITRRecoveryPeriodDays</code> String (empty) PITR period (1-35 days, empty for AWS default) <code>EnableAlarms</code> String <code>true</code> Whether to deploy CloudWatch alarms <code>AlarmSNSTopicArn</code> String (empty) SNS topic ARN for alarm notifications <code>LogRetentionDays</code> Number <code>30</code> CloudWatch log retention (standard periods) <code>PermissionBoundary</code> String (empty) IAM permission boundary (ARN or policy name) <code>RoleName</code> String (empty) Custom IAM role name (use <code>{}</code> as placeholder for base name) <code>EnableAuditArchival</code> String <code>true</code> Archive expired audit events to S3 <code>AuditArchiveGlacierDays</code> Number <code>90</code> Days before Glacier IR transition (1-3650) <code>EnableTracing</code> String <code>false</code> Enable AWS X-Ray tracing for Lambda <code>EnableIAMRoles</code> String <code>true</code> Create App/Admin/ReadOnly IAM roles"},{"location":"infra/cloudformation/#dynamodb-table","title":"DynamoDB Table","text":""},{"location":"infra/cloudformation/#schema","title":"Schema","text":"<pre><code>AttributeDefinitions:\n  - AttributeName: PK\n    AttributeType: S\n  - AttributeName: SK\n    AttributeType: S\n  - AttributeName: GSI1PK\n    AttributeType: S\n  - AttributeName: GSI1SK\n    AttributeType: S\n  - AttributeName: GSI2PK\n    AttributeType: S\n  - AttributeName: GSI2SK\n    AttributeType: S\n\nKeySchema:\n  - AttributeName: PK\n    KeyType: HASH\n  - AttributeName: SK\n    KeyType: RANGE\n</code></pre>"},{"location":"infra/cloudformation/#global-secondary-indexes","title":"Global Secondary Indexes","text":"<p>GSI1 - Parent to children lookups:</p> <pre><code>GlobalSecondaryIndexes:\n  - IndexName: GSI1\n    KeySchema:\n      - AttributeName: GSI1PK  # PARENT#{parent_id}\n        KeyType: HASH\n      - AttributeName: GSI1SK  # CHILD#{child_id}\n        KeyType: RANGE\n</code></pre> <p>GSI2 - Resource aggregation:</p> <pre><code>  - IndexName: GSI2\n    KeySchema:\n      - AttributeName: GSI2PK  # RESOURCE#{resource}\n        KeyType: HASH\n      - AttributeName: GSI2SK  # BUCKET#{entity_id}#{limit_name}\n        KeyType: RANGE\n</code></pre>"},{"location":"infra/cloudformation/#stream-configuration","title":"Stream Configuration","text":"<pre><code>StreamSpecification:\n  StreamViewType: NEW_AND_OLD_IMAGES\n</code></pre>"},{"location":"infra/cloudformation/#s3-audit-archive-bucket","title":"S3 Audit Archive Bucket","text":"<p>When <code>EnableAuditArchival</code> is <code>true</code>, the template creates an S3 bucket to store expired audit events.</p> <pre><code>AuditArchiveBucket:\n  Type: AWS::S3::Bucket\n  Condition: DeployAuditArchive\n  Properties:\n    # BucketName omitted: CloudFormation auto-generates a globally unique name\n    BucketEncryption:\n      ServerSideEncryptionConfiguration:\n        - ServerSideEncryptionByDefault:\n            SSEAlgorithm: AES256\n    PublicAccessBlockConfiguration:\n      BlockPublicAcls: true\n      BlockPublicPolicy: true\n      IgnorePublicAcls: true\n      RestrictPublicBuckets: true\n    LifecycleConfiguration:\n      Rules:\n        - Id: GlacierTransition\n          Status: Enabled\n          Prefix: audit/\n          Transitions:\n            - StorageClass: GLACIER_IR\n              TransitionInDays: !Ref AuditArchiveGlacierDays\n</code></pre>"},{"location":"infra/cloudformation/#object-structure","title":"Object Structure","text":"<p>Archived audit events are stored as gzip-compressed JSONL files:</p> <pre><code>s3://&lt;auto-generated-bucket-name&gt;/\n  audit/\n    year=2024/\n      month=01/\n        day=15/\n          audit-{request_id}-{timestamp}.jsonl.gz\n</code></pre>"},{"location":"infra/cloudformation/#lifecycle-policy","title":"Lifecycle Policy","text":"<p>Objects transition to Glacier Instant Retrieval after <code>AuditArchiveGlacierDays</code> (default: 90 days) for cost-effective long-term storage while maintaining millisecond retrieval times.</p>"},{"location":"infra/cloudformation/#lambda-aggregator","title":"Lambda Aggregator","text":"<p>The aggregator Lambda processes DynamoDB Stream events to maintain usage snapshots and archive expired audit events.</p> <p>Performance Tuning</p> <p>For guidance on memory tuning, concurrency management, and error handling configuration, see the Performance Tuning Guide.</p>"},{"location":"infra/cloudformation/#function-configuration","title":"Function Configuration","text":"<pre><code>AggregatorFunction:\n  Type: AWS::Lambda::Function\n  Properties:\n    Runtime: python3.12\n    Handler: zae_limiter.aggregator.handler.lambda_handler\n    MemorySize: 256\n    Timeout: 60\n    Environment:\n      Variables:\n        TABLE_NAME: !Ref AWS::StackName\n        SNAPSHOT_WINDOWS: !Ref SnapshotWindows\n        SNAPSHOT_TTL_DAYS: !Ref SnapshotRetentionDays\n        # When audit archival is enabled:\n        ENABLE_ARCHIVAL: \"true\"\n        ARCHIVE_BUCKET_NAME: !Ref AuditArchiveBucket\n</code></pre>"},{"location":"infra/cloudformation/#event-source-mapping","title":"Event Source Mapping","text":"<pre><code>StreamEventMapping:\n  Type: AWS::Lambda::EventSourceMapping\n  Properties:\n    EventSourceArn: !GetAtt Table.StreamArn\n    FunctionName: !Ref AggregatorFunction\n    StartingPosition: LATEST\n    BatchSize: 100\n    MaximumBatchingWindowInSeconds: 5\n</code></pre>"},{"location":"infra/cloudformation/#iam-permissions","title":"IAM Permissions","text":""},{"location":"infra/cloudformation/#lambda-execution-role","title":"Lambda Execution Role","text":"<pre><code>AggregatorRole:\n  Type: AWS::IAM::Role\n  Properties:\n    AssumeRolePolicyDocument:\n      Statement:\n        - Effect: Allow\n          Principal:\n            Service: lambda.amazonaws.com\n          Action: sts:AssumeRole\n    Policies:\n      - PolicyName: DynamoDBAccess\n        PolicyDocument:\n          Statement:\n            - Effect: Allow\n              Action:\n                - dynamodb:GetItem\n                - dynamodb:PutItem\n                - dynamodb:UpdateItem\n                - dynamodb:Query\n              Resource: !GetAtt Table.Arn\n            - Effect: Allow\n              Action:\n                - dynamodb:GetRecords\n                - dynamodb:GetShardIterator\n                - dynamodb:DescribeStream\n                - dynamodb:ListStreams\n              Resource: !Sub \"${Table.Arn}/stream/*\"\n            # When audit archival is enabled:\n            - Effect: Allow\n              Action:\n                - s3:PutObject\n              Resource: !Sub \"${AuditArchiveBucket.Arn}/*\"\n</code></pre>"},{"location":"infra/cloudformation/#application-iam-roles","title":"Application IAM Roles","text":"<p>When <code>EnableIAMRoles</code> is <code>true</code> (default), the template creates three IAM roles for different access patterns. These roles allow applications, administrators, and monitoring systems to assume least-privilege access to the DynamoDB table.</p>"},{"location":"infra/cloudformation/#role-summary","title":"Role Summary","text":"Role Use Case DynamoDB Permissions <code>AppRole</code> Applications calling <code>acquire()</code> GetItem, Query, TransactWriteItems <code>AdminRole</code> Ops teams managing config App + PutItem, DeleteItem, UpdateItem, BatchWriteItem <code>ReadOnlyRole</code> Monitoring and dashboards GetItem, Query, Scan, DescribeTable"},{"location":"infra/cloudformation/#trust-policy","title":"Trust Policy","text":"<p>All roles trust the same AWS account root principal, allowing any IAM entity in the account to assume the role (subject to their own permissions):</p> <pre><code>AssumeRolePolicyDocument:\n  Version: '2012-10-17'\n  Statement:\n    - Effect: Allow\n      Principal:\n        AWS: !Sub arn:${AWS::Partition}:iam::${AWS::AccountId}:root\n      Action: sts:AssumeRole\n</code></pre>"},{"location":"infra/cloudformation/#role-naming","title":"Role Naming","text":"<ul> <li>Default: <code>${StackName}-{app,admin,readonly}-role</code></li> <li>With custom <code>RoleName</code>: <code>${RoleName}-{app,admin,readonly}</code></li> </ul> <p>Roles respect <code>PermissionBoundary</code> if configured.</p>"},{"location":"infra/cloudformation/#usage-example","title":"Usage Example","text":"<pre><code>import boto3\n\n# Assume the AppRole for rate limiting operations\nsts = boto3.client('sts')\ncredentials = sts.assume_role(\n    RoleArn='arn:aws:iam::123456789012:role/my-app-app-role',\n    RoleSessionName='my-app'\n)['Credentials']\n\n# Use assumed credentials\nsession = boto3.Session(\n    aws_access_key_id=credentials['AccessKeyId'],\n    aws_secret_access_key=credentials['SecretAccessKey'],\n    aws_session_token=credentials['SessionToken']\n)\n</code></pre>"},{"location":"infra/cloudformation/#customization","title":"Customization","text":""},{"location":"infra/cloudformation/#add-dead-letter-queue","title":"Add Dead Letter Queue","text":"<pre><code>Parameters:\n  EnableDLQ:\n    Type: String\n    Default: \"false\"\n    AllowedValues: [\"true\", \"false\"]\n\nConditions:\n  CreateDLQ: !Equals [!Ref EnableDLQ, \"true\"]\n\nResources:\n  DeadLetterQueue:\n    Type: AWS::SQS::Queue\n    Condition: CreateDLQ\n    Properties:\n      QueueName: !Sub \"${AWS::StackName}-aggregator-dlq\"\n      MessageRetentionPeriod: 1209600  # 14 days\n\n  StreamEventMapping:\n    Properties:\n      DestinationConfig:\n        OnFailure:\n          Destination: !If\n            - CreateDLQ\n            - !GetAtt DeadLetterQueue.Arn\n            - !Ref AWS::NoValue\n</code></pre>"},{"location":"infra/cloudformation/#add-cloudwatch-alarms","title":"Add CloudWatch Alarms","text":"<pre><code>ReadThrottleAlarm:\n  Type: AWS::CloudWatch::Alarm\n  Properties:\n    AlarmName: !Sub \"${AWS::StackName}-read-throttle\"\n    AlarmDescription: Alert when DynamoDB read requests are throttled\n    MetricName: ReadThrottleEvents\n    Namespace: AWS/DynamoDB\n    Statistic: Sum\n    Period: 300  # 5 minutes\n    EvaluationPeriods: 2\n    Threshold: 1\n    ComparisonOperator: GreaterThanThreshold\n    Dimensions:\n      - Name: TableName\n        Value: !Ref RateLimitsTable\n    TreatMissingData: notBreaching\n    AlarmActions: !If\n      - HasSNSTopic\n      - [!Ref AlarmSNSTopicArn]\n      - !Ref AWS::NoValue\n</code></pre>"},{"location":"infra/cloudformation/#enable-encryption-with-cmk","title":"Enable Encryption with CMK","text":"<pre><code>Parameters:\n  KmsKeyArn:\n    Type: String\n    Default: \"\"\n\nConditions:\n  UseCustomKey: !Not [!Equals [!Ref KmsKeyArn, \"\"]]\n\nResources:\n  Table:\n    Properties:\n      SSESpecification:\n        SSEEnabled: true\n        SSEType: !If [UseCustomKey, \"KMS\", \"AWS_OWNED_KEY\"]\n        KMSMasterKeyId: !If [UseCustomKey, !Ref KmsKeyArn, !Ref AWS::NoValue]\n</code></pre>"},{"location":"infra/cloudformation/#deployment-examples","title":"Deployment Examples","text":""},{"location":"infra/cloudformation/#basic-deployment","title":"Basic Deployment","text":"<pre><code>aws cloudformation deploy \\\n    --template-file template.yaml \\\n    --stack-name zae-limiter \\\n    --capabilities CAPABILITY_NAMED_IAM\n</code></pre>"},{"location":"infra/cloudformation/#with-custom-parameters","title":"With Custom Parameters","text":"<pre><code>aws cloudformation deploy \\\n    --template-file template.yaml \\\n    --stack-name prod \\\n    --parameter-overrides \\\n        PITRRecoveryPeriodDays=35 \\\n        SnapshotRetentionDays=365 \\\n        LogRetentionDays=90 \\\n        EnableAlarms=true \\\n    --capabilities CAPABILITY_NAMED_IAM\n</code></pre> <p>Note: The DynamoDB table name is automatically set to match the stack name (e.g., <code>prod</code>).</p>"},{"location":"infra/cloudformation/#using-sam","title":"Using SAM","text":"<pre><code># samconfig.toml\n[default.deploy.parameters]\nstack_name = \"limiter\"\ncapabilities = \"CAPABILITY_NAMED_IAM\"\n</code></pre> <pre><code>sam deploy --guided\n</code></pre>"},{"location":"infra/cloudformation/#outputs","title":"Outputs","text":"<p>The template exports:</p> Output Description <code>TableArn</code> DynamoDB table ARN <code>StreamArn</code> DynamoDB stream ARN <code>FunctionArn</code> Lambda function ARN <code>AuditArchiveBucketName</code> S3 bucket for audit archives (when enabled) <code>AuditArchiveBucketArn</code> S3 bucket ARN (when enabled) <code>AppRoleArn</code> IAM role ARN for applications (when IAM roles enabled) <code>AppRoleName</code> IAM role name for applications <code>AdminRoleArn</code> IAM role ARN for administrators <code>AdminRoleName</code> IAM role name for administrators <code>ReadOnlyRoleArn</code> IAM role ARN for read-only access <code>ReadOnlyRoleName</code> IAM role name for read-only access <p>Access outputs:</p> <pre><code>aws cloudformation describe-stacks \\\n    --stack-name zae-limiter \\\n    --query \"Stacks[0].Outputs\"\n</code></pre>"},{"location":"infra/cloudformation/#next-steps","title":"Next Steps","text":"<ul> <li>Deployment - Deployment guide</li> <li>LocalStack - Local development</li> </ul>"},{"location":"infra/deployment/","title":"Deployment","text":"<p>This guide covers deploying zae-limiter infrastructure to AWS.</p>"},{"location":"infra/deployment/#overview","title":"Overview","text":"<p>zae-limiter uses CloudFormation to deploy:</p> <ul> <li>DynamoDB Table - Stores rate limit state, entities, and usage data</li> <li>DynamoDB Streams - Captures changes for usage aggregation</li> <li>Lambda Function - Aggregates usage into hourly/daily snapshots and archives audit events</li> <li>S3 Bucket - Archives expired audit events (when audit archival is enabled)</li> <li>IAM Roles - Least-privilege access for Lambda and application access (App/Admin/ReadOnly)</li> <li>CloudWatch Logs - Lambda function logs</li> </ul>"},{"location":"infra/deployment/#cli-deployment-recommended","title":"CLI Deployment (Recommended)","text":"<p>The simplest way to deploy:</p> <pre><code># Deploy with defaults\nzae-limiter deploy --name limiter --region us-east-1\n\n# Deploy with custom settings\nzae-limiter deploy \\\n    --name limiter \\\n    --region us-east-1 \\\n    --log-retention-days 90 \\\n    --pitr-recovery-days 7\n</code></pre>"},{"location":"infra/deployment/#cli-options","title":"CLI Options","text":"Option Description Default <code>--name</code> Resource identifier <code>limiter</code> <code>--region</code> AWS region boto3 default <code>--endpoint-url</code> Custom endpoint (LocalStack) None <code>--enable-aggregator/--no-aggregator</code> Deploy Lambda aggregator <code>true</code> <code>--log-retention-days</code> CloudWatch log retention <code>30</code> <code>--pitr-recovery-days</code> Point-in-time recovery (1-35 days) None (disabled) <code>--enable-audit-archival/--no-audit-archival</code> Archive expired audit events to S3 <code>true</code> <code>--audit-archive-glacier-days</code> Days before Glacier IR transition <code>90</code> <code>--enable-iam-roles/--no-iam-roles</code> Create App/Admin/ReadOnly IAM roles <code>true</code> <p>For the full list of options, see the CLI Reference.</p>"},{"location":"infra/deployment/#check-stack-status","title":"Check Stack Status","text":"CLIProgrammatic <pre><code>zae-limiter status --name limiter --region us-east-1\n</code></pre> <pre><code>from zae_limiter import RateLimiter\n\nlimiter = RateLimiter(name=\"limiter\", region=\"us-east-1\")\n\nstatus = await limiter.get_status()  # Returns Status dataclass\n\nif not status.available:\n    print(\"DynamoDB not reachable\")\nelif status.stack_status == \"CREATE_COMPLETE\":\n    print(\"Stack is ready\")\n    print(f\"Schema version: {status.schema_version}\")\n    print(f\"Table items: {status.table_item_count}\")\nelif status.stack_status and \"IN_PROGRESS\" in status.stack_status:\n    print(f\"Operation in progress: {status.stack_status}\")\n</code></pre>"},{"location":"infra/deployment/#discover-deployed-instances","title":"Discover Deployed Instances","text":"<p>List all zae-limiter stacks in a region:</p> CLIProgrammatic <pre><code>zae-limiter list --region us-east-1\n</code></pre> <p>Output: <pre><code>Rate Limiter Instances (us-east-1)\n===========================================================================================\n\nName                 Status                    Version      Lambda       Schema     Created\n-------------------------------------------------------------------------------------------\nprod-api             \u2713 CREATE_COMPLETE         0.2.0        0.2.0        1.0.0      2024-01-15\nstaging              \u2713 CREATE_COMPLETE         0.2.0        0.2.0        1.0.0      2024-01-10\ndev-test             \u23f3 UPDATE_IN_PROGRESS     0.1.0        0.1.0        1.0.0      2023-12-01\n\nTotal: 3 instance(s)\n\u26a0\ufe0f  1 instance(s) need attention\n</code></pre></p> <pre><code>from zae_limiter import RateLimiter, LimiterInfo\n\n# List all deployed limiters in a region\nlimiters = await RateLimiter.list_deployed(region=\"us-east-1\")\n\nfor limiter in limiters:\n    if limiter.is_healthy:\n        print(f\"\u2713 {limiter.user_name}: {limiter.version}\")\n    elif limiter.is_failed:\n        print(f\"\u2717 {limiter.user_name}: {limiter.stack_status}\")\n    elif limiter.is_in_progress:\n        print(f\"\u23f3 {limiter.user_name}: {limiter.stack_status}\")\n</code></pre> <p>The <code>LimiterInfo</code> object provides:</p> <ul> <li><code>stack_name</code> / <code>user_name</code> - Full and user-friendly names</li> <li><code>stack_status</code> - CloudFormation status</li> <li><code>version</code> / <code>lambda_version</code> / <code>schema_version</code> - Version info from tags</li> <li><code>is_healthy</code> / <code>is_in_progress</code> / <code>is_failed</code> - Status helpers</li> </ul>"},{"location":"infra/deployment/#delete-stack","title":"Delete Stack","text":"<pre><code>zae-limiter delete --name limiter --region us-east-1 --yes\n</code></pre>"},{"location":"infra/deployment/#admin-vs-application-workflow","title":"Admin vs Application Workflow","text":"<p>zae-limiter supports separation of concerns between infrastructure administrators and application developers.</p>"},{"location":"infra/deployment/#admin-workflow-infrastructure-config","title":"Admin Workflow (Infrastructure &amp; Config)","text":"<p>Admins deploy infrastructure and configure rate limits centrally:</p> <pre><code># 1. Deploy infrastructure\nzae-limiter deploy --name prod --region us-east-1\n\n# 2. Configure system-wide defaults\nzae-limiter system set-defaults -l rpm:100 -l tpm:10000 --on-unavailable block\n\n# 3. Configure resource-specific limits\nzae-limiter resource set-defaults gpt-4 -l rpm:50 -l tpm:100000\nzae-limiter resource set-defaults gpt-3.5-turbo -l rpm:200 -l tpm:500000\n\n# 4. Configure premium user tiers\nzae-limiter entity set-limits user-premium --resource gpt-4 -l rpm:500 -l tpm:500000\n</code></pre>"},{"location":"infra/deployment/#application-workflow-connect-only","title":"Application Workflow (Connect Only)","text":"<p>Application code connects to existing infrastructure without managing it:</p> <pre><code>from zae_limiter import RateLimiter\n\n# Connect only - no stack_options means no infrastructure changes\nlimiter = RateLimiter(\n    name=\"prod\",\n    region=\"us-east-1\",\n    # No stack_options = connect only, no create/update\n)\n\n# Limits are automatically resolved from stored config\nasync with limiter.acquire(\n    entity_id=\"user-123\",\n    resource=\"gpt-4\",\n    limits=None,  # Auto-resolves: Entity &gt; Resource &gt; System\n    consume={\"rpm\": 1},\n) as lease:\n    await call_api()\n</code></pre>"},{"location":"infra/deployment/#benefits","title":"Benefits","text":"Concern Admin Application Infrastructure \u2713 Deploy, update, delete stacks Connect only Rate limits \u2713 Configure at all levels Auto-resolved Credentials Full AWS access (or AdminRole) DynamoDB read/write only (or AppRole) Changes Through CLI/IaC None <p>This separation allows:</p> <ul> <li>Centralized control - Admins manage limits without code changes</li> <li>Simplified apps - No hardcoded limits, automatic resolution</li> <li>Audit trail - All config changes logged to DynamoDB</li> <li>Dynamic updates - Change limits without redeploying apps</li> </ul> <p>See Configuration Hierarchy for limit resolution details.</p>"},{"location":"infra/deployment/#stack-lifecycle-management","title":"Stack Lifecycle Management","text":""},{"location":"infra/deployment/#programmatic-cleanup","title":"Programmatic Cleanup","text":"<p>In addition to the CLI, you can manage stack lifecycle programmatically using the <code>delete_stack()</code> method:</p> <pre><code>from zae_limiter import RateLimiter, StackOptions\n\n# Create stack\nlimiter = RateLimiter(\n    name=\"limiter\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(),\n)\n\n# Use the limiter...\nasync with limiter:\n    # Rate limiting operations here\n    pass\n\n# Delete stack when done\nawait limiter.delete_stack()\n</code></pre>"},{"location":"infra/deployment/#use-case-guidance","title":"Use-Case Guidance","text":""},{"location":"infra/deployment/#development-and-prototyping","title":"Development and Prototyping","text":"<p>For rapid iteration, declare infrastructure with cleanup:</p> <pre><code>async def dev_session():\n    limiter = RateLimiter(\n        name=\"dev\",\n        region=\"us-east-1\",\n        stack_options=StackOptions(enable_aggregator=False),\n    )\n\n    try:\n        async with limiter:\n            # Development work...\n            pass\n    finally:\n        # Clean up development stack\n        await limiter.delete_stack()\n</code></pre>"},{"location":"infra/deployment/#production","title":"Production","text":"<p>For production deployments, see the Production Guide covering:</p> <ul> <li>Production checklist (PITR, alarms, SNS)</li> <li>Security best practices</li> <li>Multi-region considerations</li> <li>Cost estimation</li> </ul>"},{"location":"infra/deployment/#cloudformation-template","title":"CloudFormation Template","text":"<p>Export and customize the template:</p> <pre><code># Export template\nzae-limiter cfn-template &gt; template.yaml\n\n# Deploy with AWS CLI\naws cloudformation deploy \\\n    --template-file template.yaml \\\n    --stack-name limiter \\\n    --parameter-overrides \\\n        SnapshotRetentionDays=90 \\\n        EnablePITR=true \\\n    --capabilities CAPABILITY_NAMED_IAM\n\n# Deploy Lambda code separately\nzae-limiter lambda-export --output lambda.zip\naws lambda update-function-code \\\n    --function-name zae-limiter-aggregator \\\n    --zip-file fileb://lambda.zip\n</code></pre>"},{"location":"infra/deployment/#template-parameters","title":"Template Parameters","text":"Parameter Description Default <code>SnapshotWindows</code> Aggregation windows <code>hourly,daily</code> <code>SnapshotRetentionDays</code> Usage data retention <code>90</code> <code>EnablePITR</code> Point-in-time recovery <code>false</code> <code>LogRetentionDays</code> CloudWatch log retention <code>14</code>"},{"location":"infra/deployment/#programmatic-creation","title":"Programmatic Creation","text":"<p>Create infrastructure directly from your application:</p> <pre><code>from zae_limiter import RateLimiter, StackOptions\n\nlimiter = RateLimiter(\n    name=\"limiter\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(\n        snapshot_windows=\"hourly,daily\",\n        retention_days=90,\n    ),\n)\n</code></pre> <p><code>StackOptions</code> declares the desired infrastructure state. CloudFormation ensures the actual infrastructure matches your declaration\u2014creating, updating, or leaving unchanged as needed.</p> <p>Deployment Options</p> <p>For organizations requiring strict infrastructure/application separation, use CLI deployment or CloudFormation template export. See the Production Guide for deployment recommendations.</p>"},{"location":"infra/deployment/#infrastructure-details","title":"Infrastructure Details","text":""},{"location":"infra/deployment/#dynamodb-table","title":"DynamoDB Table","text":"<ul> <li>Billing: Pay-per-request (on-demand)</li> <li>Encryption: AWS-managed keys (default)</li> <li>Streams: NEW_AND_OLD_IMAGES for Lambda trigger</li> </ul>"},{"location":"infra/deployment/#dynamodb-schema","title":"DynamoDB Schema","text":"Key Pattern Purpose PK <code>ENTITY#{id}</code> Partition key SK <code>#META</code>, <code>#BUCKET#...</code>, <code>#LIMIT#...</code> Sort key GSI1PK <code>PARENT#{id}</code> Parent lookups GSI2PK <code>RESOURCE#{name}</code> Resource aggregation"},{"location":"infra/deployment/#lambda-function","title":"Lambda Function","text":"<ul> <li>Runtime: Python 3.12</li> <li>Memory: 256 MB</li> <li>Timeout: 60 seconds</li> <li>Trigger: DynamoDB Streams</li> </ul>"},{"location":"infra/deployment/#iam-permissions","title":"IAM Permissions","text":"<p>The Lambda function has minimal permissions:</p> <pre><code>- dynamodb:GetItem\n- dynamodb:PutItem\n- dynamodb:UpdateItem\n- dynamodb:Query\n</code></pre> <p>When X-Ray tracing is enabled, additional permissions are granted:</p> <pre><code>- xray:PutTraceSegments\n- xray:PutTelemetryRecords\n</code></pre>"},{"location":"infra/deployment/#aws-x-ray-tracing","title":"AWS X-Ray Tracing","text":"<p>Enable X-Ray tracing to gain visibility into Lambda aggregator performance and troubleshoot issues.</p>"},{"location":"infra/deployment/#enabling-tracing","title":"Enabling Tracing","text":"CLIProgrammatic <pre><code>zae-limiter deploy --name limiter --region us-east-1 --enable-tracing\n</code></pre> <pre><code>from zae_limiter import RateLimiter, StackOptions\n\nlimiter = RateLimiter(\n    name=\"limiter\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(enable_tracing=True),\n)\n</code></pre>"},{"location":"infra/deployment/#key-details","title":"Key Details","text":"Aspect Value Default Disabled (opt-in) Tracing Mode Active IAM Conditional (only when enabled) <p>Why Active Mode?</p> <p>The Lambda aggregator uses Active tracing mode rather than PassThrough because DynamoDB Streams do not propagate X-Ray trace context. Active mode ensures traces are always generated for Lambda invocations regardless of upstream trace headers.</p>"},{"location":"infra/deployment/#what-you-get","title":"What You Get","text":"<p>With X-Ray tracing enabled, you can:</p> <ul> <li>Trace Lambda cold starts - Identify initialization latency</li> <li>Monitor DynamoDB operations - See query and update times</li> <li>Debug failures - Trace errors through the processing pipeline</li> <li>Analyze performance - Find bottlenecks in stream processing</li> </ul>"},{"location":"infra/deployment/#cost-considerations","title":"Cost Considerations","text":"<p>X-Ray charges based on traces recorded and retrieved. For typical usage:</p> <ul> <li>First 100,000 traces/month are free</li> <li>After free tier: $5.00 per million traces recorded</li> </ul> <p>For high-volume deployments, consider sampling strategies or enabling tracing only for troubleshooting.</p>"},{"location":"infra/deployment/#application-iam-roles","title":"Application IAM Roles","text":"<p>The stack creates three optional IAM roles for different access patterns. These are enabled by default and provide least-privilege access for applications, administrators, and monitoring systems.</p>"},{"location":"infra/deployment/#enablingdisabling-roles","title":"Enabling/Disabling Roles","text":"CLIProgrammatic <pre><code># Deploy with IAM roles (default)\nzae-limiter deploy --name limiter --region us-east-1\n\n# Deploy without IAM roles (for custom IAM)\nzae-limiter deploy --name limiter --region us-east-1 --no-iam-roles\n</code></pre> <pre><code>from zae_limiter import RateLimiter, StackOptions\n\n# With IAM roles (default)\nlimiter = RateLimiter(\n    name=\"limiter\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(),  # create_iam_roles=True by default\n)\n\n# Without IAM roles\nlimiter = RateLimiter(\n    name=\"limiter\",\n    region=\"us-east-1\",\n    stack_options=StackOptions(create_iam_roles=False),\n)\n</code></pre>"},{"location":"infra/deployment/#role-summary","title":"Role Summary","text":"Role Use Case DynamoDB Permissions <code>AppRole</code> Applications calling <code>acquire()</code> GetItem, Query, TransactWriteItems <code>AdminRole</code> Ops teams managing config App + PutItem, DeleteItem, UpdateItem, BatchWriteItem <code>ReadOnlyRole</code> Monitoring and dashboards GetItem, Query, Scan, DescribeTable"},{"location":"infra/deployment/#viewing-role-arns","title":"Viewing Role ARNs","text":"<p>The <code>status</code> command shows IAM role ARNs when roles are enabled:</p> <pre><code>zae-limiter status --name limiter --region us-east-1\n</code></pre> <p>Output includes: <pre><code>IAM Roles\n  App:           arn:aws:iam::123456789012:role/limiter-app-role\n  Admin:         arn:aws:iam::123456789012:role/limiter-admin-role\n  ReadOnly:      arn:aws:iam::123456789012:role/limiter-readonly-role\n</code></pre></p>"},{"location":"infra/deployment/#role-naming","title":"Role Naming","text":"<ul> <li>Default: <code>${StackName}-{app,admin,readonly}-role</code> (e.g., <code>limiter-app-role</code>)</li> <li>With <code>--role-name-format</code>: Custom naming pattern applied to all roles</li> </ul> <p>Roles respect <code>--permission-boundary</code> if configured.</p> <p>For detailed IAM role configuration and usage examples, see CloudFormation - Application IAM Roles.</p>"},{"location":"infra/deployment/#next-steps","title":"Next Steps","text":"<ul> <li>Production - Production checklist, security, cost estimation</li> <li>CloudFormation - Template details</li> <li>Monitoring - Dashboards, alerts, Logs Insights</li> <li>LocalStack - Local development setup</li> </ul>"},{"location":"infra/production/","title":"Production Deployment","text":"<p>This guide covers production-readiness for zae-limiter deployments.</p>"},{"location":"infra/production/#production-checklist","title":"Production Checklist","text":"<p>Before deploying to production:</p> Feature CLI Flag Default Recommendation Point-in-Time Recovery <code>--pitr-recovery-days N</code> Disabled Enable (7-35 days) CloudWatch Alarms <code>--enable-alarms</code> Enabled Keep enabled SNS Notifications <code>--alarm-sns-topic ARN</code> None Configure for alerts Log Retention <code>--log-retention-days N</code> 30 90+ for compliance Audit Archival <code>--enable-audit-archival</code> Enabled Keep enabled for compliance Glacier Transition <code>--audit-archive-glacier-days N</code> 90 Adjust based on access patterns Permission Boundary <code>--permission-boundary ARN</code> None Use in restricted IAM environments X-Ray Tracing <code>--enable-tracing</code> Disabled Enable for debugging/performance analysis IAM Roles <code>--enable-iam-roles</code> Enabled Use for least-privilege app access"},{"location":"infra/production/#example-production-deployment","title":"Example Production Deployment","text":"<pre><code>zae-limiter deploy \\\n    --name prod-limiter \\\n    --region us-east-1 \\\n    --pitr-recovery-days 7 \\\n    --log-retention-days 90 \\\n    --alarm-sns-topic arn:aws:sns:us-east-1:123456789012:alerts \\\n    --enable-audit-archival \\\n    --audit-archive-glacier-days 90\n</code></pre>"},{"location":"infra/production/#security-best-practices","title":"Security Best Practices","text":""},{"location":"infra/production/#encryption","title":"Encryption","text":"<ul> <li>DynamoDB uses AWS-managed keys (SSE-S3) by default</li> <li>For customer-managed keys (CMK), use CloudFormation template customization</li> </ul>"},{"location":"infra/production/#iam","title":"IAM","text":""},{"location":"infra/production/#application-access-roles","title":"Application Access Roles","text":"<p>The stack creates three IAM roles for different access patterns (enabled by default):</p> Role Use Case When to Use AppRole Applications calling <code>acquire()</code> Production workloads, Lambda functions, ECS tasks AdminRole Ops teams managing config CLI tools, admin scripts, CI/CD pipelines ReadOnlyRole Monitoring and dashboards Grafana, CloudWatch dashboards, audit tools <p>Best practices:</p> <ul> <li>Use AppRole for applications - Provides only the permissions needed for rate limiting</li> <li>Use AdminRole for config management - Separate from application credentials</li> <li>Use ReadOnlyRole for observability - Safe access for monitoring systems</li> <li>Disable with <code>--no-iam-roles</code> - When using existing IAM policies or cross-account access</li> </ul>"},{"location":"infra/production/#lambda-aggregator","title":"Lambda Aggregator","text":"<p>The Lambda aggregator uses a separate execution role with least-privilege permissions:</p> <ul> <li><code>dynamodb:GetItem</code>, <code>PutItem</code>, <code>UpdateItem</code>, <code>Query</code></li> <li><code>s3:PutObject</code> (when audit archival is enabled)</li> </ul>"},{"location":"infra/production/#permission-boundaries","title":"Permission Boundaries","text":"<p>For restricted IAM environments:</p> <ul> <li>Use <code>--permission-boundary</code> to apply organizational policies to all created roles</li> <li>Use <code>--role-name-format</code> for organizational naming conventions</li> </ul> <pre><code># Enterprise deployment with permission boundary\nzae-limiter deploy \\\n    --name prod-limiter \\\n    --permission-boundary arn:aws:iam::123456789012:policy/ServiceBoundary \\\n    --role-name-format \"svc-{}\"\n</code></pre>"},{"location":"infra/production/#network","title":"Network","text":"<ul> <li>No VPC required; uses AWS service endpoints</li> <li>For VPC deployment, configure VPC endpoints for DynamoDB and Lambda</li> </ul>"},{"location":"infra/production/#secrets-management","title":"Secrets Management","text":"<ul> <li>No secrets stored in DynamoDB</li> <li>Use AWS Secrets Manager or Parameter Store for API keys</li> <li>Rate limit entity IDs should not contain sensitive data</li> </ul>"},{"location":"infra/production/#audit-logging","title":"Audit Logging","text":"<ul> <li>All entity and limit changes are automatically logged</li> <li>Track who made changes with optional <code>principal</code> parameter</li> <li>Events auto-expire after 90 days (configurable via <code>--audit-ttl-days</code>)</li> <li>Expired events are archived to S3 for long-term retention</li> <li>Archives transition to Glacier IR after 90 days (configurable via <code>--audit-archive-glacier-days</code>)</li> <li>For compliance requirements, see Audit Logging Guide</li> </ul>"},{"location":"infra/production/#multi-region-considerations","title":"Multi-Region Considerations","text":"<p>zae-limiter is designed for single-region deployment:</p> Scenario Approach Single region Standard deployment Multi-region (independent) Deploy separate stacks per region Global rate limiting Application-level coordination required"},{"location":"infra/production/#why-not-global-tables","title":"Why Not Global Tables?","text":"<ul> <li>Rate limit state is time-sensitive (token buckets refill continuously)</li> <li>Cross-region replication lag would cause inconsistent limits</li> <li>Each region should enforce its own limits</li> </ul>"},{"location":"infra/production/#cross-region-pattern","title":"Cross-Region Pattern","text":"<pre><code># Deploy separate stacks per region\nus_limiter = RateLimiter(name=\"prod\", region=\"us-east-1\")\neu_limiter = RateLimiter(name=\"prod\", region=\"eu-west-1\")\n\n# Application coordinates between regions if needed\n</code></pre>"},{"location":"infra/production/#monitoring-alerting","title":"Monitoring &amp; Alerting","text":"<p>The stack deploys CloudWatch alarms by default:</p> Alarm Trigger Action Lambda Errors &gt; 1 per 5 min Check logs, verify DynamoDB access Lambda Duration &gt; 80% timeout Increase memory or timeout Iterator Age &gt; 30 seconds Check Lambda concurrency DLQ Messages &gt;= 1 Investigate failed records DynamoDB Throttles &gt; 1 per 5 min Review capacity planning <p>For dashboard templates and Logs Insights queries, see Monitoring Guide.</p>"},{"location":"infra/production/#cost-estimation","title":"Cost Estimation","text":"<p>Costs scale with request volume (us-east-1 pricing):</p> Volume DynamoDB Lambda CloudWatch S3 Archive Total 10K req/day ~$0.50 ~$0.20 ~$0.10 ~$0.01 ~$1/month 100K req/day ~$5 ~$2 ~$1 ~$0.10 ~$8/month 1M req/day ~$45 ~$12 ~$5 ~$1 ~$63/month <p>S3 costs include: - Standard storage: First 90 days (or configured <code>--audit-archive-glacier-days</code>) - Glacier IR storage: After transition (~80% cheaper than Standard) - PUT requests: One per Lambda batch (~$0.005 per 1000 requests)</p> <p>For detailed capacity planning and optimization, see Performance Guide.</p>"},{"location":"infra/production/#next-steps","title":"Next Steps","text":"<ul> <li>Monitoring Guide - Dashboards, alerts, Logs Insights</li> <li>Audit Logging - Compliance tracking and incident investigation</li> <li>Performance Guide - Capacity planning, optimization</li> <li>Operations Guide - Troubleshooting, recovery</li> </ul>"},{"location":"operations/","title":"Operations Guide","text":"<p>This guide consolidates troubleshooting and operational procedures for zae-limiter deployments. Navigate using the interactive map below or jump directly to a topic.</p>"},{"location":"operations/#navigation","title":"Navigation","text":"flowchart LR     OPS[Operations Guide]      subgraph alerts[Alerts &amp; Issues]         LAMBDA[Lambda Aggregator]         DYNAMO[DynamoDB]         STREAMS[Stream Processing]         VERSION[Version Errors]         LIMITS[Rate Limits]     end      subgraph planned[Planned Operations]         UPGRADE[Version Upgrade]         LAMBDA_UPDATE[Lambda Update]         SCALE_LIMITS[Adjust Limits]         SCALE_DYNAMO[Scale DynamoDB]         BACKUP[Backup &amp; Restore]         ROLLBACK[Emergency Rollback]     end      OPS --&gt; alerts     OPS --&gt; planned      click LAMBDA \"lambda/\" \"Lambda troubleshooting\"     click DYNAMO \"dynamodb/\" \"DynamoDB operations\"     click STREAMS \"streams/\" \"Stream processing\"     click VERSION \"version/\" \"Version management\"     click LIMITS \"rate-limits/\" \"Rate limit issues\"     click UPGRADE \"version/#upgrade-procedure\" \"Upgrade procedure\"     click LAMBDA_UPDATE \"lambda/#lambda-redeployment\" \"Redeploy Lambda\"     click SCALE_LIMITS \"rate-limits/#adjust-limits-at-runtime\" \"Adjust limits\"     click SCALE_DYNAMO \"dynamodb/#scaling-procedures\" \"Scale capacity\"     click BACKUP \"recovery/#dynamodb-backup-and-restore\" \"Backup &amp; restore\"     click ROLLBACK \"recovery/#emergency-rollback-decision-matrix\" \"Emergency rollback\""},{"location":"operations/#alerts-issues","title":"Alerts &amp; Issues","text":"Component Common Issues Lambda Error rate, Duration/timeout, DLQ messages DynamoDB Throttling, Capacity Streams Iterator age, Processing lag Version VersionMismatchError, IncompatibleSchemaError Rate Limits Unexpected RateLimitExceeded, Limits not enforcing"},{"location":"operations/#planned-operations","title":"Planned Operations","text":"Operation Guides Upgrades Version upgrade, Lambda update Scaling Adjust rate limits, DynamoDB capacity Recovery Emergency rollback, Backup/restore, PITR"},{"location":"operations/#quick-reference","title":"Quick Reference","text":"Symptom Go To <code>RateLimitExceeded</code> unexpected Rate Limits <code>ProvisionedThroughputExceededException</code> DynamoDB DLQ messages accumulating Lambda <code>VersionMismatchError</code> Version High <code>IteratorAge</code> Streams Need to rollback Recovery"},{"location":"operations/#cli-diagnostic-commands","title":"CLI Diagnostic Commands","text":"Command Description <code>zae-limiter status --name &lt;name&gt;</code> Check stack status and resources <code>zae-limiter version --name &lt;name&gt;</code> Show version information <code>zae-limiter check --name &lt;name&gt;</code> Check client/infrastructure compatibility"},{"location":"operations/#cloudwatch-metrics-overview","title":"CloudWatch Metrics Overview","text":"Metric Namespace Threshold Guide <code>Errors</code> AWS/Lambda &gt; 1/5min Lambda <code>Duration</code> AWS/Lambda &gt; 80% timeout Lambda <code>IteratorAge</code> AWS/Lambda &gt; 30,000ms Streams <code>ReadThrottleEvents</code> AWS/DynamoDB &gt; 0 DynamoDB <code>WriteThrottleEvents</code> AWS/DynamoDB &gt; 0 DynamoDB <code>ApproximateNumberOfMessagesVisible</code> AWS/SQS &gt; 0 Lambda"},{"location":"operations/#exception-reference","title":"Exception Reference","text":"Exception Cause Guide <code>RateLimitExceeded</code> Rate limit violated Rate Limits <code>RateLimiterUnavailable</code> DynamoDB unavailable DynamoDB <code>EntityNotFoundError</code> Entity doesn't exist Rate Limits <code>VersionMismatchError</code> Client/Lambda version mismatch Version <code>IncompatibleSchemaError</code> Major version difference Version <code>StackCreationError</code> CloudFormation failed Recovery"},{"location":"operations/#related-documentation","title":"Related Documentation","text":"<ul> <li>Monitoring Guide - CloudWatch metrics, dashboards, and alerts</li> <li>Performance Tuning - Capacity planning and optimization</li> <li>Migration Guide - Schema versioning and upgrades</li> <li>Unavailability Handling - Configure behavior when DynamoDB is unavailable</li> </ul>"},{"location":"operations/dynamodb/","title":"DynamoDB Operations","text":"<p>This guide covers troubleshooting and operational procedures for DynamoDB capacity management and throttling issues.</p>"},{"location":"operations/dynamodb/#decision-tree","title":"Decision Tree","text":"flowchart TD     START([DynamoDB Issue]) --&gt; Q1{What's the symptom?}      Q1 --&gt;|ReadThrottleEvents| A1[Check read patterns]     Q1 --&gt;|WriteThrottleEvents| A2[Check write patterns]     Q1 --&gt;|High latency| A3[Check capacity utilization]     Q1 --&gt;|Planning scale| A4[Capacity planning]      A1 --&gt; FIX1{Emergency?}     A2 --&gt; FIX1     FIX1 --&gt;|Yes| EMERGENCY[Switch to on-demand]     FIX1 --&gt;|No| PLAN[Increase provisioned capacity]      A3 --&gt; CHECK[Check CloudWatch metrics]     CHECK --&gt; FIX1      A4 --&gt; LINK([\u2192 Performance Guide])      click A1 \"#throttling\" \"Diagnose throttling\"     click A2 \"#throttling\" \"Diagnose throttling\"     click A3 \"#capacity-planning\" \"Check capacity\"     click EMERGENCY \"#emergency-on-demand\" \"Switch to on-demand\"     click PLAN \"#scaling-procedures\" \"Scale capacity\"     click CHECK \"#diagnostic-queries\" \"View metrics\"     click LINK \"../performance/\" \"Capacity planning guide\""},{"location":"operations/dynamodb/#troubleshooting","title":"Troubleshooting","text":""},{"location":"operations/dynamodb/#symptoms","title":"Symptoms","text":"<ul> <li><code>ProvisionedThroughputExceededException</code> errors</li> <li>Increased latency on rate limit checks</li> <li>CloudWatch throttle alarms triggered</li> <li><code>RateLimiterUnavailable</code> with <code>OnUnavailable.BLOCK</code> mode</li> </ul>"},{"location":"operations/dynamodb/#diagnostic-steps","title":"Diagnostic Steps","text":"<p>Check CloudWatch metrics:</p> <pre><code># View throttle events (last hour)\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/DynamoDB \\\n  --metric-name ReadThrottleEvents \\\n  --dimensions Name=TableName,Value=&lt;name&gt; \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 300 \\\n  --statistics Sum\n</code></pre> <p>Check capacity utilization:</p> <pre><code>aws cloudwatch get-metric-statistics \\\n  --namespace AWS/DynamoDB \\\n  --metric-name ConsumedReadCapacityUnits \\\n  --dimensions Name=TableName,Value=&lt;name&gt; \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 300 \\\n  --statistics Sum\n</code></pre> <p>Identify hot partitions with Contributor Insights:</p> <p>See Per-Partition Monitoring below for detailed setup and troubleshooting.</p>"},{"location":"operations/dynamodb/#throttling","title":"Throttling","text":""},{"location":"operations/dynamodb/#common-causes-and-solutions","title":"Common Causes and Solutions","text":"Cause Solution Provisioned capacity too low Increase RCU/WCU or switch to on-demand Hot partition Distribute entity IDs more evenly Burst traffic Enable auto-scaling or use on-demand GSI throttling Check GSI capacity separately"},{"location":"operations/dynamodb/#read-throttling","title":"Read Throttling","text":"<p>Check which operations are throttling:</p> <pre><code>aws cloudwatch get-metric-statistics \\\n  --namespace AWS/DynamoDB \\\n  --metric-name ThrottledRequests \\\n  --dimensions Name=TableName,Value=&lt;name&gt; Name=Operation,Value=GetItem \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 300 \\\n  --statistics Sum\n</code></pre>"},{"location":"operations/dynamodb/#write-throttling","title":"Write Throttling","text":"<p>Write throttling typically occurs during high-volume rate limiting or when the aggregator Lambda is processing many stream events.</p> <p>Check write patterns:</p> <pre><code>aws cloudwatch get-metric-statistics \\\n  --namespace AWS/DynamoDB \\\n  --metric-name ConsumedWriteCapacityUnits \\\n  --dimensions Name=TableName,Value=&lt;name&gt; \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 60 \\\n  --statistics Sum\n</code></pre>"},{"location":"operations/dynamodb/#capacity-planning","title":"Capacity Planning","text":"<p>For detailed capacity calculations, see the Performance Tuning Guide.</p> <p>Quick estimates:</p> Operation RCU WCU <code>acquire()</code> ~2 ~4 <code>acquire()</code> with cascade ~4 ~8 <code>available()</code> ~2 ~0 Aggregator (per record) ~1 ~2"},{"location":"operations/dynamodb/#procedures","title":"Procedures","text":""},{"location":"operations/dynamodb/#emergency-capacity-increase","title":"Emergency Capacity Increase","text":"<p>Switch to on-demand capacity (immediate relief):</p> <p>Billing Impact</p> <p>On-demand pricing is typically 5-7x more expensive than provisioned capacity at steady state, but has no throttling.</p> <pre><code>aws dynamodb update-table \\\n  --table-name &lt;name&gt; \\\n  --billing-mode PAY_PER_REQUEST\n</code></pre> <p>Increase provisioned capacity:</p> <pre><code>aws dynamodb update-table \\\n  --table-name &lt;name&gt; \\\n  --provisioned-throughput ReadCapacityUnits=1000,WriteCapacityUnits=500\n</code></pre>"},{"location":"operations/dynamodb/#scaling-procedures","title":"Scaling Procedures","text":""},{"location":"operations/dynamodb/#planned-capacity-scaling","title":"Planned Capacity Scaling","text":"<p>Step 1: Analyze current usage</p> <pre><code># Get average consumption over last 24 hours\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/DynamoDB \\\n  --metric-name ConsumedReadCapacityUnits \\\n  --dimensions Name=TableName,Value=&lt;name&gt; \\\n  --start-time $(date -u -d '24 hours ago' +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 3600 \\\n  --statistics Average\n</code></pre> <p>Step 2: Calculate required capacity</p> <p>Use the formulas from Performance Tuning:</p> <ul> <li><code>RCU = (requests_per_second \u00d7 2) + (cascade_requests \u00d7 2)</code></li> <li><code>WCU = (requests_per_second \u00d7 4) + (cascade_requests \u00d7 4)</code></li> </ul> <p>Add 20% headroom for bursts.</p> <p>Step 3: Apply changes</p> <pre><code>aws dynamodb update-table \\\n  --table-name &lt;name&gt; \\\n  --provisioned-throughput ReadCapacityUnits=&lt;new_rcu&gt;,WriteCapacityUnits=&lt;new_wcu&gt;\n</code></pre> <p>Step 4: Verify</p> <pre><code>aws dynamodb describe-table --table-name &lt;name&gt; \\\n  --query 'Table.ProvisionedThroughput'\n</code></pre>"},{"location":"operations/dynamodb/#enable-auto-scaling","title":"Enable Auto-Scaling","text":"<p>Create scaling targets:</p> <pre><code># Register read capacity target\naws application-autoscaling register-scalable-target \\\n  --service-namespace dynamodb \\\n  --resource-id \"table/&lt;name&gt;\" \\\n  --scalable-dimension \"dynamodb:table:ReadCapacityUnits\" \\\n  --min-capacity 5 \\\n  --max-capacity 1000\n\n# Register write capacity target\naws application-autoscaling register-scalable-target \\\n  --service-namespace dynamodb \\\n  --resource-id \"table/&lt;name&gt;\" \\\n  --scalable-dimension \"dynamodb:table:WriteCapacityUnits\" \\\n  --min-capacity 5 \\\n  --max-capacity 500\n</code></pre> <p>Create scaling policies:</p> <pre><code># Read capacity policy (target 70% utilization)\naws application-autoscaling put-scaling-policy \\\n  --service-namespace dynamodb \\\n  --resource-id \"table/&lt;name&gt;\" \\\n  --scalable-dimension \"dynamodb:table:ReadCapacityUnits\" \\\n  --policy-name \"&lt;name&gt;-read-scaling\" \\\n  --policy-type \"TargetTrackingScaling\" \\\n  --target-tracking-scaling-policy-configuration '{\n    \"TargetValue\": 70.0,\n    \"PredefinedMetricSpecification\": {\n      \"PredefinedMetricType\": \"DynamoDBReadCapacityUtilization\"\n    }\n  }'\n</code></pre>"},{"location":"operations/dynamodb/#switch-to-on-demand","title":"Switch to On-Demand","text":"<p>When to use on-demand:</p> <ul> <li>Unpredictable traffic patterns</li> <li>New deployments without baseline data</li> <li>Cost is less important than avoiding throttling</li> </ul> <p>Switch from provisioned to on-demand:</p> <pre><code>aws dynamodb update-table \\\n  --table-name &lt;name&gt; \\\n  --billing-mode PAY_PER_REQUEST\n</code></pre> <p>Switch back to provisioned:</p> <p>Cooldown Period</p> <p>You can only switch billing modes once per 24 hours.</p> <pre><code>aws dynamodb update-table \\\n  --table-name &lt;name&gt; \\\n  --billing-mode PROVISIONED \\\n  --provisioned-throughput ReadCapacityUnits=100,WriteCapacityUnits=50\n</code></pre>"},{"location":"operations/dynamodb/#verification","title":"Verification","text":"<p>After capacity changes, monitor for 15-30 minutes:</p> <pre><code># Watch throttle events\nwatch -n 30 \"aws cloudwatch get-metric-statistics \\\n  --namespace AWS/DynamoDB \\\n  --metric-name ReadThrottleEvents \\\n  --dimensions Name=TableName,Value=&lt;name&gt; \\\n  --start-time \\$(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time \\$(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 60 \\\n  --statistics Sum\"\n</code></pre>"},{"location":"operations/dynamodb/#per-partition-monitoring","title":"Per-Partition Monitoring","text":""},{"location":"operations/dynamodb/#understanding-hot-partitions","title":"Understanding Hot Partitions","text":"<p>DynamoDB distributes data across multiple internal partitions. Each partition has its own throughput quota:</p> <ul> <li>Per-partition read limit: ~3,000 RCU</li> <li>Per-partition write limit: ~1,000 WCU</li> </ul> <p>If a single partition receives most traffic (e.g., a high-fanout parent entity with 1,000+ children and <code>cascade=True</code>), that partition throttles while others are idle. This is called a hot partition.</p> <p>Typical symptoms: - High latency on specific operations - CloudWatch shows <code>ReadThrottleEvents</code> or <code>WriteThrottleEvents</code> - Some entity IDs throttle but others don't - Table-level capacity looks fine but requests still throttle</p>"},{"location":"operations/dynamodb/#contributor-insights-setup","title":"Contributor Insights Setup","text":"<p>DynamoDB Contributor Insights identifies which partition keys consume the most throughput, helping you spot hot partitions.</p>"},{"location":"operations/dynamodb/#enable-contributor-insights","title":"Enable Contributor Insights","text":"<pre><code># Enable for main table\naws dynamodb update-contributor-insights \\\n  --table-name &lt;name&gt; \\\n  --contributor-insights-action ENABLE\n\n# Enable for GSI (if needed)\naws dynamodb update-contributor-insights \\\n  --table-name &lt;name&gt; \\\n  --contributor-insights-action ENABLE \\\n  --index-name GSI1\n</code></pre> <p>Cost: ~$0.10 per table per day (minimal). Monitoring is available after ~30 minutes of data collection.</p>"},{"location":"operations/dynamodb/#view-contributor-insights-data","title":"View Contributor Insights Data","text":"<pre><code># Check if enabled\naws dynamodb describe-contributor-insights \\\n  --table-name &lt;name&gt;\n\n# Get top partition keys by throughput\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/DynamoDB \\\n  --metric-name ConributorValue \\\n  --dimensions Name=TableName,Value=&lt;name&gt; Name=Contributor,Value=PK \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 300 \\\n  --statistics Sum \\\n  --max-records 10\n</code></pre> <p>Or use the AWS Console:</p> <ol> <li>Go to DynamoDB \u2192 Tables \u2192 <code>&lt;name&gt;</code> \u2192 Contributor Insights</li> <li>Review \"Top Keys\" to identify which partition keys consume most throughput</li> <li>Cross-reference with your entity IDs to find the hot entity</li> </ol>"},{"location":"operations/dynamodb/#interpreting-results","title":"Interpreting Results","text":"<p>If you see:</p> Pattern Meaning Action One entity ID consuming 70%+ of throughput Hot parent with cascade Implement write sharding Multiple entity IDs evenly distributed Normal load distribution No action needed Spike on specific entity at specific time Burst traffic on one user Monitor or rate limit GSI1 showing skewed distribution Parent lookup imbalance Review parent structure"},{"location":"operations/dynamodb/#identifying-cascade-hot-partitions","title":"Identifying Cascade Hot Partitions","text":"<p>When using <code>cascade=True</code> with high-fanout parents, check for these patterns:</p> <pre><code># Query: Which parent entities have the most children?\n# This is NOT a built-in query, but you can estimate from operation patterns:\n\n# 1. Enable Contributor Insights (as above)\n# 2. In CloudWatch, look at top PK values during peak traffic\n# 3. If top 3-5 PK values account for &gt;50% of writes, those are likely high-fanout parents\n</code></pre> <p>Example diagnosis:</p> <pre><code>Top 5 Partition Keys (by write throughput):\n1. ENTITY#project-123  \u2192 45% of writes (HIGH FANOUT - HOTSPOT!)\n2. ENTITY#project-456  \u2192 12% of writes\n3. ENTITY#project-789  \u2192 10% of writes\n4. ENTITY#api-key-999  \u2192 8% of writes\n5. ENTITY#...          \u2192 remainder\n\nAnalysis:\n- project-123 likely has 1000+ API keys with cascade=True\n- This parent partition can handle ~1,000 WCU max\n- If concurrent writes exceed this, throttling occurs\n</code></pre>"},{"location":"operations/dynamodb/#mitigations","title":"Mitigations","text":"<p>Once you've identified a hot partition:</p>"},{"location":"operations/dynamodb/#1-write-sharding-recommended-for-cascade","title":"1. Write Sharding (Recommended for Cascade)","text":"<p>Split high-fanout parent into sharded parents. See Write Sharding Guide for detailed example.</p> <pre><code># Instead of:\n#   API Key \u2192 project-123 (1000+ children, hotspot)\n#\n# Create:\n#   API Key \u2192 project-123-shard-0 through project-123-shard-N\n#   (Distribute API keys across N shards)\n\n# This spreads writes across N partitions, multiplying capacity\n</code></pre> <p>Effectiveness: 10x shards = ~10x capacity improvement (linear)</p>"},{"location":"operations/dynamodb/#2-reduce-cascade-usage","title":"2. Reduce Cascade Usage","text":"<p>If hot partition is detected and write sharding isn't feasible:</p> <pre><code># Create entities without cascade\nawait limiter.create_entity(entity_id=\"api-key\", parent_id=\"project-1\", cascade=False)\n\n# Then manually check parent limits if needed\nawait limiter.available(\"project-1\", \"llm-api\", [parent_rpm_limit])\n</code></pre> <p>Trade-off: Parent limits no longer automatically enforced; you must check them explicitly.</p>"},{"location":"operations/dynamodb/#3-switch-to-on-demand-billing","title":"3. Switch to On-Demand Billing","text":"<pre><code>aws dynamodb update-table \\\n  --table-name &lt;name&gt; \\\n  --billing-mode PAY_PER_REQUEST\n</code></pre> <p>Cost: 5-7x more expensive, but no throttling and no partition limits.</p>"},{"location":"operations/dynamodb/#4-distributed-load-strategy","title":"4. Distributed Load Strategy","text":"<p>For enterprise deployments with many customers:</p> <ul> <li>Create one parent per customer tenant instead of one global parent</li> <li>Distribute high-fanout parents across multiple tables (region sharding)</li> <li>Use AWS Global Tables for multi-region replication</li> </ul>"},{"location":"operations/dynamodb/#monitoring-checklist","title":"Monitoring Checklist","text":"<p>Add these CloudWatch alarms to catch hot partitions early:</p> <pre><code># Alert on write throttling\naws cloudwatch put-metric-alarm \\\n  --alarm-name \"&lt;name&gt;-write-throttle\" \\\n  --alarm-description \"Alert when write throttling occurs\" \\\n  --metric-name WriteThrottleEvents \\\n  --namespace AWS/DynamoDB \\\n  --dimensions Name=TableName,Value=&lt;name&gt; \\\n  --statistic Sum \\\n  --period 300 \\\n  --threshold 1 \\\n  --comparison-operator GreaterThanOrEqualToThreshold \\\n  --alarm-actions arn:aws:sns:us-east-1:123456789012:alerts\n\n# Alert on read throttling\naws cloudwatch put-metric-alarm \\\n  --alarm-name \"&lt;name&gt;-read-throttle\" \\\n  --alarm-description \"Alert when read throttling occurs\" \\\n  --metric-name ReadThrottleEvents \\\n  --namespace AWS/DynamoDB \\\n  --dimensions Name=TableName,Value=&lt;name&gt; \\\n  --statistic Sum \\\n  --period 300 \\\n  --threshold 1 \\\n  --comparison-operator GreaterThanOrEqualToThreshold \\\n  --alarm-actions arn:aws:sns:us-east-1:123456789012:alerts\n\n# Alert on high consumed capacity (warning before throttle)\naws cloudwatch put-metric-alarm \\\n  --alarm-name \"&lt;name&gt;-high-capacity\" \\\n  --alarm-description \"Warning: approaching provisioned capacity\" \\\n  --metric-name ConsumedWriteCapacityUnits \\\n  --namespace AWS/DynamoDB \\\n  --dimensions Name=TableName,Value=&lt;name&gt; \\\n  --statistic Average \\\n  --period 300 \\\n  --threshold 800 \\\n  --comparison-operator GreaterThanThreshold \\\n  --alarm-actions arn:aws:sns:us-east-1:123456789012:warnings\n</code></pre>"},{"location":"operations/dynamodb/#related","title":"Related","text":"<ul> <li>Performance Tuning - Capacity planning formulas and optimization</li> <li>Lambda Operations - Aggregator throttling due to DynamoDB</li> <li>Stream Processing - Stream processing affected by DynamoDB capacity</li> </ul>"},{"location":"operations/lambda/","title":"Lambda Aggregator Operations","text":"<p>This guide covers troubleshooting and operational procedures for the Lambda aggregator function that processes DynamoDB stream events and maintains usage snapshots.</p>"},{"location":"operations/lambda/#decision-tree","title":"Decision Tree","text":"flowchart TD     START([Lambda Issue]) --&gt; Q1{What's the symptom?}      Q1 --&gt;|Error rate alarm| CHECK1[Check CloudWatch Logs]     Q1 --&gt;|Duration alarm| CHECK2[Check processing time]     Q1 --&gt;|DLQ messages| CHECK3[Inspect DLQ]     Q1 --&gt;|Cold starts| CHECK4[Check init duration]      CHECK1 --&gt; DIAG{Error type?}     DIAG --&gt;|Permission denied| FIX1[Check IAM role]     DIAG --&gt;|Timeout| FIX2[Increase memory/timeout]     DIAG --&gt;|DynamoDB error| LINK1([\u2192 DynamoDB])     DIAG --&gt;|Code error| FIX3[Check logs, deploy fix]      CHECK2 --&gt; FIX2     CHECK3 --&gt; DLQ[DLQ Processing]     CHECK4 --&gt; FIX4[Increase memory]      click CHECK1 \"#error-rate-issues\" \"View error diagnostics\"     click FIX1 \"#error-rate-issues\" \"IAM troubleshooting\"     click FIX2 \"#high-lambda-duration\" \"Increase resources\"     click LINK1 \"dynamodb/\" \"DynamoDB operations\"     click FIX3 \"#lambda-redeployment\" \"Redeploy Lambda\"     click DLQ \"#messages-in-dead-letter-queue\" \"DLQ processing\"     click FIX4 \"#cold-start-issues\" \"Cold start fixes\""},{"location":"operations/lambda/#health-indicators","title":"Health Indicators","text":"<p>Monitor these metrics for Lambda health. See Monitoring Guide for dashboard templates.</p> Metric Healthy Warning Critical Error Rate 0% &lt; 1% &gt; 1% Duration &lt; 50% timeout &lt; 80% timeout &gt; 80% timeout Iterator Age &lt; 1s &lt; 30s &gt; 30s DLQ Depth 0 1-10 &gt; 10"},{"location":"operations/lambda/#troubleshooting","title":"Troubleshooting","text":""},{"location":"operations/lambda/#symptoms","title":"Symptoms","text":"<ul> <li>Usage snapshots not updating</li> <li>Messages accumulating in Dead Letter Queue (DLQ)</li> <li>Lambda duration alarm triggered</li> <li>CloudWatch Logs showing errors</li> </ul>"},{"location":"operations/lambda/#diagnostic-steps","title":"Diagnostic Steps","text":"<p>Check Lambda errors:</p> <pre><code># View recent Lambda invocations\naws logs filter-log-events \\\n  --log-group-name /aws/lambda/&lt;name&gt;-aggregator \\\n  --start-time $(date -u -d '1 hour ago' +%s)000 \\\n  --filter-pattern \"ERROR\"\n</code></pre> <p>Check DLQ message count:</p> <pre><code>aws sqs get-queue-attributes \\\n  --queue-url https://sqs.&lt;region&gt;.amazonaws.com/&lt;account&gt;/&lt;name&gt;-aggregator-dlq \\\n  --attribute-names ApproximateNumberOfMessagesVisible\n</code></pre> <p>Inspect DLQ messages:</p> <pre><code>aws sqs receive-message \\\n  --queue-url https://sqs.&lt;region&gt;.amazonaws.com/&lt;account&gt;/&lt;name&gt;-aggregator-dlq \\\n  --max-number-of-messages 10 \\\n  --visibility-timeout 0\n</code></pre>"},{"location":"operations/lambda/#error-rate-issues","title":"Error Rate Issues","text":"<p>CloudWatch Logs Insights query for errors:</p> <pre><code>fields @timestamp, @message\n| filter @message like /ERROR/\n| sort @timestamp desc\n| limit 50\n</code></pre> <p>Common errors and solutions:</p> Error Cause Solution <code>AccessDeniedException</code> IAM role missing permissions Check role has DynamoDB and SQS permissions <code>ValidationException</code> Invalid DynamoDB operation Check for schema changes <code>ResourceNotFoundException</code> Table or stream doesn't exist Verify table name, redeploy stack <code>ProvisionedThroughputExceededException</code> DynamoDB throttling See DynamoDB Operations"},{"location":"operations/lambda/#high-lambda-duration","title":"High Lambda Duration","text":"<p>Symptoms: Duration alarm triggered, <code>processing_time_ms</code> &gt; 80% of timeout</p> <p>Diagnostic query:</p> <pre><code>fields @timestamp, @message\n| filter @message like /Batch processing completed/\n| parse @message /processing_time_ms\":(?&lt;duration&gt;[\\d.]+)/\n| stats avg(duration) as avg_ms,\n        pct(duration, 50) as p50_ms,\n        pct(duration, 95) as p95_ms,\n        pct(duration, 99) as p99_ms\n  by bin(1h)\n| sort @timestamp desc\n</code></pre> <p>Solutions:</p> <ol> <li> <p>Increase Lambda memory (CPU scales with memory):    <pre><code>aws lambda update-function-configuration \\\n  --function-name &lt;name&gt;-aggregator \\\n  --memory-size 512\n</code></pre></p> </li> <li> <p>Reduce batch size in event source mapping:    <pre><code>MAPPING_UUID=$(aws lambda list-event-source-mappings \\\n  --function-name &lt;name&gt;-aggregator \\\n  --query 'EventSourceMappings[0].UUID' \\\n  --output text)\n\naws lambda update-event-source-mapping \\\n  --uuid $MAPPING_UUID \\\n  --batch-size 50\n</code></pre></p> </li> <li> <p>Check DynamoDB latency - see DynamoDB Operations</p> </li> </ol>"},{"location":"operations/lambda/#messages-in-dead-letter-queue","title":"Messages in Dead Letter Queue","text":"<p>Symptoms: DLQ alarm triggered, messages accumulating</p> <p>Investigation:</p> <ol> <li>Check Lambda logs for the error that caused the failure</li> <li>Identify if it's a transient error or persistent issue</li> <li>Fix the root cause before reprocessing</li> </ol> <p>Reprocess DLQ messages after fix:</p> <pre><code>import boto3\nimport json\n\nsqs = boto3.client('sqs')\nlambda_client = boto3.client('lambda')\n\ndlq_url = \"https://sqs.&lt;region&gt;.amazonaws.com/&lt;account&gt;/&lt;name&gt;-aggregator-dlq\"\n\nwhile True:\n    response = sqs.receive_message(\n        QueueUrl=dlq_url,\n        MaxNumberOfMessages=10,\n        WaitTimeSeconds=5,\n    )\n\n    messages = response.get('Messages', [])\n    if not messages:\n        break\n\n    for msg in messages:\n        # Reprocess the failed event\n        body = json.loads(msg['Body'])\n\n        # Invoke Lambda directly with the failed records\n        lambda_client.invoke(\n            FunctionName='&lt;name&gt;-aggregator',\n            InvocationType='Event',\n            Payload=json.dumps(body),\n        )\n\n        # Delete from DLQ after successful reprocessing\n        sqs.delete_message(\n            QueueUrl=dlq_url,\n            ReceiptHandle=msg['ReceiptHandle'],\n        )\n\n        print(f\"Reprocessed message: {msg['MessageId']}\")\n</code></pre> <p>Purge DLQ (discard all messages):</p> <p>Data Loss</p> <p>This permanently discards failed events. Only use after confirming data is not needed.</p> <pre><code>aws sqs purge-queue \\\n  --queue-url https://sqs.&lt;region&gt;.amazonaws.com/&lt;account&gt;/&lt;name&gt;-aggregator-dlq\n</code></pre>"},{"location":"operations/lambda/#cold-start-issues","title":"Cold Start Issues","text":"<p>Diagnostic query:</p> <pre><code>fields @timestamp, @message, @duration\n| filter @type = \"REPORT\"\n| filter @message like /Init Duration/\n| parse @message /Init Duration: (?&lt;init_duration&gt;[\\d.]+) ms/\n| stats count() as cold_starts,\n        avg(init_duration) as avg_init_ms\n  by bin(1h)\n</code></pre> <p>Solutions:</p> <ol> <li>Increase Lambda memory (faster initialization)</li> <li>Enable provisioned concurrency for consistent latency:    <pre><code># Publish a new version first (provisioned concurrency requires a version or alias)\nVERSION=$(aws lambda publish-version \\\n  --function-name &lt;name&gt;-aggregator \\\n  --query 'Version' --output text)\n\n# Configure provisioned concurrency on the published version\naws lambda put-provisioned-concurrency-config \\\n  --function-name &lt;name&gt;-aggregator \\\n  --qualifier $VERSION \\\n  --provisioned-concurrent-executions 2\n</code></pre></li> </ol>"},{"location":"operations/lambda/#procedures","title":"Procedures","text":""},{"location":"operations/lambda/#lambda-redeployment","title":"Lambda Redeployment","text":"<p>Update Lambda code only:</p> <pre><code>zae-limiter upgrade --name &lt;name&gt; --region &lt;region&gt; --lambda-only\n</code></pre> <p>Full stack update:</p> <pre><code>zae-limiter deploy --name &lt;name&gt; --region &lt;region&gt;\n</code></pre>"},{"location":"operations/lambda/#memorytimeout-adjustment","title":"Memory/Timeout Adjustment","text":"<p>Via CLI (recommended):</p> <pre><code># Redeploy with new settings\nzae-limiter deploy --name &lt;name&gt; --region &lt;region&gt; \\\n  --lambda-memory 512 \\\n  --lambda-timeout 120\n</code></pre> <p>Direct Lambda update:</p> <pre><code>aws lambda update-function-configuration \\\n  --function-name &lt;name&gt;-aggregator \\\n  --memory-size 512 \\\n  --timeout 120\n</code></pre>"},{"location":"operations/lambda/#verification","title":"Verification","text":"<p>After any Lambda changes, verify health:</p> <pre><code># Check function configuration\naws lambda get-function-configuration \\\n  --function-name &lt;name&gt;-aggregator\n\n# Watch for errors in real-time\naws logs tail /aws/lambda/&lt;name&gt;-aggregator --follow\n</code></pre>"},{"location":"operations/lambda/#related","title":"Related","text":"<ul> <li>Stream Processing - Iterator age and stream lag issues</li> <li>DynamoDB Operations - Throttling and capacity issues</li> <li>Monitoring Guide - CloudWatch dashboards and alerts</li> </ul>"},{"location":"operations/rate-limits/","title":"Rate Limit Operations","text":"<p>This guide covers troubleshooting rate limit enforcement issues and procedures for adjusting limits at runtime.</p>"},{"location":"operations/rate-limits/#decision-tree","title":"Decision Tree","text":"flowchart TD     START([Rate Limit Issue]) --&gt; Q1{What's happening?}      Q1 --&gt;|Unexpected RateLimitExceeded| A1[Check bucket state]     Q1 --&gt;|Limits not enforcing| A2[Check entity exists]     Q1 --&gt;|Cascade not working| A3[Check parent entity]     Q1 --&gt;|Need to adjust limits| A4[Runtime adjustment]      A1 --&gt; CHECK1[Query bucket in DynamoDB]     A2 --&gt; CHECK2[Query entity metadata]     A3 --&gt; CHECK3[Verify parent_id and entity cascade setting]      CHECK1 --&gt; FIX1{Bucket state?}     FIX1 --&gt;|Depleted| WAIT[Wait for refill or reset]     FIX1 --&gt;|Has capacity| CONFIG[Check limit configuration]      CHECK2 --&gt; FIX2{Entity exists?}     FIX2 --&gt;|No| CREATE[Create entity first]     FIX2 --&gt;|Yes| LIMITS[Check use_stored_limits]      click A1 \"#unexpected-ratelimitexceeded\" \"Diagnose unexpected limits\"     click A2 \"#limits-not-enforcing\" \"Check entity setup\"     click A3 \"#cascade-not-working\" \"Check parent setup\"     click A4 \"#adjust-limits-at-runtime\" \"Adjust limits\"     click WAIT \"#reset-bucket-state\" \"Reset bucket\"     click CHECK1 \"#debug-bucket-state\" \"Query bucket state\""},{"location":"operations/rate-limits/#troubleshooting","title":"Troubleshooting","text":""},{"location":"operations/rate-limits/#symptoms","title":"Symptoms","text":"<ul> <li>Requests succeed when they should be rate limited</li> <li><code>RateLimitExceeded</code> raised unexpectedly</li> <li>Cascade to parent entity not working</li> <li>Bucket state appears incorrect</li> </ul>"},{"location":"operations/rate-limits/#diagnostic-steps","title":"Diagnostic Steps","text":"<p>Check entity and bucket state:</p> <pre><code># Query entity metadata\naws dynamodb get-item --table-name &lt;name&gt; \\\n  --key '{\"PK\": {\"S\": \"ENTITY#&lt;entity_id&gt;\"}, \"SK\": {\"S\": \"#META\"}}'\n\n# Query bucket state for a specific limit\naws dynamodb get-item --table-name &lt;name&gt; \\\n  --key '{\"PK\": {\"S\": \"ENTITY#&lt;entity_id&gt;\"}, \"SK\": {\"S\": \"#BUCKET#&lt;resource&gt;#&lt;limit_name&gt;\"}}'\n</code></pre> <p>Verify stored limits (if using <code>use_stored_limits=True</code>):</p> <pre><code>aws dynamodb query --table-name &lt;name&gt; \\\n  --key-condition-expression \"PK = :pk AND begins_with(SK, :sk)\" \\\n  --expression-attribute-values '{\":pk\": {\"S\": \"ENTITY#&lt;entity_id&gt;\"}, \":sk\": {\"S\": \"#LIMIT#\"}}'\n</code></pre>"},{"location":"operations/rate-limits/#unexpected-ratelimitexceeded","title":"Unexpected RateLimitExceeded","text":"<p>Possible causes:</p> Cause Diagnosis Solution Bucket depleted Check bucket <code>tokens</code> value Wait for refill or increase limit Limit too restrictive Compare limit capacity vs usage Increase capacity Cascade triggered Check <code>violations</code> array Increase parent limit Clock skew Compare server time with bucket <code>last_update</code> Sync NTP <p>Check bucket state:</p> <pre><code>aws dynamodb get-item --table-name &lt;name&gt; \\\n  --key '{\"PK\": {\"S\": \"ENTITY#&lt;entity_id&gt;\"}, \"SK\": {\"S\": \"#BUCKET#&lt;resource&gt;#&lt;limit_name&gt;\"}}' \\\n  --projection-expression \"tokens, last_update, capacity\"\n</code></pre> <p>Interpret the response:</p> <ul> <li><code>tokens</code>: Current token count (in millitokens, divide by 1000)</li> <li><code>last_update</code>: Unix timestamp of last access</li> <li><code>capacity</code>: Maximum bucket capacity (in millitokens)</li> </ul>"},{"location":"operations/rate-limits/#limits-not-enforcing","title":"Limits Not Enforcing","text":"<p>Common causes:</p> Cause Solution Entity not created Create entity before rate limiting: <code>await limiter.create_entity(...)</code> Wrong <code>use_stored_limits</code> setting Set <code>use_stored_limits=True</code> if limits are in DynamoDB Stale bucket state Bucket refills over time; tokens may have refilled Limit configuration mismatch Verify limit <code>capacity</code>, <code>burst</code>, and <code>refill_rate</code> match expectations <p>Verify entity exists:</p> <pre><code>aws dynamodb get-item --table-name &lt;name&gt; \\\n  --key '{\"PK\": {\"S\": \"ENTITY#&lt;entity_id&gt;\"}, \"SK\": {\"S\": \"#META\"}}'\n</code></pre>"},{"location":"operations/rate-limits/#cascade-not-working","title":"Cascade Not Working","text":"<p>If cascade to parent is not enforced:</p> <p>Step 1: Verify parent entity exists:</p> <pre><code>aws dynamodb get-item --table-name &lt;name&gt; \\\n  --key '{\"PK\": {\"S\": \"ENTITY#&lt;parent_id&gt;\"}, \"SK\": {\"S\": \"#META\"}}'\n</code></pre> <p>Step 2: Verify child has <code>parent_id</code> set:</p> <pre><code>aws dynamodb get-item --table-name &lt;name&gt; \\\n  --key '{\"PK\": {\"S\": \"ENTITY#&lt;child_id&gt;\"}, \"SK\": {\"S\": \"#META\"}}'\n# Check the \"parent_id\" attribute in response\n</code></pre> <p>Step 3: Ensure entity was created with <code>cascade=True</code>:</p> <pre><code># Check the entity's cascade setting in its metadata\naws dynamodb get-item --table-name &lt;name&gt; \\\n  --key '{\"PK\": {\"S\": \"ENTITY#&lt;child_id&gt;\"}, \"SK\": {\"S\": \"#META\"}}'\n# Check the \"cascade\" attribute in response \u2014 should be true\n</code></pre> <p>If cascade is not enabled, recreate the entity with cascade:</p> <pre><code>await limiter.create_entity(\n    entity_id=\"child-id\",\n    parent_id=\"parent-id\",\n    cascade=True,  # Entity property \u2014 applies to all acquire() calls\n)\n</code></pre>"},{"location":"operations/rate-limits/#verification","title":"Verification","text":"<p>Test that rate limiting is working correctly:</p> <pre><code>from zae_limiter import RateLimiter, Limit, RateLimitExceeded\n\nlimiter = RateLimiter(name=\"&lt;name&gt;\", region=\"&lt;region&gt;\")\n\n# Consume all capacity\nfor i in range(100):\n    try:\n        async with limiter.acquire(\n            entity_id=\"test-entity\",\n            resource=\"test\",\n            limits=[Limit.per_minute(\"rpm\", 100)],\n            consume={\"rpm\": 1},\n        ):\n            pass\n    except RateLimitExceeded as e:\n        print(f\"Rate limited after {i} requests, retry_after={e.retry_after_seconds}s\")\n        break\n</code></pre>"},{"location":"operations/rate-limits/#procedures","title":"Procedures","text":""},{"location":"operations/rate-limits/#adjust-limits-at-runtime","title":"Adjust Limits at Runtime","text":""},{"location":"operations/rate-limits/#programmatic-adjustment","title":"Programmatic Adjustment","text":"<p>Update stored limits for an entity without redeployment:</p> <pre><code>from zae_limiter import RateLimiter, Limit\n\nlimiter = RateLimiter(name=\"&lt;name&gt;\", region=\"&lt;region&gt;\")\n\n# Update limits for an entity\nawait limiter.set_limits(\n    entity_id=\"api-key-123\",\n    resource=\"gpt-4\",  # Resource these limits apply to\n    limits=[\n        Limit.per_minute(\"rpm\", 1000),      # Requests per minute\n        Limit.per_minute(\"tpm\", 100_000),   # Tokens per minute\n    ],\n)\n\nprint(\"Limits updated successfully\")\n</code></pre>"},{"location":"operations/rate-limits/#direct-dynamodb-update","title":"Direct DynamoDB Update","text":"<p>Advanced</p> <p>Direct DynamoDB updates bypass validation. Use programmatic API when possible.</p> <pre><code># Update a stored limit\naws dynamodb put-item --table-name &lt;name&gt; \\\n  --item '{\n    \"PK\": {\"S\": \"ENTITY#&lt;entity_id&gt;\"},\n    \"SK\": {\"S\": \"#LIMIT#&lt;resource&gt;#&lt;limit_name&gt;\"},\n    \"capacity\": {\"N\": \"1000000\"},\n    \"burst\": {\"N\": \"1000000\"},\n    \"refill_amount\": {\"N\": \"1000000\"},\n    \"refill_period\": {\"N\": \"60\"}\n  }'\n</code></pre>"},{"location":"operations/rate-limits/#reset-bucket-state","title":"Reset Bucket State","text":"<p>Reset a bucket to restore full capacity:</p> <pre><code># Delete the bucket (will be recreated on next acquire with full capacity)\naws dynamodb delete-item --table-name &lt;name&gt; \\\n  --key '{\"PK\": {\"S\": \"ENTITY#&lt;entity_id&gt;\"}, \"SK\": {\"S\": \"#BUCKET#&lt;resource&gt;#&lt;limit_name&gt;\"}}'\n</code></pre>"},{"location":"operations/rate-limits/#debug-bucket-state","title":"Debug Bucket State","text":"<p>Query all buckets for an entity:</p> <pre><code>aws dynamodb query --table-name &lt;name&gt; \\\n  --key-condition-expression \"PK = :pk AND begins_with(SK, :sk)\" \\\n  --expression-attribute-values '{\":pk\": {\"S\": \"ENTITY#&lt;entity_id&gt;\"}, \":sk\": {\"S\": \"#BUCKET#\"}}' \\\n  --projection-expression \"SK, tokens, capacity, last_update\"\n</code></pre> <p>Interpret bucket values:</p> <p>All token values are stored as millitokens (multiply by 1000):</p> Field Description Example <code>tokens</code> Current available tokens \u00d7 1000 <code>50000</code> = 50 tokens <code>capacity</code> Maximum bucket size \u00d7 1000 <code>100000</code> = 100 tokens <code>last_update</code> Unix timestamp <code>1705312800</code>"},{"location":"operations/rate-limits/#verification-after-changes","title":"Verification After Changes","text":"<p>After adjusting limits, verify:</p> <pre><code># Check available capacity\navailable = await limiter.available(\n    entity_id=\"&lt;entity_id&gt;\",\n    resource=\"&lt;resource&gt;\",\n    limits=[Limit.per_minute(\"&lt;limit_name&gt;\", &lt;capacity&gt;)],\n)\n\nprint(f\"Available: {available}\")\n# Output: {'&lt;limit_name&gt;': &lt;available_tokens&gt;}\n</code></pre>"},{"location":"operations/rate-limits/#dynamodb-key-patterns","title":"DynamoDB Key Patterns","text":"Pattern Key Description Entity metadata <code>PK=ENTITY#&lt;id&gt;, SK=#META</code> Entity configuration Bucket state <code>PK=ENTITY#&lt;id&gt;, SK=#BUCKET#&lt;resource&gt;#&lt;limit&gt;</code> Token bucket Stored limit <code>PK=ENTITY#&lt;id&gt;, SK=#LIMIT#&lt;resource&gt;#&lt;limit&gt;</code> Limit configuration"},{"location":"operations/rate-limits/#related","title":"Related","text":"<ul> <li>Recovery &amp; Rollback - Reset corrupted buckets</li> <li>Performance Tuning - Capacity planning for rate limits</li> <li>Hierarchical Limits - Cascade configuration</li> </ul>"},{"location":"operations/recovery/","title":"Recovery &amp; Rollback","text":"<p>This guide covers backup, restore, and rollback procedures for zae-limiter deployments.</p>"},{"location":"operations/recovery/#decision-tree","title":"Decision Tree","text":"flowchart TD     START([What went wrong?]) --&gt; Q1{Issue type?}      Q1 --&gt;|Bad deployment| DEP{Deployment issue?}     Q1 --&gt;|Bad migration| MIG{Reversible?}     Q1 --&gt;|Data corruption| DATA{How recent?}     Q1 --&gt;|Complete failure| FAIL{Table exists?}      DEP --&gt;|CloudFormation failed| STACK_ROLL[Stack Rollback]     DEP --&gt;|Lambda broken| LAMBDA_FIX[Lambda Redeployment]     DEP --&gt;|Config wrong| STACK_UPDATE[Stack Redeployment]      MIG --&gt;|Yes| MIG_ROLL[Migration Rollback]     MIG --&gt;|No| BACKUP_RESTORE[Restore from Backup]      DATA --&gt;|&lt; 35 days| PITR[PITR Recovery]     DATA --&gt;|Older| BACKUP_RESTORE      FAIL --&gt;|Yes| STACK_UPDATE     FAIL --&gt;|No| FULL[Full Recovery]      click STACK_ROLL \"#stack-rollback\" \"CloudFormation auto-rollback\"     click LAMBDA_FIX \"#lambda-redeployment\" \"Redeploy Lambda code\"     click STACK_UPDATE \"#stack-redeployment\" \"Delete and recreate stack\"     click MIG_ROLL \"#migration-rollback\" \"Rollback reversible migration\"     click BACKUP_RESTORE \"#dynamodb-backup-and-restore\" \"Restore from on-demand backup\"     click PITR \"#point-in-time-recovery-pitr\" \"Point-in-time recovery\"     click FULL \"#full-recovery\" \"Complete recovery procedure\""},{"location":"operations/recovery/#emergency-rollback-decision-matrix","title":"Emergency Rollback Decision Matrix","text":"Situation Action Time to Recovery Data Loss Risk Lambda errors after upgrade <code>zae-limiter upgrade --lambda-only</code> with previous version ~2 min None Schema migration failed Restore from pre-migration backup ~10-30 min Changes since backup Stack update failed CloudFormation auto-rollback ~5 min None Data corruption (recent) PITR restore ~15-30 min None (point-in-time) Data corruption (old) On-demand backup restore ~15-30 min Changes since backup Complete stack failure Delete + redeploy (if deletion protection on) ~10 min None (table retained)"},{"location":"operations/recovery/#dynamodb-backup-and-restore","title":"DynamoDB Backup and Restore","text":""},{"location":"operations/recovery/#create-on-demand-backup","title":"Create On-Demand Backup","text":"<p>Before any risky operation:</p> <pre><code>aws dynamodb create-backup \\\n  --table-name &lt;name&gt; \\\n  --backup-name \"manual-backup-$(date +%Y%m%d-%H%M%S)\"\n</code></pre> <p>Verify backup created:</p> <pre><code>aws dynamodb list-backups \\\n  --table-name &lt;name&gt; \\\n  --time-range-lower-bound $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ)\n</code></pre>"},{"location":"operations/recovery/#list-available-backups","title":"List Available Backups","text":"<pre><code>aws dynamodb list-backups \\\n  --table-name &lt;name&gt;\n</code></pre>"},{"location":"operations/recovery/#restore-from-backup","title":"Restore from Backup","text":"<p>New Table Name</p> <p>Restore creates a new table. You must update your application to use the new table name or rename after restore.</p> <pre><code>aws dynamodb restore-table-from-backup \\\n  --target-table-name &lt;name&gt;-restored \\\n  --backup-arn &lt;backup-arn&gt;\n</code></pre> <p>Wait for restore to complete:</p> <pre><code>aws dynamodb describe-table --table-name &lt;name&gt;-restored \\\n  --query 'Table.TableStatus'\n</code></pre>"},{"location":"operations/recovery/#point-in-time-recovery-pitr","title":"Point-in-Time Recovery (PITR)","text":"<p>PITR allows restore to any point in the last 35 days.</p> <p>Check if PITR is enabled:</p> <pre><code>aws dynamodb describe-continuous-backups \\\n  --table-name &lt;name&gt;\n</code></pre> <p>Enable PITR (if not enabled):</p> <pre><code>aws dynamodb update-continuous-backups \\\n  --table-name &lt;name&gt; \\\n  --point-in-time-recovery-specification PointInTimeRecoveryEnabled=true\n</code></pre> <p>Restore to specific point in time:</p> <pre><code>aws dynamodb restore-table-to-point-in-time \\\n  --source-table-name &lt;name&gt; \\\n  --target-table-name &lt;name&gt;-restored \\\n  --restore-date-time \"2024-01-15T10:00:00Z\"\n</code></pre> <p>Restore to latest restorable time:</p> <pre><code>aws dynamodb restore-table-to-point-in-time \\\n  --source-table-name &lt;name&gt; \\\n  --target-table-name &lt;name&gt;-restored \\\n  --use-latest-restorable-time\n</code></pre>"},{"location":"operations/recovery/#migration-rollback","title":"Migration Rollback","text":""},{"location":"operations/recovery/#reversible-migrations","title":"Reversible Migrations","text":"<p>For migrations that support rollback:</p> <pre><code>from zae_limiter.migrations import get_migrations\nfrom zae_limiter.repository import Repository\n\nasync def rollback_migration(name: str, region: str, target_version: str):\n    \"\"\"Rollback a reversible migration.\"\"\"\n    repo = Repository(name, region, None)\n\n    try:\n        migrations = get_migrations()\n        target = next((m for m in migrations if m.version == target_version), None)\n\n        if target and target.reversible and target.rollback:\n            await target.rollback(repo)\n            print(f\"Rolled back migration {target_version}\")\n\n            # Update version record\n            await repo.set_version_record(\n                schema_version=\"&lt;previous_version&gt;\",\n                updated_by=\"manual_rollback\",\n            )\n        else:\n            print(\"Migration is not reversible - restore from backup\")\n    finally:\n        await repo.close()\n</code></pre>"},{"location":"operations/recovery/#non-reversible-migrations","title":"Non-Reversible Migrations","text":"<p>For migrations without rollback support:</p> <ol> <li>Restore from backup taken before migration</li> <li>Update stack to match restored schema version</li> <li>Downgrade client to compatible version</li> </ol>"},{"location":"operations/recovery/#stack-operations","title":"Stack Operations","text":""},{"location":"operations/recovery/#stack-rollback","title":"Stack Rollback","text":"<p>CloudFormation automatically rolls back failed updates. If a stack is stuck:</p> <p>Check stack status:</p> <pre><code>aws cloudformation describe-stacks \\\n  --stack-name &lt;name&gt; \\\n  --query 'Stacks[0].StackStatus'\n</code></pre> <p>View rollback events:</p> <pre><code>aws cloudformation describe-stack-events \\\n  --stack-name &lt;name&gt; \\\n  --query 'StackEvents[?ResourceStatus==`UPDATE_ROLLBACK_IN_PROGRESS` || ResourceStatus==`UPDATE_ROLLBACK_COMPLETE`]'\n</code></pre> <p>Continue rollback (if stuck):</p> <pre><code>aws cloudformation continue-update-rollback \\\n  --stack-name &lt;name&gt;\n</code></pre>"},{"location":"operations/recovery/#stack-redeployment","title":"Stack Redeployment","text":"<p>Delete and recreate stack (preserves DynamoDB data with deletion protection):</p> <pre><code># Step 1: Verify deletion protection is enabled\naws dynamodb describe-table --table-name &lt;name&gt; \\\n  --query 'Table.DeletionProtectionEnabled'\n\n# Step 2: Delete stack (table retained if deletion protection enabled)\nzae-limiter delete --name &lt;name&gt; --region &lt;region&gt; --yes\n\n# Step 3: Wait for deletion\naws cloudformation wait stack-delete-complete --stack-name &lt;name&gt;\n\n# Step 4: Redeploy\nzae-limiter deploy --name &lt;name&gt; --region &lt;region&gt;\n</code></pre> <p>Update existing stack:</p> <pre><code># Export current template\nzae-limiter cfn-template &gt; updated-template.yaml\n\n# Update via CloudFormation\naws cloudformation update-stack \\\n  --stack-name &lt;name&gt; \\\n  --template-body file://updated-template.yaml \\\n  --capabilities CAPABILITY_NAMED_IAM\n\n# Wait for update\naws cloudformation wait stack-update-complete --stack-name &lt;name&gt;\n</code></pre>"},{"location":"operations/recovery/#lambda-redeployment","title":"Lambda Redeployment","text":"<p>Redeploy Lambda with current code:</p> <pre><code>zae-limiter upgrade --name &lt;name&gt; --region &lt;region&gt; --lambda-only\n</code></pre> <p>Redeploy with specific version:</p> <pre><code># First, install the specific version\npip install zae-limiter==&lt;version&gt;\n\n# Then upgrade Lambda\nzae-limiter upgrade --name &lt;name&gt; --region &lt;region&gt; --lambda-only\n</code></pre>"},{"location":"operations/recovery/#data-reconciliation","title":"Data Reconciliation","text":""},{"location":"operations/recovery/#reset-corrupted-bucket","title":"Reset Corrupted Bucket","text":"<p>Delete a specific bucket (will be recreated on next acquire):</p> <pre><code>aws dynamodb delete-item --table-name &lt;name&gt; \\\n  --key '{\"PK\": {\"S\": \"ENTITY#&lt;entity_id&gt;\"}, \"SK\": {\"S\": \"#BUCKET#&lt;resource&gt;#&lt;limit_name&gt;\"}}'\n</code></pre>"},{"location":"operations/recovery/#reset-all-buckets-for-entity","title":"Reset All Buckets for Entity","text":"<pre><code># Query all buckets\nBUCKETS=$(aws dynamodb query --table-name &lt;name&gt; \\\n  --key-condition-expression \"PK = :pk AND begins_with(SK, :sk)\" \\\n  --expression-attribute-values '{\":pk\": {\"S\": \"ENTITY#&lt;entity_id&gt;\"}, \":sk\": {\"S\": \"#BUCKET#\"}}' \\\n  --projection-expression \"PK, SK\" \\\n  --output json)\n\n# Delete each bucket\necho $BUCKETS | jq -r '.Items[] | @base64' | while read item; do\n  PK=$(echo $item | base64 -d | jq -r '.PK.S')\n  SK=$(echo $item | base64 -d | jq -r '.SK.S')\n\n  aws dynamodb delete-item --table-name &lt;name&gt; \\\n    --key \"{\\\"PK\\\": {\\\"S\\\": \\\"$PK\\\"}, \\\"SK\\\": {\\\"S\\\": \\\"$SK\\\"}}\"\n\n  echo \"Deleted: $SK\"\ndone\n</code></pre>"},{"location":"operations/recovery/#verify-entity-integrity","title":"Verify Entity Integrity","text":"<pre><code>async def verify_entity(limiter, entity_id: str) -&gt; bool:\n    \"\"\"Verify entity can perform rate limiting operations.\"\"\"\n    from zae_limiter import Limit\n\n    try:\n        # Check entity exists\n        entity = await limiter.get_entity(entity_id)\n        print(f\"Entity: {entity.entity_id}, parent: {entity.parent_id}\")\n\n        # Check rate limiting works\n        available = await limiter.available(\n            entity_id=entity_id,\n            resource=\"health-check\",\n            limits=[Limit.per_minute(\"test\", 1000)],\n        )\n        print(f\"Available capacity: {available}\")\n\n        return True\n    except Exception as e:\n        print(f\"Entity verification failed: {e}\")\n        return False\n</code></pre>"},{"location":"operations/recovery/#full-recovery","title":"Full Recovery","text":"<p>Complete recovery from total failure:</p>"},{"location":"operations/recovery/#step-1-assess-situation","title":"Step 1: Assess Situation","text":"<pre><code># Check if table exists\naws dynamodb describe-table --table-name &lt;name&gt; 2&gt;/dev/null &amp;&amp; echo \"Table exists\" || echo \"Table missing\"\n\n# Check if stack exists\naws cloudformation describe-stacks --stack-name &lt;name&gt; 2&gt;/dev/null &amp;&amp; echo \"Stack exists\" || echo \"Stack missing\"\n</code></pre>"},{"location":"operations/recovery/#step-2-restore-table-if-needed","title":"Step 2: Restore Table (if needed)","text":"<pre><code># List available backups\naws dynamodb list-backups --table-name &lt;name&gt;\n\n# Restore from backup\naws dynamodb restore-table-from-backup \\\n  --target-table-name &lt;name&gt; \\\n  --backup-arn &lt;backup-arn&gt;\n\n# Wait for restore\naws dynamodb wait table-exists --table-name &lt;name&gt;\n</code></pre>"},{"location":"operations/recovery/#step-3-redeploy-stack","title":"Step 3: Redeploy Stack","text":"<pre><code>zae-limiter deploy --name &lt;name&gt; --region &lt;region&gt;\n</code></pre>"},{"location":"operations/recovery/#step-4-verify","title":"Step 4: Verify","text":"<pre><code># Check stack status\nzae-limiter status --name &lt;name&gt; --region &lt;region&gt;\n\n# Check version\nzae-limiter version --name &lt;name&gt; --region &lt;region&gt;\n\n# Check compatibility\nzae-limiter check --name &lt;name&gt; --region &lt;region&gt;\n</code></pre>"},{"location":"operations/recovery/#prevention","title":"Prevention","text":""},{"location":"operations/recovery/#enable-deletion-protection","title":"Enable Deletion Protection","text":"<pre><code>aws dynamodb update-table \\\n  --table-name &lt;name&gt; \\\n  --deletion-protection-enabled\n</code></pre>"},{"location":"operations/recovery/#enable-pitr","title":"Enable PITR","text":"<pre><code>aws dynamodb update-continuous-backups \\\n  --table-name &lt;name&gt; \\\n  --point-in-time-recovery-specification PointInTimeRecoveryEnabled=true\n</code></pre>"},{"location":"operations/recovery/#schedule-regular-backups","title":"Schedule Regular Backups","text":"<p>Create a CloudWatch Events rule to backup daily:</p> <pre><code># Create backup Lambda or use AWS Backup service\naws backup create-backup-plan \\\n  --backup-plan '{\"BackupPlanName\": \"zae-limiter-daily\", \"Rules\": [{\"RuleName\": \"daily\", \"TargetBackupVaultName\": \"Default\", \"ScheduleExpression\": \"cron(0 5 * * ? *)\", \"Lifecycle\": {\"DeleteAfterDays\": 30}}]}'\n</code></pre>"},{"location":"operations/recovery/#related","title":"Related","text":"<ul> <li>Version Management - Version upgrade and rollback</li> <li>DynamoDB Operations - Capacity and throttling issues</li> <li>Migration Guide - Schema migration procedures</li> </ul>"},{"location":"operations/streams/","title":"Stream Processing Operations","text":"<p>This guide covers troubleshooting and operational procedures for DynamoDB Streams processing, which powers the usage aggregation feature.</p>"},{"location":"operations/streams/#decision-tree","title":"Decision Tree","text":"flowchart TD     START([Stream Issue]) --&gt; Q1{What's the symptom?}      Q1 --&gt;|High IteratorAge| A1[Check processing lag]     Q1 --&gt;|Lambda throttling| A2[Check concurrency]     Q1 --&gt;|Missing snapshots| A3[Check Lambda errors]     Q1 --&gt;|Planning optimization| A4[Tune configuration]      A1 --&gt; CHECK1{Lambda healthy?}     CHECK1 --&gt;|No| LINK1([\u2192 Lambda])     CHECK1 --&gt;|Yes| SCALE[Increase concurrency]      A2 --&gt; SCALE     A3 --&gt; LINK1     A4 --&gt; TUNE[Adjust batch size / concurrency]      click A1 \"#high-iterator-age\" \"Diagnose lag\"     click LINK1 \"lambda/\" \"Lambda troubleshooting\"     click SCALE \"#scaling-stream-processing\" \"Scale concurrency\"     click A3 \"lambda/#error-rate-issues\" \"Check Lambda errors\"     click TUNE \"#tuning-stream-processing\" \"Configuration tuning\""},{"location":"operations/streams/#troubleshooting","title":"Troubleshooting","text":""},{"location":"operations/streams/#symptoms","title":"Symptoms","text":"<ul> <li><code>IteratorAge</code> metric growing</li> <li>Usage snapshots delayed</li> <li>Stream iterator age alarm triggered</li> <li>Lambda throttling</li> </ul>"},{"location":"operations/streams/#diagnostic-steps","title":"Diagnostic Steps","text":"<p>Check IteratorAge metric:</p> <pre><code>aws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name IteratorAge \\\n  --dimensions Name=FunctionName,Value=&lt;name&gt;-aggregator \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 60 \\\n  --statistics Maximum\n</code></pre> <p>Check stream status:</p> <pre><code>aws dynamodb describe-table --table-name &lt;name&gt; \\\n  --query 'Table.StreamSpecification'\n</code></pre> <p>Check Lambda event source mapping:</p> <pre><code>aws lambda list-event-source-mappings \\\n  --function-name &lt;name&gt;-aggregator\n</code></pre> <p>Check Lambda concurrent executions:</p> <pre><code>aws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name ConcurrentExecutions \\\n  --dimensions Name=FunctionName,Value=&lt;name&gt;-aggregator \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 60 \\\n  --statistics Maximum\n</code></pre>"},{"location":"operations/streams/#high-iterator-age","title":"High Iterator Age","text":"<p>Common causes and solutions:</p> Cause Solution Lambda errors Fix errors (check DLQ and logs) - see Lambda Operations Lambda throttling Increase reserved concurrency Low Lambda concurrency Match concurrency to shard count DynamoDB throttling Increase table capacity - see DynamoDB Operations Large batch sizes Reduce batch size in event source mapping <p>Understanding IteratorAge:</p> <ul> <li><code>IteratorAge</code> measures the delay between when a record is written to the stream and when Lambda processes it</li> <li>Healthy: &lt; 1 second</li> <li>Warning: &lt; 30 seconds</li> <li>Critical: &gt; 30 seconds (default alarm threshold)</li> </ul>"},{"location":"operations/streams/#lambda-throttling","title":"Lambda Throttling","text":"<p>Check if Lambda is being throttled:</p> <pre><code>aws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name Throttles \\\n  --dimensions Name=FunctionName,Value=&lt;name&gt;-aggregator \\\n  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 300 \\\n  --statistics Sum\n</code></pre> <p>Solution: Increase reserved concurrency (see procedures below).</p>"},{"location":"operations/streams/#procedures","title":"Procedures","text":""},{"location":"operations/streams/#increase-lambda-concurrency","title":"Increase Lambda Concurrency","text":"<p>Set reserved concurrency:</p> <pre><code>aws lambda put-function-concurrency \\\n  --function-name &lt;name&gt;-aggregator \\\n  --reserved-concurrent-executions 10\n</code></pre> <p>Check current concurrency:</p> <pre><code>aws lambda get-function-concurrency \\\n  --function-name &lt;name&gt;-aggregator\n</code></pre> <p>Remove concurrency limit (use account default):</p> <pre><code>aws lambda delete-function-concurrency \\\n  --function-name &lt;name&gt;-aggregator\n</code></pre>"},{"location":"operations/streams/#adjust-batch-size","title":"Adjust Batch Size","text":"<p>Get current mapping UUID:</p> <pre><code>MAPPING_UUID=$(aws lambda list-event-source-mappings \\\n  --function-name &lt;name&gt;-aggregator \\\n  --query 'EventSourceMappings[0].UUID' \\\n  --output text)\n\necho \"Mapping UUID: $MAPPING_UUID\"\n</code></pre> <p>Reduce batch size (process fewer records per invocation):</p> <pre><code>aws lambda update-event-source-mapping \\\n  --uuid $MAPPING_UUID \\\n  --batch-size 50\n</code></pre> <p>Increase batch size (higher throughput, higher latency):</p> <pre><code>aws lambda update-event-source-mapping \\\n  --uuid $MAPPING_UUID \\\n  --batch-size 100\n</code></pre> <p>Adjust batch window (wait for more records):</p> <pre><code>aws lambda update-event-source-mapping \\\n  --uuid $MAPPING_UUID \\\n  --maximum-batching-window-in-seconds 5\n</code></pre>"},{"location":"operations/streams/#shard-scaling","title":"Shard Scaling","text":"<p>DynamoDB Streams automatically scales shards based on table throughput.</p> <p>Check shard count:</p> <pre><code>aws dynamodbstreams describe-stream \\\n  --stream-arn $(aws dynamodb describe-table --table-name &lt;name&gt; \\\n    --query 'Table.LatestStreamArn' --output text) \\\n  --query 'StreamDescription.Shards | length(@)'\n</code></pre> <p>Rule of thumb: Lambda concurrency should be &gt;= shard count for optimal processing.</p> <p>If you have 10 shards but only 5 concurrent Lambda executions, processing will lag.</p>"},{"location":"operations/streams/#enable-parallelization-factor","title":"Enable Parallelization Factor","text":"<p>Process multiple batches from the same shard concurrently:</p> <pre><code>aws lambda update-event-source-mapping \\\n  --uuid $MAPPING_UUID \\\n  --parallelization-factor 2\n</code></pre> <p>Valid values: 1-10 (default: 1)</p> <p>Ordering</p> <p>Increasing parallelization factor may result in out-of-order processing within a shard. This is acceptable for usage aggregation but may not be suitable for all use cases.</p>"},{"location":"operations/streams/#verify-stream-health","title":"Verify Stream Health","text":"<p>Check stream is enabled:</p> <pre><code>aws dynamodb describe-table --table-name &lt;name&gt; \\\n  --query 'Table.StreamSpecification'\n</code></pre> <p>Expected output: <pre><code>{\n    \"StreamEnabled\": true,\n    \"StreamViewType\": \"NEW_AND_OLD_IMAGES\"\n}\n</code></pre></p> <p>Check event source mapping is active:</p> <pre><code>aws lambda list-event-source-mappings \\\n  --function-name &lt;name&gt;-aggregator \\\n  --query 'EventSourceMappings[0].State'\n</code></pre> <p>Expected output: <code>\"Enabled\"</code></p>"},{"location":"operations/streams/#monitor-after-changes","title":"Monitor After Changes","text":"<p>After tuning, monitor for 15-30 minutes:</p> <pre><code># Watch IteratorAge\nwatch -n 30 \"aws cloudwatch get-metric-statistics \\\n  --namespace AWS/Lambda \\\n  --metric-name IteratorAge \\\n  --dimensions Name=FunctionName,Value=&lt;name&gt;-aggregator \\\n  --start-time \\$(date -u -d '30 minutes ago' +%Y-%m-%dT%H:%M:%SZ) \\\n  --end-time \\$(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  --period 60 \\\n  --statistics Maximum \\\n  --query 'Datapoints | sort_by(@, &amp;Timestamp) | [-1].Maximum'\"\n</code></pre>"},{"location":"operations/streams/#tuning-guidelines","title":"Tuning Guidelines","text":"Scenario Batch Size Concurrency Parallelization Low volume (&lt; 100 req/s) 100 2 1 Medium volume (100-1000 req/s) 100 5-10 1 High volume (&gt; 1000 req/s) 50-100 10+ 2 Real-time requirements 10-50 10+ 2-5"},{"location":"operations/streams/#related","title":"Related","text":"<ul> <li>Lambda Operations - Lambda errors and duration issues</li> <li>DynamoDB Operations - Throttling affecting stream processing</li> <li>Monitoring Guide - CloudWatch dashboards for streams</li> </ul>"},{"location":"operations/version/","title":"Version Management","text":"<p>This guide covers version compatibility issues and upgrade procedures for zae-limiter.</p>"},{"location":"operations/version/#decision-tree","title":"Decision Tree","text":"flowchart TD     START([Version Issue]) --&gt; Q1{What's happening?}      Q1 --&gt;|VersionMismatchError| A1[Lambda needs update]     Q1 --&gt;|IncompatibleSchemaError| A2[Schema migration required]     Q1 --&gt;|Minimum client error| A3[Upgrade pip package]     Q1 --&gt;|Planning upgrade| A4[Follow upgrade procedure]      A1 --&gt; CMD1[\"zae-limiter upgrade --name X\"]     A2 --&gt; CMD2[Follow migrations guide]     A3 --&gt; CMD3[\"pip install --upgrade zae-limiter\"]     A4 --&gt; PROC[Pre-upgrade checklist]      CMD1 --&gt; VERIFY     CMD2 --&gt; VERIFY     CMD3 --&gt; VERIFY     VERIFY([Verify: zae-limiter check])      click A1 \"#versionmismatcherror\" \"Version mismatch details\"     click A2 \"#incompatibleschemaerror\" \"Schema error details\"     click CMD1 \"#upgrade-procedure\" \"Upgrade steps\"     click CMD2 \"../migrations/\" \"Migration guide\"     click A4 \"#upgrade-procedure\" \"Upgrade checklist\"     click VERIFY \"#verification\" \"Verify upgrade\""},{"location":"operations/version/#troubleshooting","title":"Troubleshooting","text":""},{"location":"operations/version/#symptoms","title":"Symptoms","text":"<ul> <li><code>VersionMismatchError</code> exception raised</li> <li><code>IncompatibleSchemaError</code> exception raised</li> <li>CLI commands fail with version errors</li> <li>Rate limiter initialization fails</li> </ul>"},{"location":"operations/version/#diagnostic-steps","title":"Diagnostic Steps","text":"<p>Check compatibility with CLI:</p> <pre><code>zae-limiter check --name &lt;name&gt; --region &lt;region&gt;\n</code></pre> <p>View detailed version information:</p> <pre><code>zae-limiter version --name &lt;name&gt; --region &lt;region&gt;\n</code></pre> <p>Query version record directly:</p> <pre><code>aws dynamodb get-item --table-name &lt;name&gt; \\\n  --key '{\"PK\": {\"S\": \"SYSTEM#\"}, \"SK\": {\"S\": \"#VERSION\"}}'\n</code></pre>"},{"location":"operations/version/#versionmismatcherror","title":"VersionMismatchError","text":"<p>Cause: Client library version differs from deployed Lambda version.</p> <p>Example error: <pre><code>VersionMismatchError: Version mismatch: client=1.2.0, schema=1.0.0, lambda=1.0.0.\nLambda update available.\n</code></pre></p> <p>Solution: Upgrade Lambda to match client:</p> <pre><code>zae-limiter upgrade --name &lt;name&gt; --region &lt;region&gt;\n</code></pre> <p>Or programmatically:</p> <pre><code>from zae_limiter import RateLimiter, StackOptions\n\n# Auto-update Lambda on initialization\nlimiter = RateLimiter(\n    name=\"&lt;name&gt;\",\n    region=\"&lt;region&gt;\",\n    stack_options=StackOptions(),  # Enables auto-update\n)\n</code></pre>"},{"location":"operations/version/#incompatibleschemaerror","title":"IncompatibleSchemaError","text":"<p>Cause: Major version difference requiring schema migration.</p> <p>Example error: <pre><code>IncompatibleSchemaError: Incompatible schema: client 2.0.0 is not compatible\nwith schema 1.0.0. Migration required.\n</code></pre></p> <p>Solution: Follow the Migration Guide to upgrade the schema:</p> <ol> <li>Create a backup</li> <li>Run migration</li> <li>Update client</li> </ol> <pre><code># Create backup before migration\naws dynamodb create-backup \\\n  --table-name &lt;name&gt; \\\n  --backup-name \"pre-migration-$(date +%Y%m%d)\"\n</code></pre> <p>Then follow the migration procedures in the Migration Guide.</p>"},{"location":"operations/version/#minimum-client-version-error","title":"Minimum Client Version Error","text":"<p>Cause: Infrastructure requires a newer client version.</p> <p>Solution: Upgrade the client library:</p> <pre><code>pip install --upgrade zae-limiter\n</code></pre>"},{"location":"operations/version/#upgrade-procedure","title":"Upgrade Procedure","text":""},{"location":"operations/version/#pre-upgrade-checklist","title":"Pre-upgrade Checklist","text":"<p>Before upgrading, verify the following:</p> <ul> <li> Check current version: <code>zae-limiter version --name &lt;name&gt;</code></li> <li> Check compatibility: <code>zae-limiter check --name &lt;name&gt;</code></li> <li> Review release notes for breaking changes</li> <li> Verify PITR is enabled for rollback capability</li> <li> Schedule maintenance window (if major upgrade)</li> <li> Notify stakeholders</li> </ul>"},{"location":"operations/version/#upgrade-execution","title":"Upgrade Execution","text":"<p>Standard upgrade (Lambda + client):</p> <pre><code># Step 1: Upgrade client library\npip install --upgrade zae-limiter\n\n# Step 2: Update infrastructure\nzae-limiter upgrade --name &lt;name&gt; --region &lt;region&gt;\n\n# Step 3: Verify\nzae-limiter check --name &lt;name&gt; --region &lt;region&gt;\n</code></pre> <p>Lambda-only upgrade:</p> <pre><code># Update Lambda without schema changes\nzae-limiter upgrade --name &lt;name&gt; --region &lt;region&gt; --lambda-only\n</code></pre> <p>Force upgrade (skip compatibility check):</p> <p>Use with caution</p> <p>Only use <code>--force</code> when you understand the implications.</p> <pre><code>zae-limiter upgrade --name &lt;name&gt; --region &lt;region&gt; --force\n</code></pre>"},{"location":"operations/version/#post-upgrade-verification","title":"Post-upgrade Verification","text":"<p>After upgrading, verify the system is healthy:</p> <ol> <li> <p>Check version alignment: <pre><code>zae-limiter version --name &lt;name&gt;\n</code></pre></p> </li> <li> <p>Run smoke tests: <pre><code>from zae_limiter import RateLimiter, Limit\n\nlimiter = RateLimiter(name=\"&lt;name&gt;\", region=\"&lt;region&gt;\")\n\n# Test basic operation\nasync with limiter.acquire(\n    entity_id=\"test-entity\",\n    resource=\"test\",\n    limits=[Limit.per_minute(\"rpm\", 100)],\n    consume={\"rpm\": 1},\n):\n    print(\"Rate limiting working\")\n</code></pre></p> </li> <li> <p>Monitor for 15 minutes:</p> </li> <li>Check Lambda error rate in CloudWatch</li> <li>Verify usage snapshots are updating</li> <li>Watch for unexpected exceptions in application logs</li> </ol>"},{"location":"operations/version/#rollback","title":"Rollback","text":"<p>If issues occur after upgrade, see Recovery &amp; Rollback.</p>"},{"location":"operations/version/#related","title":"Related","text":"<ul> <li>Migration Guide - Schema versioning and migration procedures</li> <li>Recovery &amp; Rollback - Emergency rollback procedures</li> <li>CLI Reference - Full CLI command documentation</li> </ul>"}]}